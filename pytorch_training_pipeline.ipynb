{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53989a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello WOrld!!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello WOrld!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ff00b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f918527b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 33)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d492549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-5d45952c-316c-4f1f-99dc-08beadabdc0e\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5d45952c-316c-4f1f-99dc-08beadabdc0e')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-5d45952c-316c-4f1f-99dc-08beadabdc0e button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-5d45952c-316c-4f1f-99dc-08beadabdc0e');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d556c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['id', 'Unnamed: 32'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f6590d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-42b2bffe-482b-4be9-8cc2-c80d7e2123b4\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42b2bffe-482b-4be9-8cc2-c80d7e2123b4')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-42b2bffe-482b-4be9-8cc2-c80d7e2123b4 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-42b2bffe-482b-4be9-8cc2-c80d7e2123b4');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0         M        17.99         10.38          122.80     1001.0   \n",
       "1         M        20.57         17.77          132.90     1326.0   \n",
       "2         M        19.69         21.25          130.00     1203.0   \n",
       "3         M        11.42         20.38           77.58      386.1   \n",
       "4         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "4         0.1809  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f608f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:], df.iloc[:, 0], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7afd1563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30)\n",
      "(569, 31)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce8489b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bcb38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_test_encoded = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ec4fe",
   "metadata": {},
   "source": [
    "Numpy Arrays to PyTorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "379c2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train_scaled)\n",
    "X_test_tensor = torch.from_numpy(X_test_scaled)\n",
    "y_train_tensor = torch.from_numpy(y_train_encoded)\n",
    "y_test_tensor = torch.from_numpy(y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96dde298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4408, -0.4353, -1.3621,  ...,  0.9320,  2.0972,  1.8865],\n",
       "        [ 1.9741,  1.7330,  2.0917,  ...,  2.6989,  1.8912,  2.4978],\n",
       "        [-1.4000, -1.2496, -1.3452,  ..., -0.9702,  0.5976,  0.0579],\n",
       "        ...,\n",
       "        [ 0.0488, -0.5550, -0.0651,  ..., -1.2390, -0.7086, -1.2715],\n",
       "        [-0.0390,  0.1021, -0.0314,  ...,  1.0500,  0.4343,  1.2134],\n",
       "        [-0.5486,  0.3133, -0.6035,  ..., -0.6110, -0.3345, -0.8463]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd18c07f",
   "metadata": {},
   "source": [
    "Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a373a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN():\n",
    "    \n",
    "    def __init__(self, X):\n",
    "        self.weights = torch.rand(X.shape[1], 1, dtype=torch.float64, requires_grad=True)\n",
    "        self.bias = torch.zeros(1, dtype=torch.float64, requires_grad=True)\n",
    "        \n",
    "    def forward_pass(self, X):\n",
    "        y = torch.matmul(X, self.weights) + self.bias\n",
    "        \n",
    "        y_pred = torch.sigmoid(y)\n",
    "        return y_pred\n",
    "        \n",
    "        \n",
    "    def loss_calculation(self, y_pred, y_true):\n",
    "        epsilon = 1e-7\n",
    "        \n",
    "        y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        loss = -(y_true*torch.log(y_pred) + (1-y_true)*torch.log(1-y_pred)).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    " \n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc47a8",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a873ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4581e765",
   "metadata": {},
   "source": [
    "Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0f5571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], dtype=torch.float64, requires_grad=True)\n",
      " for epoch : 0/500, loss is 3.4049844348333385\n",
      " for epoch : 1/500, loss is 3.3385267460177896\n",
      " for epoch : 2/500, loss is 3.2712022158174863\n",
      " for epoch : 3/500, loss is 3.203201425458466\n",
      " for epoch : 4/500, loss is 3.135638959226461\n",
      " for epoch : 5/500, loss is 3.0676654436718764\n",
      " for epoch : 6/500, loss is 2.9971589960680562\n",
      " for epoch : 7/500, loss is 2.9266572235722115\n",
      " for epoch : 8/500, loss is 2.856077324197744\n",
      " for epoch : 9/500, loss is 2.7860426016759328\n",
      " for epoch : 10/500, loss is 2.714509398435148\n",
      " for epoch : 11/500, loss is 2.6413414079036333\n",
      " for epoch : 12/500, loss is 2.568114352291198\n",
      " for epoch : 13/500, loss is 2.4957833977784465\n",
      " for epoch : 14/500, loss is 2.424416957249039\n",
      " for epoch : 15/500, loss is 2.352839109519073\n",
      " for epoch : 16/500, loss is 2.2806521071437675\n",
      " for epoch : 17/500, loss is 2.20709449137832\n",
      " for epoch : 18/500, loss is 2.1316732649123304\n",
      " for epoch : 19/500, loss is 2.0578353319323597\n",
      " for epoch : 20/500, loss is 1.9857087548781798\n",
      " for epoch : 21/500, loss is 1.9145916757332557\n",
      " for epoch : 22/500, loss is 1.8417968129629343\n",
      " for epoch : 23/500, loss is 1.7700002626320377\n",
      " for epoch : 24/500, loss is 1.7006906672904176\n",
      " for epoch : 25/500, loss is 1.634029892697208\n",
      " for epoch : 26/500, loss is 1.5688095138448124\n",
      " for epoch : 27/500, loss is 1.5042843137786701\n",
      " for epoch : 28/500, loss is 1.4429862412836123\n",
      " for epoch : 29/500, loss is 1.3850371494053864\n",
      " for epoch : 30/500, loss is 1.3305325510239996\n",
      " for epoch : 31/500, loss is 1.279537595525866\n",
      " for epoch : 32/500, loss is 1.2320835235658527\n",
      " for epoch : 33/500, loss is 1.1881645322279006\n",
      " for epoch : 34/500, loss is 1.1477352482636445\n",
      " for epoch : 35/500, loss is 1.1107093949817821\n",
      " for epoch : 36/500, loss is 1.0769604829099917\n",
      " for epoch : 37/500, loss is 1.046325215270926\n",
      " for epoch : 38/500, loss is 1.0186097437216606\n",
      " for epoch : 39/500, loss is 0.9935981675220964\n",
      " for epoch : 40/500, loss is 0.9710620984181667\n",
      " for epoch : 41/500, loss is 0.9507699635246952\n",
      " for epoch : 42/500, loss is 0.9324949855671911\n",
      " for epoch : 43/500, loss is 0.9160212637212856\n",
      " for epoch : 44/500, loss is 0.9011478503540681\n",
      " for epoch : 45/500, loss is 0.8876910438370634\n",
      " for epoch : 46/500, loss is 0.8754852702970148\n",
      " for epoch : 47/500, loss is 0.8643829488673748\n",
      " for epoch : 48/500, loss is 0.8542536840365439\n",
      " for epoch : 49/500, loss is 0.8449830521997632\n",
      " for epoch : 50/500, loss is 0.8364711753317453\n",
      " for epoch : 51/500, loss is 0.8286312144200443\n",
      " for epoch : 52/500, loss is 0.8213878707473502\n",
      " for epoch : 53/500, loss is 0.8146759517583323\n",
      " for epoch : 54/500, loss is 0.808439036542494\n",
      " for epoch : 55/500, loss is 0.8026282608347679\n",
      " for epoch : 56/500, loss is 0.7972012306969226\n",
      " for epoch : 57/500, loss is 0.7921210663156587\n",
      " for epoch : 58/500, loss is 0.7873555718196403\n",
      " for epoch : 59/500, loss is 0.7828765231890492\n",
      " for epoch : 60/500, loss is 0.7786590638824713\n",
      " for epoch : 61/500, loss is 0.7746811964801894\n",
      " for epoch : 62/500, loss is 0.7709233582093483\n",
      " for epoch : 63/500, loss is 0.7673680684589118\n",
      " for epoch : 64/500, loss is 0.763999637112868\n",
      " for epoch : 65/500, loss is 0.7608039235555437\n",
      " for epoch : 66/500, loss is 0.7577681373897157\n",
      " for epoch : 67/500, loss is 0.7548806731446629\n",
      " for epoch : 68/500, loss is 0.7521309724558961\n",
      " for epoch : 69/500, loss is 0.7495094083165644\n",
      " for epoch : 70/500, loss is 0.7470071870003961\n",
      " for epoch : 71/500, loss is 0.7446162641229742\n",
      " for epoch : 72/500, loss is 0.7423292720405147\n",
      " for epoch : 73/500, loss is 0.7401394563900676\n",
      " for epoch : 74/500, loss is 0.7380406200645261\n",
      " for epoch : 75/500, loss is 0.7360270733048315\n",
      " for epoch : 76/500, loss is 0.7340935888959389\n",
      " for epoch : 77/500, loss is 0.7322353616873931\n",
      " for epoch : 78/500, loss is 0.7304479718374218\n",
      " for epoch : 79/500, loss is 0.7287273513131164\n",
      " for epoch : 80/500, loss is 0.7270697532785572\n",
      " for epoch : 81/500, loss is 0.7254717240758395\n",
      " for epoch : 82/500, loss is 0.7239300775574287\n",
      " for epoch : 83/500, loss is 0.7224418715672685\n",
      " for epoch : 84/500, loss is 0.7210043863965494\n",
      " for epoch : 85/500, loss is 0.7196151050609849\n",
      " for epoch : 86/500, loss is 0.7182716952621132\n",
      " for epoch : 87/500, loss is 0.7169719929071208\n",
      " for epoch : 88/500, loss is 0.715713987071174\n",
      " for epoch : 89/500, loss is 0.7144958062940656\n",
      " for epoch : 90/500, loss is 0.7133157061096994\n",
      " for epoch : 91/500, loss is 0.712172057712935\n",
      " for epoch : 92/500, loss is 0.7110633376738604\n",
      " for epoch : 93/500, loss is 0.7099881186147996\n",
      " for epoch : 94/500, loss is 0.7089450607703873\n",
      " for epoch : 95/500, loss is 0.7079329043559288\n",
      " for epoch : 96/500, loss is 0.7069504626739986\n",
      " for epoch : 97/500, loss is 0.7059966158938521\n",
      " for epoch : 98/500, loss is 0.7050703054427101\n",
      " for epoch : 99/500, loss is 0.7041705289523195\n",
      " for epoch : 100/500, loss is 0.7032963357083832\n",
      " for epoch : 101/500, loss is 0.7024468225544837\n",
      " for epoch : 102/500, loss is 0.7016211302059564\n",
      " for epoch : 103/500, loss is 0.7008184399328571\n",
      " for epoch : 104/500, loss is 0.7000379705746107\n",
      " for epoch : 105/500, loss is 0.6992789758522223\n",
      " for epoch : 106/500, loss is 0.6985407419469927\n",
      " for epoch : 107/500, loss is 0.6978225853175624\n",
      " for epoch : 108/500, loss is 0.6971238507297838\n",
      " for epoch : 109/500, loss is 0.6964439094764021\n",
      " for epoch : 110/500, loss is 0.6957821577658279\n",
      " for epoch : 111/500, loss is 0.6951380152613899\n",
      " for epoch : 112/500, loss is 0.6945109237544042\n",
      " for epoch : 113/500, loss is 0.6939003459561576\n",
      " for epoch : 114/500, loss is 0.6933057643955273\n",
      " for epoch : 115/500, loss is 0.6927266804104146\n",
      " for epoch : 116/500, loss is 0.6921626132225056\n",
      " for epoch : 117/500, loss is 0.6916130990860567\n",
      " for epoch : 118/500, loss is 0.691077690502489\n",
      " for epoch : 119/500, loss is 0.690555955493525\n",
      " for epoch : 120/500, loss is 0.6900474769264792\n",
      " for epoch : 121/500, loss is 0.6895518518860632\n",
      " for epoch : 122/500, loss is 0.6890686910877621\n",
      " for epoch : 123/500, loss is 0.6885976183284341\n",
      " for epoch : 124/500, loss is 0.6881382699703235\n",
      " for epoch : 125/500, loss is 0.6876902944551472\n",
      " for epoch : 126/500, loss is 0.6872533518453274\n",
      " for epoch : 127/500, loss is 0.6868271133898104\n",
      " for epoch : 128/500, loss is 0.6864112611122258\n",
      " for epoch : 129/500, loss is 0.6860054874194201\n",
      " for epoch : 130/500, loss is 0.6856094947286413\n",
      " for epoch : 131/500, loss is 0.6852229951118648\n",
      " for epoch : 132/500, loss is 0.6848457099559276\n",
      " for epoch : 133/500, loss is 0.6844773696373057\n",
      " for epoch : 134/500, loss is 0.6841177132105003\n",
      " for epoch : 135/500, loss is 0.6837664881091222\n",
      " for epoch : 136/500, loss is 0.6834234498588657\n",
      " for epoch : 137/500, loss is 0.68308836180165\n",
      " for epoch : 138/500, loss is 0.6827609948302903\n",
      " for epoch : 139/500, loss is 0.6824411271331144\n",
      " for epoch : 140/500, loss is 0.6821285439480158\n",
      " for epoch : 141/500, loss is 0.6818230373254621\n",
      " for epoch : 142/500, loss is 0.6815244059000424\n",
      " for epoch : 143/500, loss is 0.6812324546701559\n",
      " for epoch : 144/500, loss is 0.6809469947854877\n",
      " for epoch : 145/500, loss is 0.680667843341939\n",
      " for epoch : 146/500, loss is 0.6803948231837064\n",
      " for epoch : 147/500, loss is 0.6801277627122255\n",
      " for epoch : 148/500, loss is 0.6798664957017112\n",
      " for epoch : 149/500, loss is 0.6796108611210416\n",
      " for epoch : 150/500, loss is 0.6793607029617544\n",
      " for epoch : 151/500, loss is 0.6791158700719268\n",
      " for epoch : 152/500, loss is 0.6788762159957287\n",
      " for epoch : 153/500, loss is 0.678641598818447\n",
      " for epoch : 154/500, loss is 0.6784118810167876\n",
      " for epoch : 155/500, loss is 0.6781869293142699\n",
      " for epoch : 156/500, loss is 0.6779666145415365\n",
      " for epoch : 157/500, loss is 0.67775081150141\n",
      " for epoch : 158/500, loss is 0.6775393988385316\n",
      " for epoch : 159/500, loss is 0.6773322589134232\n",
      " for epoch : 160/500, loss is 0.6771292776808249\n",
      " for epoch : 161/500, loss is 0.6769303445721578\n",
      " for epoch : 162/500, loss is 0.6767353523819745\n",
      " for epoch : 163/500, loss is 0.6765441971582566\n",
      " for epoch : 164/500, loss is 0.6763567780964328\n",
      " for epoch : 165/500, loss is 0.676172997436987\n",
      " for epoch : 166/500, loss is 0.6759927603665341\n",
      " for epoch : 167/500, loss is 0.6758159749222448\n",
      " for epoch : 168/500, loss is 0.6756425518995065\n",
      " for epoch : 169/500, loss is 0.6754724047627053\n",
      " for epoch : 170/500, loss is 0.6753054495590266\n",
      " for epoch : 171/500, loss is 0.6751416048351659\n",
      " for epoch : 172/500, loss is 0.6749807915568541\n",
      " for epoch : 173/500, loss is 0.6748229330310963\n",
      " for epoch : 174/500, loss is 0.6746679548310339\n",
      " for epoch : 175/500, loss is 0.674515784723337\n",
      " for epoch : 176/500, loss is 0.674366352598041\n",
      " for epoch : 177/500, loss is 0.6742195904007429\n",
      " for epoch : 178/500, loss is 0.6740754320670759\n",
      " for epoch : 179/500, loss is 0.6739338134593823\n",
      " for epoch : 180/500, loss is 0.6737946723055092\n",
      " for epoch : 181/500, loss is 0.673657948139656\n",
      " for epoch : 182/500, loss is 0.6735235822451978\n",
      " for epoch : 183/500, loss is 0.6733915175994214\n",
      " for epoch : 184/500, loss is 0.6732616988201032\n",
      " for epoch : 185/500, loss is 0.67313407211387\n",
      " for epoch : 186/500, loss is 0.6730085852262756\n",
      " for epoch : 187/500, loss is 0.6728851873935398\n",
      " for epoch : 188/500, loss is 0.6727638292958873\n",
      " for epoch : 189/500, loss is 0.672644463012433\n",
      " for epoch : 190/500, loss is 0.6725270419775636\n",
      " for epoch : 191/500, loss is 0.6724115209387569\n",
      " for epoch : 192/500, loss is 0.6722978559157962\n",
      " for epoch : 193/500, loss is 0.6721860041613267\n",
      " for epoch : 194/500, loss is 0.6720759241227127\n",
      " for epoch : 195/500, loss is 0.6719675754051446\n",
      " for epoch : 196/500, loss is 0.6718609187359584\n",
      " for epoch : 197/500, loss is 0.6717559159301241\n",
      " for epoch : 198/500, loss is 0.6716525298568604\n",
      " for epoch : 199/500, loss is 0.6715507244073429\n",
      " for epoch : 200/500, loss is 0.6714504644634627\n",
      " for epoch : 201/500, loss is 0.6713517158676013\n",
      " for epoch : 202/500, loss is 0.6712544453933911\n",
      " for epoch : 203/500, loss is 0.6711586207174202\n",
      " for epoch : 204/500, loss is 0.6710642103918573\n",
      " for epoch : 205/500, loss is 0.6709711838179601\n",
      " for epoch : 206/500, loss is 0.6708795112204394\n",
      " for epoch : 207/500, loss is 0.6707891636226494\n",
      " for epoch : 208/500, loss is 0.6707001128225769\n",
      " for epoch : 209/500, loss is 0.6706123313696015\n",
      " for epoch : 210/500, loss is 0.6705257925420011\n",
      " for epoch : 211/500, loss is 0.6704404703251787\n",
      " for epoch : 212/500, loss is 0.6703563393905834\n",
      " for epoch : 213/500, loss is 0.6702733750753068\n",
      " for epoch : 214/500, loss is 0.6701915533623269\n",
      " for epoch : 215/500, loss is 0.670110850861382\n",
      " for epoch : 216/500, loss is 0.6700312447904518\n",
      " for epoch : 217/500, loss is 0.6699527129578253\n",
      " for epoch : 218/500, loss is 0.6698752337447368\n",
      " for epoch : 219/500, loss is 0.6697987860885495\n",
      " for epoch : 220/500, loss is 0.6697233494664718\n",
      " for epoch : 221/500, loss is 0.6696489038797832\n",
      " for epoch : 222/500, loss is 0.6695754298385587\n",
      " for epoch : 223/500, loss is 0.6695029083468733\n",
      " for epoch : 224/500, loss is 0.6694313208884666\n",
      " for epoch : 225/500, loss is 0.6693606494128584\n",
      " for epoch : 226/500, loss is 0.6692908763218994\n",
      " for epoch : 227/500, loss is 0.6692219844567371\n",
      " for epoch : 228/500, loss is 0.6691539570851902\n",
      " for epoch : 229/500, loss is 0.669086777889515\n",
      " for epoch : 230/500, loss is 0.6690204309545515\n",
      " for epoch : 231/500, loss is 0.6689549007562344\n",
      " for epoch : 232/500, loss is 0.6688901721504628\n",
      " for epoch : 233/500, loss is 0.6688262303623094\n",
      " for epoch : 234/500, loss is 0.6687630609755663\n",
      " for epoch : 235/500, loss is 0.6687006499226086\n",
      " for epoch : 236/500, loss is 0.6686389834745715\n",
      " for epoch : 237/500, loss is 0.6685780482318281\n",
      " for epoch : 238/500, loss is 0.6685178311147569\n",
      " for epoch : 239/500, loss is 0.6684583193547943\n",
      " for epoch : 240/500, loss is 0.6683995004857568\n",
      " for epoch : 241/500, loss is 0.6683413623354291\n",
      " for epoch : 242/500, loss is 0.6682838930174076\n",
      " for epoch : 243/500, loss is 0.6682270809231906\n",
      " for epoch : 244/500, loss is 0.6681709147145086\n",
      " for epoch : 245/500, loss is 0.6681153833158858\n",
      " for epoch : 246/500, loss is 0.668060475907427\n",
      " for epoch : 247/500, loss is 0.6680061819178205\n",
      " for epoch : 248/500, loss is 0.667952491017551\n",
      " for epoch : 249/500, loss is 0.667899393112319\n",
      " for epoch : 250/500, loss is 0.6678468783366538\n",
      " for epoch : 251/500, loss is 0.6677949370477184\n",
      " for epoch : 252/500, loss is 0.667743559819301\n",
      " for epoch : 253/500, loss is 0.6676927374359838\n",
      " for epoch : 254/500, loss is 0.6676424608874845\n",
      " for epoch : 255/500, loss is 0.6675927213631678\n",
      " for epoch : 256/500, loss is 0.667543510246718\n",
      " for epoch : 257/500, loss is 0.6674948191109691\n",
      " for epoch : 258/500, loss is 0.6674466397128881\n",
      " for epoch : 259/500, loss is 0.6673989639887054\n",
      " for epoch : 260/500, loss is 0.667351784049188\n",
      " for epoch : 261/500, loss is 0.6673050921750537\n",
      " for epoch : 262/500, loss is 0.6672588808125155\n",
      " for epoch : 263/500, loss is 0.6672131425689596\n",
      " for epoch : 264/500, loss is 0.6671678702087485\n",
      " for epoch : 265/500, loss is 0.6671230566491447\n",
      " for epoch : 266/500, loss is 0.6670786949563559\n",
      " for epoch : 267/500, loss is 0.6670347783416912\n",
      " for epoch : 268/500, loss is 0.666991300157831\n",
      " for epoch : 269/500, loss is 0.666948253895204\n",
      " for epoch : 270/500, loss is 0.6669056331784676\n",
      " for epoch : 271/500, loss is 0.6668634317630917\n",
      " for epoch : 272/500, loss is 0.6668216435320373\n",
      " for epoch : 273/500, loss is 0.666780262492533\n",
      " for epoch : 274/500, loss is 0.6667392827729419\n",
      " for epoch : 275/500, loss is 0.666698698619719\n",
      " for epoch : 276/500, loss is 0.6666585043944535\n",
      " for epoch : 277/500, loss is 0.6666186945709972\n",
      " for epoch : 278/500, loss is 0.6665792637326723\n",
      " for epoch : 279/500, loss is 0.6665402065695586\n",
      " for epoch : 280/500, loss is 0.6665015178758581\n",
      " for epoch : 281/500, loss is 0.6664631925473329\n",
      " for epoch : 282/500, loss is 0.6664252255788142\n",
      " for epoch : 283/500, loss is 0.6663876120617831\n",
      " for epoch : 284/500, loss is 0.6663503471820174\n",
      " for epoch : 285/500, loss is 0.6663134262173038\n",
      " for epoch : 286/500, loss is 0.6662768445352157\n",
      " for epoch : 287/500, loss is 0.66624059759095\n",
      " for epoch : 288/500, loss is 0.6662046809252257\n",
      " for epoch : 289/500, loss is 0.6661690901622394\n",
      " for epoch : 290/500, loss is 0.666133821007678\n",
      " for epoch : 291/500, loss is 0.6660988692467851\n",
      " for epoch : 292/500, loss is 0.6660642307424814\n",
      " for epoch : 293/500, loss is 0.6660299014335356\n",
      " for epoch : 294/500, loss is 0.6659958773327848\n",
      " for epoch : 295/500, loss is 0.6659621545254059\n",
      " for epoch : 296/500, loss is 0.6659287291672294\n",
      " for epoch : 297/500, loss is 0.6658955974831041\n",
      " for epoch : 298/500, loss is 0.6658627557653017\n",
      " for epoch : 299/500, loss is 0.6658302003719658\n",
      " for epoch : 300/500, loss is 0.6657979277256044\n",
      " for epoch : 301/500, loss is 0.6657659343116187\n",
      " for epoch : 302/500, loss is 0.6657342166768754\n",
      " for epoch : 303/500, loss is 0.6657027714283147\n",
      " for epoch : 304/500, loss is 0.665671595231596\n",
      " for epoch : 305/500, loss is 0.6656406848097786\n",
      " for epoch : 306/500, loss is 0.6656100369420387\n",
      " for epoch : 307/500, loss is 0.6655796484624193\n",
      " for epoch : 308/500, loss is 0.6655495162586132\n",
      " for epoch : 309/500, loss is 0.6655196372707773\n",
      " for epoch : 310/500, loss is 0.6654900084903799\n",
      " for epoch : 311/500, loss is 0.6654606269590744\n",
      " for epoch : 312/500, loss is 0.6654314897676062\n",
      " for epoch : 313/500, loss is 0.6654025940547458\n",
      " for epoch : 314/500, loss is 0.6653739370062492\n",
      " for epoch : 315/500, loss is 0.6653455158538468\n",
      " for epoch : 316/500, loss is 0.665317327874256\n",
      " for epoch : 317/500, loss is 0.6652893703882213\n",
      " for epoch : 318/500, loss is 0.6652616407595776\n",
      " for epoch : 319/500, loss is 0.665234136394337\n",
      " for epoch : 320/500, loss is 0.6652068547398009\n",
      " for epoch : 321/500, loss is 0.6651797932836918\n",
      " for epoch : 322/500, loss is 0.6651529495533087\n",
      " for epoch : 323/500, loss is 0.6651263211147037\n",
      " for epoch : 324/500, loss is 0.6650999055718788\n",
      " for epoch : 325/500, loss is 0.6650737005660028\n",
      " for epoch : 326/500, loss is 0.6650477037746477\n",
      " for epoch : 327/500, loss is 0.6650219129110444\n",
      " for epoch : 328/500, loss is 0.6649963257233567\n",
      " for epoch : 329/500, loss is 0.6649709399939729\n",
      " for epoch : 330/500, loss is 0.6649457535388152\n",
      " for epoch : 331/500, loss is 0.6649207642066661\n",
      " for epoch : 332/500, loss is 0.6648959698785113\n",
      " for epoch : 333/500, loss is 0.6648713684668971\n",
      " for epoch : 334/500, loss is 0.6648469579153065\n",
      " for epoch : 335/500, loss is 0.6648227361975473\n",
      " for epoch : 336/500, loss is 0.6647987013171576\n",
      " for epoch : 337/500, loss is 0.6647748513068226\n",
      " for epoch : 338/500, loss is 0.6647511842278083\n",
      " for epoch : 339/500, loss is 0.6647276981694075\n",
      " for epoch : 340/500, loss is 0.6647043912483982\n",
      " for epoch : 341/500, loss is 0.6646812616085165\n",
      " for epoch : 342/500, loss is 0.6646583074199403\n",
      " for epoch : 343/500, loss is 0.6646355268787867\n",
      " for epoch : 344/500, loss is 0.6646129182066203\n",
      " for epoch : 345/500, loss is 0.6645904796499729\n",
      " for epoch : 346/500, loss is 0.6645682094798747\n",
      " for epoch : 347/500, loss is 0.6645461059913969\n",
      " for epoch : 348/500, loss is 0.6645241675032035\n",
      " for epoch : 349/500, loss is 0.6645023923571158\n",
      " for epoch : 350/500, loss is 0.6644807789176836\n",
      " for epoch : 351/500, loss is 0.6644593255717697\n",
      " for epoch : 352/500, loss is 0.6644380307281404\n",
      " for epoch : 353/500, loss is 0.6644168928170702\n",
      " for epoch : 354/500, loss is 0.6643959102899485\n",
      " for epoch : 355/500, loss is 0.6643750816189022\n",
      " for epoch : 356/500, loss is 0.6643544052964231\n",
      " for epoch : 357/500, loss is 0.6643338798350028\n",
      " for epoch : 358/500, loss is 0.6643135037667787\n",
      " for epoch : 359/500, loss is 0.6642932756431865\n",
      " for epoch : 360/500, loss is 0.6642731940346188\n",
      " for epoch : 361/500, loss is 0.6642532575300946\n",
      " for epoch : 362/500, loss is 0.6642334647369326\n",
      " for epoch : 363/500, loss is 0.6642138142804344\n",
      " for epoch : 364/500, loss is 0.6641943048035736\n",
      " for epoch : 365/500, loss is 0.6641749349666906\n",
      " for epoch : 366/500, loss is 0.6641557034471957\n",
      " for epoch : 367/500, loss is 0.6641366089392778\n",
      " for epoch : 368/500, loss is 0.6641176501536195\n",
      " for epoch : 369/500, loss is 0.664098825817118\n",
      " for epoch : 370/500, loss is 0.664080134672613\n",
      " for epoch : 371/500, loss is 0.6640615754786188\n",
      " for epoch : 372/500, loss is 0.6640431470090633\n",
      " for epoch : 373/500, loss is 0.6640248480530323\n",
      " for epoch : 374/500, loss is 0.6640066774145189\n",
      " for epoch : 375/500, loss is 0.6639886339121781\n",
      " for epoch : 376/500, loss is 0.6639707163790866\n",
      " for epoch : 377/500, loss is 0.6639529236625082\n",
      " for epoch : 378/500, loss is 0.663935254623663\n",
      " for epoch : 379/500, loss is 0.6639177081375024\n",
      " for epoch : 380/500, loss is 0.6639002830924876\n",
      " for epoch : 381/500, loss is 0.6638829783903741\n",
      " for epoch : 382/500, loss is 0.6638657929459986\n",
      " for epoch : 383/500, loss is 0.6638487256870728\n",
      " for epoch : 384/500, loss is 0.6638317755539789\n",
      " for epoch : 385/500, loss is 0.6638149414995702\n",
      " for epoch : 386/500, loss is 0.663798222488977\n",
      " for epoch : 387/500, loss is 0.6637816174994137\n",
      " for epoch : 388/500, loss is 0.663765125519992\n",
      " for epoch : 389/500, loss is 0.663748745551537\n",
      " for epoch : 390/500, loss is 0.6637324766064067\n",
      " for epoch : 391/500, loss is 0.6637163177083152\n",
      " for epoch : 392/500, loss is 0.6637002678921596\n",
      " for epoch : 393/500, loss is 0.6636843262038512\n",
      " for epoch : 394/500, loss is 0.663668491700146\n",
      " for epoch : 395/500, loss is 0.6636527634484854\n",
      " for epoch : 396/500, loss is 0.6636371405268321\n",
      " for epoch : 397/500, loss is 0.6636216220235159\n",
      " for epoch : 398/500, loss is 0.6636062070370784\n",
      " for epoch : 399/500, loss is 0.6635908946761212\n",
      " for epoch : 400/500, loss is 0.6635756840591592\n",
      " for epoch : 401/500, loss is 0.6635605743144738\n",
      " for epoch : 402/500, loss is 0.6635455645799709\n",
      " for epoch : 403/500, loss is 0.6635306540030403\n",
      " for epoch : 404/500, loss is 0.663515841740419\n",
      " for epoch : 405/500, loss is 0.663501126958055\n",
      " for epoch : 406/500, loss is 0.663486508830976\n",
      " for epoch : 407/500, loss is 0.6634719865431599\n",
      " for epoch : 408/500, loss is 0.6634575592874057\n",
      " for epoch : 409/500, loss is 0.6634432262652094\n",
      " for epoch : 410/500, loss is 0.6634289866866407\n",
      " for epoch : 411/500, loss is 0.6634148397702224\n",
      " for epoch : 412/500, loss is 0.6634007847428118\n",
      " for epoch : 413/500, loss is 0.6633868208394846\n",
      " for epoch : 414/500, loss is 0.6633729473034201\n",
      " for epoch : 415/500, loss is 0.6633591633857894\n",
      " for epoch : 416/500, loss is 0.6633454683456447\n",
      " for epoch : 417/500, loss is 0.6633318614498118\n",
      " for epoch : 418/500, loss is 0.6633183419727827\n",
      " for epoch : 419/500, loss is 0.663304909196612\n",
      " for epoch : 420/500, loss is 0.6632915624108136\n",
      " for epoch : 421/500, loss is 0.6632783009122599\n",
      " for epoch : 422/500, loss is 0.6632651240050835\n",
      " for epoch : 423/500, loss is 0.6632520310005784\n",
      " for epoch : 424/500, loss is 0.6632390212171052\n",
      " for epoch : 425/500, loss is 0.663226093979997\n",
      " for epoch : 426/500, loss is 0.6632132486214668\n",
      " for epoch : 427/500, loss is 0.663200484480516\n",
      " for epoch : 428/500, loss is 0.6631878009028465\n",
      " for epoch : 429/500, loss is 0.6631751972407709\n",
      " for epoch : 430/500, loss is 0.663162672853128\n",
      " for epoch : 431/500, loss is 0.663150227105197\n",
      " for epoch : 432/500, loss is 0.6631378593686137\n",
      " for epoch : 433/500, loss is 0.6631255690212892\n",
      " for epoch : 434/500, loss is 0.6631133554473284\n",
      " for epoch : 435/500, loss is 0.6631012180369513\n",
      " for epoch : 436/500, loss is 0.6630891561864147\n",
      " for epoch : 437/500, loss is 0.6630771692979351\n",
      " for epoch : 438/500, loss is 0.6630652567796129\n",
      " for epoch : 439/500, loss is 0.6630534180453594\n",
      " for epoch : 440/500, loss is 0.6630416525148219\n",
      " for epoch : 441/500, loss is 0.6630299596133129\n",
      " for epoch : 442/500, loss is 0.6630183387717393\n",
      " for epoch : 443/500, loss is 0.6630067894265322\n",
      " for epoch : 444/500, loss is 0.6629953110195789\n",
      " for epoch : 445/500, loss is 0.6629839029981547\n",
      " for epoch : 446/500, loss is 0.6629725648148576\n",
      " for epoch : 447/500, loss is 0.6629612959275423\n",
      " for epoch : 448/500, loss is 0.6629500957992556\n",
      " for epoch : 449/500, loss is 0.6629389638981741\n",
      " for epoch : 450/500, loss is 0.6629278996975405\n",
      " for epoch : 451/500, loss is 0.662916902675604\n",
      " for epoch : 452/500, loss is 0.6629059723155581\n",
      " for epoch : 453/500, loss is 0.6628951081054826\n",
      " for epoch : 454/500, loss is 0.6628843095382838\n",
      " for epoch : 455/500, loss is 0.6628735761116387\n",
      " for epoch : 456/500, loss is 0.6628629073279355\n",
      " for epoch : 457/500, loss is 0.6628523026942205\n",
      " for epoch : 458/500, loss is 0.6628417617221413\n",
      " for epoch : 459/500, loss is 0.662831283927893\n",
      " for epoch : 460/500, loss is 0.6628208688321647\n",
      " for epoch : 461/500, loss is 0.6628105159600876\n",
      " for epoch : 462/500, loss is 0.6628002248411825\n",
      " for epoch : 463/500, loss is 0.6627899950093081\n",
      " for epoch : 464/500, loss is 0.6627798260026125\n",
      " for epoch : 465/500, loss is 0.6627697173634821\n",
      " for epoch : 466/500, loss is 0.662759668638493\n",
      " for epoch : 467/500, loss is 0.6627496793783639\n",
      " for epoch : 468/500, loss is 0.6627397491379076\n",
      " for epoch : 469/500, loss is 0.6627298774759844\n",
      " for epoch : 470/500, loss is 0.6627200639554569\n",
      " for epoch : 471/500, loss is 0.6627103081431435\n",
      " for epoch : 472/500, loss is 0.6627006096097746\n",
      " for epoch : 473/500, loss is 0.6626909679299482\n",
      " for epoch : 474/500, loss is 0.6626813826820858\n",
      " for epoch : 475/500, loss is 0.6626718534483911\n",
      " for epoch : 476/500, loss is 0.6626623798148061\n",
      " for epoch : 477/500, loss is 0.6626529613709705\n",
      " for epoch : 478/500, loss is 0.6626435977101804\n",
      " for epoch : 479/500, loss is 0.6626342884293482\n",
      " for epoch : 480/500, loss is 0.6626250331289615\n",
      " for epoch : 481/500, loss is 0.6626158314130455\n",
      " for epoch : 482/500, loss is 0.6626066828891226\n",
      " for epoch : 483/500, loss is 0.6625975871681755\n",
      " for epoch : 484/500, loss is 0.6625885438646086\n",
      " for epoch : 485/500, loss is 0.6625795525962112\n",
      " for epoch : 486/500, loss is 0.66257061298412\n",
      " for epoch : 487/500, loss is 0.6625617246527844\n",
      " for epoch : 488/500, loss is 0.6625528872299296\n",
      " for epoch : 489/500, loss is 0.6625441003465219\n",
      " for epoch : 490/500, loss is 0.6625353636367334\n",
      " for epoch : 491/500, loss is 0.662526676737909\n",
      " for epoch : 492/500, loss is 0.6625180392905311\n",
      " for epoch : 493/500, loss is 0.6625094509381869\n",
      " for epoch : 494/500, loss is 0.662500911327536\n",
      " for epoch : 495/500, loss is 0.6624924201082768\n",
      " for epoch : 496/500, loss is 0.6624839769331153\n",
      " for epoch : 497/500, loss is 0.6624755814577338\n",
      " for epoch : 498/500, loss is 0.6624672333407585\n",
      " for epoch : 499/500, loss is 0.6624589322437296\n"
     ]
    }
   ],
   "source": [
    "# creating model\n",
    "model = SimpleNN(X_train_tensor)\n",
    "\n",
    "print(model.bias)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # forward pass\n",
    "    \n",
    "    y_pred = model.forward_pass(X_train_tensor)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = model.loss_calculation(y_pred, y_train_tensor)\n",
    "    print(f\" for epoch : {epoch}/{epochs}, loss is {loss.item()}\")\n",
    "    \n",
    "    # backward pass\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    # parameters update\n",
    "    with torch.no_grad():\n",
    "        model.weights -= learning_rate * model.weights.grad\n",
    "        model.bias -= learning_rate * model.bias.grad\n",
    "\n",
    "    # zero gradientsy_train_tensor\n",
    "    model.weights.grad.zero_()\n",
    "    model.bias.grad.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe591585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 63.16%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():oss is 0.9898135662078857\n",
    " for epoch : 3/500, loss is 0.989813566207885\n",
    "    y_test_pred = model.forward_pass(X_test_tensor)\n",
    "    y_test_pred_labels = (y_test_pred >= 0.5).float()\n",
    "    \n",
    "    accuracy = (y_test_pred_labels.squeeze() == y_test_tensor).float().mean()\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy.item()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eace08d",
   "metadata": {},
   "source": [
    "# Using Torch's NN Module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8dd598aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b70e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features, hidden_layer1_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.hidden_layer1_size = hidden_layer1_size\n",
    "        \n",
    "        self.neural_network = nn.Sequential(\n",
    "            nn.Linear(num_features, hidden_layer1_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer1_size, 1),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, features):\n",
    "       \n",
    "        activation2_out = self.neural_network(features)\n",
    "\n",
    "        return activation2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd329772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([455, 30])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e97e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5344721674919128\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hidden_layer_size = 15\n",
    "learning_rate = 0.05\n",
    "epochs = 5000\n",
    "\n",
    "#loss_function = nn.BCELoss() # binary cross entropy loss\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = Model(X_train_tensor.shape[1], hidden_layer_size)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "# model\n",
    "\n",
    "    # internally NN Module calls forward method\n",
    "    y_pred = model(X_train_tensor.float()) # same as model.forward(features)\n",
    "    loss = loss_function(y_pred, y_train_tensor.float().unsqueeze(1))\n",
    "\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #print(f\" for epoch : {epoch}/{epochs}, loss is {loss.item()}\")\n",
    "\n",
    "\n",
    "y_pred_f = model(X_test_tensor.float())\n",
    "\n",
    "y_pred_f = (y_pred_f >= 0.5).float()\n",
    "\n",
    "accuracy = (y_pred_f == y_test_tensor).float().mean()\n",
    "\n",
    "print(accuracy.item())\n",
    "\n",
    "\n",
    "    # print(y_pred)\n",
    "# 0.5323176383972168\n",
    "\n",
    "# 0.5280086398124695 - ADAM\n",
    "# 0.5344721674919128 - SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85c11451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5323176383972168\n"
     ]
    }
   ],
   "source": [
    "y_pred = model(X_test_tensor.float())\n",
    "\n",
    "y_pred = (y_pred >= 0.5).float()\n",
    "\n",
    "accuracy = (y_pred == y_test_tensor).float().mean()\n",
    "\n",
    "print(accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab156d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
