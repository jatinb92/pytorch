{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53989a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello WOrld!!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello WOrld!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adf6bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install numpy\n",
    "# ! pip install pandas\n",
    "# ! pip install torch\n",
    "\n",
    "# ! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ff00b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f918527b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 33)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d492549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d556c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['id', 'Unnamed: 32'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f6590d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0         M        17.99         10.38          122.80     1001.0   \n",
       "1         M        20.57         17.77          132.90     1326.0   \n",
       "2         M        19.69         21.25          130.00     1203.0   \n",
       "3         M        11.42         20.38           77.58      386.1   \n",
       "4         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "4         0.1809  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f608f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:], df.iloc[:, 0], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7afd1563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30)\n",
      "(569, 31)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce8489b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bcb38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_test_encoded = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ec4fe",
   "metadata": {},
   "source": [
    "Numpy Arrays to PyTorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "379c2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train_scaled)\n",
    "X_test_tensor = torch.from_numpy(X_test_scaled)\n",
    "y_train_tensor = torch.from_numpy(y_train_encoded)\n",
    "y_test_tensor = torch.from_numpy(y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96dde298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4408, -0.4353, -1.3621,  ...,  0.9320,  2.0972,  1.8865],\n",
       "        [ 1.9741,  1.7330,  2.0917,  ...,  2.6989,  1.8912,  2.4978],\n",
       "        [-1.4000, -1.2496, -1.3452,  ..., -0.9702,  0.5976,  0.0579],\n",
       "        ...,\n",
       "        [ 0.0488, -0.5550, -0.0651,  ..., -1.2390, -0.7086, -1.2715],\n",
       "        [-0.0390,  0.1021, -0.0314,  ...,  1.0500,  0.4343,  1.2134],\n",
       "        [-0.5486,  0.3133, -0.6035,  ..., -0.6110, -0.3345, -0.8463]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd18c07f",
   "metadata": {},
   "source": [
    "Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a373a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN():\n",
    "    \n",
    "    def __init__(self, X):\n",
    "        self.weights = torch.rand(X.shape[1], 1, dtype=torch.float64, requires_grad=True)\n",
    "        self.bias = torch.zeros(1, dtype=torch.float64, requires_grad=True)\n",
    "        \n",
    "    def forward_pass(self, X):\n",
    "        y = torch.matmul(X, self.weights) + self.bias\n",
    "        \n",
    "        y_pred = torch.sigmoid(y)\n",
    "        return y_pred\n",
    "        \n",
    "        \n",
    "    def loss_calculation(self, y_pred, y_true):\n",
    "        epsilon = 1e-7\n",
    "        \n",
    "        y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        loss = -(y_true*torch.log(y_pred) + (1-y_true)*torch.log(1-y_pred)).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    " \n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc47a8",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a873ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4581e765",
   "metadata": {},
   "source": [
    "Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a0f5571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], dtype=torch.float64, requires_grad=True)\n",
      " for epoch : 0/500, loss is 3.6124326274773275\n",
      " for epoch : 1/500, loss is 3.5481924334424\n",
      " for epoch : 2/500, loss is 3.481123478940542\n",
      " for epoch : 3/500, loss is 3.413144508125074\n",
      " for epoch : 4/500, loss is 3.3445658976395145\n",
      " for epoch : 5/500, loss is 3.2747331597003995\n",
      " for epoch : 6/500, loss is 3.20409797148116\n",
      " for epoch : 7/500, loss is 3.132289500569909\n",
      " for epoch : 8/500, loss is 3.0586889685041334\n",
      " for epoch : 9/500, loss is 2.9840201261481867\n",
      " for epoch : 10/500, loss is 2.907285372582708\n",
      " for epoch : 11/500, loss is 2.8293071056791237\n",
      " for epoch : 12/500, loss is 2.748155963581068\n",
      " for epoch : 13/500, loss is 2.6677690838747163\n",
      " for epoch : 14/500, loss is 2.5876000524215184\n",
      " for epoch : 15/500, loss is 2.507362485939635\n",
      " for epoch : 16/500, loss is 2.4281085883886417\n",
      " for epoch : 17/500, loss is 2.3488000400776885\n",
      " for epoch : 18/500, loss is 2.267195003865613\n",
      " for epoch : 19/500, loss is 2.1860416232556377\n",
      " for epoch : 20/500, loss is 2.1049401744884264\n",
      " for epoch : 21/500, loss is 2.0254282609402168\n",
      " for epoch : 22/500, loss is 1.9473356612571135\n",
      " for epoch : 23/500, loss is 1.8697687802909941\n",
      " for epoch : 24/500, loss is 1.794279664700961\n",
      " for epoch : 25/500, loss is 1.7187803839561833\n",
      " for epoch : 26/500, loss is 1.6434653511844002\n",
      " for epoch : 27/500, loss is 1.569538887166859\n",
      " for epoch : 28/500, loss is 1.4987073767745995\n",
      " for epoch : 29/500, loss is 1.4311896814241563\n",
      " for epoch : 30/500, loss is 1.367195716915602\n",
      " for epoch : 31/500, loss is 1.3069191168971634\n",
      " for epoch : 32/500, loss is 1.2505286527023067\n",
      " for epoch : 33/500, loss is 1.1981580889608903\n",
      " for epoch : 34/500, loss is 1.1498944588846902\n",
      " for epoch : 35/500, loss is 1.105765914500832\n",
      " for epoch : 36/500, loss is 1.065732013510696\n",
      " for epoch : 37/500, loss is 1.029680097038969\n",
      " for epoch : 38/500, loss is 0.9974298970251795\n",
      " for epoch : 39/500, loss is 0.9687453217957387\n",
      " for epoch : 40/500, loss is 0.9433499807156162\n",
      " for epoch : 41/500, loss is 0.9209430294053252\n",
      " for epoch : 42/500, loss is 0.9012135405347801\n",
      " for epoch : 43/500, loss is 0.8838529998255271\n",
      " for epoch : 44/500, loss is 0.8685658221741568\n",
      " for epoch : 45/500, loss is 0.8550774827548941\n",
      " for epoch : 46/500, loss is 0.8431398093542695\n",
      " for epoch : 47/500, loss is 0.8325333961151706\n",
      " for epoch : 48/500, loss is 0.8230676116838501\n",
      " for epoch : 49/500, loss is 0.8145789453251736\n",
      " for epoch : 50/500, loss is 0.8069284197069374\n",
      " for epoch : 51/500, loss is 0.7999986294929762\n",
      " for epoch : 52/500, loss is 0.7936907687667155\n",
      " for epoch : 53/500, loss is 0.7879218511112442\n",
      " for epoch : 54/500, loss is 0.7826222175939272\n",
      " for epoch : 55/500, loss is 0.7777333618535496\n",
      " for epoch : 56/500, loss is 0.7732060651192559\n",
      " for epoch : 57/500, loss is 0.7689988162632609\n",
      " for epoch : 58/500, loss is 0.7650764850643467\n",
      " for epoch : 59/500, loss is 0.7614092157121255\n",
      " for epoch : 60/500, loss is 0.7579715092369668\n",
      " for epoch : 61/500, loss is 0.7547414663762356\n",
      " for epoch : 62/500, loss is 0.7517001655627183\n",
      " for epoch : 63/500, loss is 0.7488311538697933\n",
      " for epoch : 64/500, loss is 0.7461200317155406\n",
      " for epoch : 65/500, loss is 0.7435541148580677\n",
      " for epoch : 66/500, loss is 0.7411221596888765\n",
      " for epoch : 67/500, loss is 0.7388141400461545\n",
      " for epoch : 68/500, loss is 0.7366210657277901\n",
      " for epoch : 69/500, loss is 0.7345348345919785\n",
      " for epoch : 70/500, loss is 0.7325481116035604\n",
      " for epoch : 71/500, loss is 0.7306542294332514\n",
      " for epoch : 72/500, loss is 0.7288471062646551\n",
      " for epoch : 73/500, loss is 0.7271211773323984\n",
      " for epoch : 74/500, loss is 0.7254713374266996\n",
      " for epoch : 75/500, loss is 0.7238928921774223\n",
      " for epoch : 76/500, loss is 0.7223815163951439\n",
      " for epoch : 77/500, loss is 0.7209332181170257\n",
      " for epoch : 78/500, loss is 0.7195443072981339\n",
      " for epoch : 79/500, loss is 0.7182113683188953\n",
      " for epoch : 80/500, loss is 0.7169312356589324\n",
      " for epoch : 81/500, loss is 0.7157009722269071\n",
      " for epoch : 82/500, loss is 0.7145178499437047\n",
      " for epoch : 83/500, loss is 0.7133793322591884\n",
      " for epoch : 84/500, loss is 0.7122830583464056\n",
      " for epoch : 85/500, loss is 0.7112268287659151\n",
      " for epoch : 86/500, loss is 0.7102085924303285\n",
      " for epoch : 87/500, loss is 0.709226434727904\n",
      " for epoch : 88/500, loss is 0.7082785666862368\n",
      " for epoch : 89/500, loss is 0.7073633150743264\n",
      " for epoch : 90/500, loss is 0.7064791133548532\n",
      " for epoch : 91/500, loss is 0.7056244934092561\n",
      " for epoch : 92/500, loss is 0.7047980779668952\n",
      " for epoch : 93/500, loss is 0.7039985736767311\n",
      " for epoch : 94/500, loss is 0.703224764765927\n",
      " for epoch : 95/500, loss is 0.7024755072348892\n",
      " for epoch : 96/500, loss is 0.701749723542698\n",
      " for epoch : 97/500, loss is 0.7010463977407981\n",
      " for epoch : 98/500, loss is 0.7003645710163301\n",
      " for epoch : 99/500, loss is 0.6997033376096687\n",
      " for epoch : 100/500, loss is 0.6990618410736299\n",
      " for epoch : 101/500, loss is 0.6984392708444945\n",
      " for epoch : 102/500, loss is 0.6978348590974512\n",
      " for epoch : 103/500, loss is 0.6972478778613487\n",
      " for epoch : 104/500, loss is 0.696677636369767\n",
      " for epoch : 105/500, loss is 0.6961234786273666\n",
      " for epoch : 106/500, loss is 0.6955847811723029\n",
      " for epoch : 107/500, loss is 0.6950609510171578\n",
      " for epoch : 108/500, loss is 0.6945514237524071\n",
      " for epoch : 109/500, loss is 0.6940556617978568\n",
      " for epoch : 110/500, loss is 0.6935731527888106\n",
      " for epoch : 111/500, loss is 0.6931034080849316\n",
      " for epoch : 112/500, loss is 0.6926459613908743\n",
      " for epoch : 113/500, loss is 0.6922003674787774\n",
      " for epoch : 114/500, loss is 0.6917662010036315\n",
      " for epoch : 115/500, loss is 0.6913430554033849\n",
      " for epoch : 116/500, loss is 0.6909305418764201\n",
      " for epoch : 117/500, loss is 0.6905282884297266\n",
      " for epoch : 118/500, loss is 0.6901359389917389\n",
      " for epoch : 119/500, loss is 0.6897531525843787\n",
      " for epoch : 120/500, loss is 0.6893796025493607\n",
      " for epoch : 121/500, loss is 0.6890149758242907\n",
      " for epoch : 122/500, loss is 0.6886589722645174\n",
      " for epoch : 123/500, loss is 0.6883113040070686\n",
      " for epoch : 124/500, loss is 0.6879716948733646\n",
      " for epoch : 125/500, loss is 0.6876398798076955\n",
      " for epoch : 126/500, loss is 0.6873156043487445\n",
      " for epoch : 127/500, loss is 0.6869986241316823\n",
      " for epoch : 128/500, loss is 0.6866887044185848\n",
      " for epoch : 129/500, loss is 0.6863856196551344\n",
      " for epoch : 130/500, loss is 0.6860891530517453\n",
      " for epoch : 131/500, loss is 0.6857990961874176\n",
      " for epoch : 132/500, loss is 0.6855152486347764\n",
      " for epoch : 133/500, loss is 0.6852374176048786\n",
      " for epoch : 134/500, loss is 0.6849654176104986\n",
      " for epoch : 135/500, loss is 0.6846990701467046\n",
      " for epoch : 136/500, loss is 0.6844382033876397\n",
      " for epoch : 137/500, loss is 0.6841826518985032\n",
      " for epoch : 138/500, loss is 0.6839322563618159\n",
      " for epoch : 139/500, loss is 0.6836868633171118\n",
      " for epoch : 140/500, loss is 0.6834463249132797\n",
      " for epoch : 141/500, loss is 0.6832104986728159\n",
      " for epoch : 142/500, loss is 0.6829792472673233\n",
      " for epoch : 143/500, loss is 0.6827524383036249\n",
      " for epoch : 144/500, loss is 0.6825299441199087\n",
      " for epoch : 145/500, loss is 0.6823116415913647\n",
      " for epoch : 146/500, loss is 0.6820974119448011\n",
      " for epoch : 147/500, loss is 0.6818871405817726\n",
      " for epoch : 148/500, loss is 0.6816807169097707\n",
      " for epoch : 149/500, loss is 0.6814780341810672\n",
      " for epoch : 150/500, loss is 0.6812789893388147\n",
      " for epoch : 151/500, loss is 0.6810834828700422\n",
      " for epoch : 152/500, loss is 0.6808914186651981\n",
      " for epoch : 153/500, loss is 0.6807027038839192\n",
      " for epoch : 154/500, loss is 0.6805172488267169\n",
      " for epoch : 155/500, loss is 0.6803349668122922\n",
      " for epoch : 156/500, loss is 0.6801557740602092\n",
      " for epoch : 157/500, loss is 0.6799795895786676\n",
      " for epoch : 158/500, loss is 0.6798063350571302\n",
      " for epoch : 159/500, loss is 0.6796359347635758\n",
      " for epoch : 160/500, loss is 0.6794683154461586\n",
      " for epoch : 161/500, loss is 0.6793034062390667\n",
      " for epoch : 162/500, loss is 0.6791411385723835\n",
      " for epoch : 163/500, loss is 0.6789814460857655\n",
      " for epoch : 164/500, loss is 0.6788242645457606\n",
      " for epoch : 165/500, loss is 0.6786695317665984\n",
      " for epoch : 166/500, loss is 0.678517187534291\n",
      " for epoch : 167/500, loss is 0.6783671735338963\n",
      " for epoch : 168/500, loss is 0.6782194332797976\n",
      " for epoch : 169/500, loss is 0.6780739120488622\n",
      " for epoch : 170/500, loss is 0.67793055681635\n",
      " for epoch : 171/500, loss is 0.6777893161944457\n",
      " for epoch : 172/500, loss is 0.6776501403733\n",
      " for epoch : 173/500, loss is 0.6775129810644653\n",
      " for epoch : 174/500, loss is 0.6773777914466187\n",
      " for epoch : 175/500, loss is 0.6772445261134714\n",
      " for epoch : 176/500, loss is 0.6771131410237672\n",
      " for epoch : 177/500, loss is 0.6769835934532789\n",
      " for epoch : 178/500, loss is 0.6768558419487113\n",
      " for epoch : 179/500, loss is 0.6767298462834327\n",
      " for epoch : 180/500, loss is 0.676605567414947\n",
      " for epoch : 181/500, loss is 0.6764829674440396\n",
      " for epoch : 182/500, loss is 0.6763620095755164\n",
      " for epoch : 183/500, loss is 0.6762426580804723\n",
      " for epoch : 184/500, loss is 0.67612487826002\n",
      " for epoch : 185/500, loss is 0.6760086364104164\n",
      " for epoch : 186/500, loss is 0.6758938997895313\n",
      " for epoch : 187/500, loss is 0.6757806365845929\n",
      " for epoch : 188/500, loss is 0.6756688158811641\n",
      " for epoch : 189/500, loss is 0.6755584076332917\n",
      " for epoch : 190/500, loss is 0.6754493826347805\n",
      " for epoch : 191/500, loss is 0.6753417124915475\n",
      " for epoch : 192/500, loss is 0.6752353695950072\n",
      " for epoch : 193/500, loss is 0.6751303270964459\n",
      " for epoch : 194/500, loss is 0.6750265588823475\n",
      " for epoch : 195/500, loss is 0.6749240395506251\n",
      " for epoch : 196/500, loss is 0.6748227443877262\n",
      " for epoch : 197/500, loss is 0.6747226493465734\n",
      " for epoch : 198/500, loss is 0.6746237310253071\n",
      " for epoch : 199/500, loss is 0.6745259666467963\n",
      " for epoch : 200/500, loss is 0.674429334038888\n",
      " for epoch : 201/500, loss is 0.674333811615365\n",
      " for epoch : 202/500, loss is 0.6742393783575814\n",
      " for epoch : 203/500, loss is 0.6741460137967525\n",
      " for epoch : 204/500, loss is 0.6740536979968704\n",
      " for epoch : 205/500, loss is 0.6739624115382192\n",
      " for epoch : 206/500, loss is 0.6738721355014712\n",
      " for epoch : 207/500, loss is 0.6737828514523343\n",
      " for epoch : 208/500, loss is 0.6736945414267353\n",
      " for epoch : 209/500, loss is 0.6736071879165146\n",
      " for epoch : 210/500, loss is 0.6735207738556127\n",
      " for epoch : 211/500, loss is 0.6734352826067308\n",
      " for epoch : 212/500, loss is 0.6733506979484463\n",
      " for epoch : 213/500, loss is 0.6732670040627671\n",
      " for epoch : 214/500, loss is 0.6731841855231039\n",
      " for epoch : 215/500, loss is 0.673102227282652\n",
      " for epoch : 216/500, loss is 0.6730211146631584\n",
      " for epoch : 217/500, loss is 0.6729408333440667\n",
      " for epoch : 218/500, loss is 0.6728613693520223\n",
      " for epoch : 219/500, loss is 0.6727827090507256\n",
      " for epoch : 220/500, loss is 0.6727048391311174\n",
      " for epoch : 221/500, loss is 0.6726277466018913\n",
      " for epoch : 222/500, loss is 0.6725514187803107\n",
      " for epoch : 223/500, loss is 0.6724758432833302\n",
      " for epoch : 224/500, loss is 0.6724010080190018\n",
      " for epoch : 225/500, loss is 0.6723269011781596\n",
      " for epoch : 226/500, loss is 0.6722535112263729\n",
      " for epoch : 227/500, loss is 0.6721808268961563\n",
      " for epoch : 228/500, loss is 0.6721088371794293\n",
      " for epoch : 229/500, loss is 0.6720375313202153\n",
      " for epoch : 230/500, loss is 0.6719668988075724\n",
      " for epoch : 231/500, loss is 0.671896929368748\n",
      " for epoch : 232/500, loss is 0.6718276129625466\n",
      " for epoch : 233/500, loss is 0.671758939772908\n",
      " for epoch : 234/500, loss is 0.6716909002026851\n",
      " for epoch : 235/500, loss is 0.6716234848676139\n",
      " for epoch : 236/500, loss is 0.6715566845904736\n",
      " for epoch : 237/500, loss is 0.6714904903954235\n",
      " for epoch : 238/500, loss is 0.6714248935025172\n",
      " for epoch : 239/500, loss is 0.6713598853223832\n",
      " for epoch : 240/500, loss is 0.6712954574510689\n",
      " for epoch : 241/500, loss is 0.6712316016650425\n",
      " for epoch : 242/500, loss is 0.6711683099163445\n",
      " for epoch : 243/500, loss is 0.6711055743278882\n",
      " for epoch : 244/500, loss is 0.671043387188901\n",
      " for epoch : 245/500, loss is 0.6709817409505021\n",
      " for epoch : 246/500, loss is 0.6709206282214142\n",
      " for epoch : 247/500, loss is 0.6708600417638025\n",
      " for epoch : 248/500, loss is 0.670799974489237\n",
      " for epoch : 249/500, loss is 0.6707404194547764\n",
      " for epoch : 250/500, loss is 0.6706813698591659\n",
      " for epoch : 251/500, loss is 0.6706228190391491\n",
      " for epoch : 252/500, loss is 0.6705647604658866\n",
      " for epoch : 253/500, loss is 0.6705071877414807\n",
      " for epoch : 254/500, loss is 0.6704500945956019\n",
      " for epoch : 255/500, loss is 0.6703934748822132\n",
      " for epoch : 256/500, loss is 0.6703373225763892\n",
      " for epoch : 257/500, loss is 0.6702816317712288\n",
      " for epoch : 258/500, loss is 0.670226396674856\n",
      " for epoch : 259/500, loss is 0.6701716116075067\n",
      " for epoch : 260/500, loss is 0.6701172709987014\n",
      " for epoch : 261/500, loss is 0.6700633693844965\n",
      " for epoch : 262/500, loss is 0.670009901404815\n",
      " for epoch : 263/500, loss is 0.6699568618008542\n",
      " for epoch : 264/500, loss is 0.6699042454125659\n",
      " for epoch : 265/500, loss is 0.6698520471762083\n",
      " for epoch : 266/500, loss is 0.6698002621219676\n",
      " for epoch : 267/500, loss is 0.6697488853716458\n",
      " for epoch : 268/500, loss is 0.669697912136415\n",
      " for epoch : 269/500, loss is 0.6696473377146326\n",
      " for epoch : 270/500, loss is 0.6695971574897196\n",
      " for epoch : 271/500, loss is 0.669547366928097\n",
      " for epoch : 272/500, loss is 0.6694979615771801\n",
      " for epoch : 273/500, loss is 0.6694489370634273\n",
      " for epoch : 274/500, loss is 0.6694002890904461\n",
      " for epoch : 275/500, loss is 0.6693520134371468\n",
      " for epoch : 276/500, loss is 0.6693041059559508\n",
      " for epoch : 277/500, loss is 0.6692565625710475\n",
      " for epoch : 278/500, loss is 0.6692093792766969\n",
      " for epoch : 279/500, loss is 0.669162552135581\n",
      " for epoch : 280/500, loss is 0.6691160772771992\n",
      " for epoch : 281/500, loss is 0.6690699508963075\n",
      " for epoch : 282/500, loss is 0.6690241692513995\n",
      " for epoch : 283/500, loss is 0.66897872866323\n",
      " for epoch : 284/500, loss is 0.668933625513377\n",
      " for epoch : 285/500, loss is 0.6688888562428429\n",
      " for epoch : 286/500, loss is 0.6688444173506939\n",
      " for epoch : 287/500, loss is 0.6688003053927355\n",
      " for epoch : 288/500, loss is 0.6687565169802233\n",
      " for epoch : 289/500, loss is 0.6687130487786078\n",
      " for epoch : 290/500, loss is 0.6686698975063144\n",
      " for epoch : 291/500, loss is 0.6686270599335535\n",
      " for epoch : 292/500, loss is 0.6685845328811639\n",
      " for epoch : 293/500, loss is 0.6685423132194859\n",
      " for epoch : 294/500, loss is 0.6685003978672643\n",
      " for epoch : 295/500, loss is 0.6684587837905814\n",
      " for epoch : 296/500, loss is 0.6684174680018151\n",
      " for epoch : 297/500, loss is 0.6683764475586278\n",
      " for epoch : 298/500, loss is 0.6683357195629799\n",
      " for epoch : 299/500, loss is 0.6682952811601687\n",
      " for epoch : 300/500, loss is 0.6682551295378932\n",
      " for epoch : 301/500, loss is 0.6682152619253432\n",
      " for epoch : 302/500, loss is 0.668175675592311\n",
      " for epoch : 303/500, loss is 0.6681363678483271\n",
      " for epoch : 304/500, loss is 0.6680973360418179\n",
      " for epoch : 305/500, loss is 0.6680585775592843\n",
      " for epoch : 306/500, loss is 0.668020089824503\n",
      " for epoch : 307/500, loss is 0.6679818702977464\n",
      " for epoch : 308/500, loss is 0.6679439164750233\n",
      " for epoch : 309/500, loss is 0.6679062258873398\n",
      " for epoch : 310/500, loss is 0.6678687960999767\n",
      " for epoch : 311/500, loss is 0.667831624711788\n",
      " for epoch : 312/500, loss is 0.6677947093545141\n",
      " for epoch : 313/500, loss is 0.6677580476921159\n",
      " for epoch : 314/500, loss is 0.6677216374201215\n",
      " for epoch : 315/500, loss is 0.6676854762649936\n",
      " for epoch : 316/500, loss is 0.6676495619835096\n",
      " for epoch : 317/500, loss is 0.6676138923621573\n",
      " for epoch : 318/500, loss is 0.6675784652165488\n",
      " for epoch : 319/500, loss is 0.6675432783908444\n",
      " for epoch : 320/500, loss is 0.6675083297571943\n",
      " for epoch : 321/500, loss is 0.6674736172151932\n",
      " for epoch : 322/500, loss is 0.667439138691347\n",
      " for epoch : 323/500, loss is 0.6674048921385541\n",
      " for epoch : 324/500, loss is 0.6673708755355995\n",
      " for epoch : 325/500, loss is 0.66733708688666\n",
      " for epoch : 326/500, loss is 0.667303524220822\n",
      " for epoch : 327/500, loss is 0.6672701855916123\n",
      " for epoch : 328/500, loss is 0.6672370690765387\n",
      " for epoch : 329/500, loss is 0.6672041727766413\n",
      " for epoch : 330/500, loss is 0.6671714948160574\n",
      " for epoch : 331/500, loss is 0.6671390333415934\n",
      " for epoch : 332/500, loss is 0.6671067865223094\n",
      " for epoch : 333/500, loss is 0.6670747525491126\n",
      " for epoch : 334/500, loss is 0.6670429296343611\n",
      " for epoch : 335/500, loss is 0.6670113160114766\n",
      " for epoch : 336/500, loss is 0.6669799099345661\n",
      " for epoch : 337/500, loss is 0.6669487096780535\n",
      " for epoch : 338/500, loss is 0.6669177135363191\n",
      " for epoch : 339/500, loss is 0.6668869198233472\n",
      " for epoch : 340/500, loss is 0.6668563268723832\n",
      " for epoch : 341/500, loss is 0.666825933035598\n",
      " for epoch : 342/500, loss is 0.6667957366837596\n",
      " for epoch : 343/500, loss is 0.6667657362059138\n",
      " for epoch : 344/500, loss is 0.6667359300090708\n",
      " for epoch : 345/500, loss is 0.6667063165179002\n",
      " for epoch : 346/500, loss is 0.6666768941744321\n",
      " for epoch : 347/500, loss is 0.6666476614377655\n",
      " for epoch : 348/500, loss is 0.6666186167837842\n",
      " for epoch : 349/500, loss is 0.6665897587048768\n",
      " for epoch : 350/500, loss is 0.6665610857096654\n",
      " for epoch : 351/500, loss is 0.6665325963227401\n",
      " for epoch : 352/500, loss is 0.6665042890843978\n",
      " for epoch : 353/500, loss is 0.6664761625503887\n",
      " for epoch : 354/500, loss is 0.6664482152916682\n",
      " for epoch : 355/500, loss is 0.666420445894153\n",
      " for epoch : 356/500, loss is 0.6663928529584847\n",
      " for epoch : 357/500, loss is 0.6663654350997962\n",
      " for epoch : 358/500, loss is 0.6663381909474858\n",
      " for epoch : 359/500, loss is 0.6663111191449946\n",
      " for epoch : 360/500, loss is 0.6662842183495887\n",
      " for epoch : 361/500, loss is 0.6662574872321475\n",
      " for epoch : 362/500, loss is 0.6662309244769552\n",
      " for epoch : 363/500, loss is 0.6662045287814978\n",
      " for epoch : 364/500, loss is 0.6661782988562636\n",
      " for epoch : 365/500, loss is 0.6661522334245485\n",
      " for epoch : 366/500, loss is 0.6661263312222664\n",
      " for epoch : 367/500, loss is 0.6661005909977612\n",
      " for epoch : 368/500, loss is 0.6660750115116251\n",
      " for epoch : 369/500, loss is 0.6660495915365197\n",
      " for epoch : 370/500, loss is 0.6660243298570011\n",
      " for epoch : 371/500, loss is 0.6659992252693492\n",
      " for epoch : 372/500, loss is 0.6659742765813985\n",
      " for epoch : 373/500, loss is 0.6659494826123757\n",
      " for epoch : 374/500, loss is 0.6659248421927374\n",
      " for epoch : 375/500, loss is 0.6659003541640134\n",
      " for epoch : 376/500, loss is 0.6658760173786525\n",
      " for epoch : 377/500, loss is 0.6658518306998712\n",
      " for epoch : 378/500, loss is 0.6658277930015051\n",
      " for epoch : 379/500, loss is 0.6658039031678649\n",
      " for epoch : 380/500, loss is 0.6657801600935938\n",
      " for epoch : 381/500, loss is 0.6657565626835282\n",
      " for epoch : 382/500, loss is 0.6657331098525607\n",
      " for epoch : 383/500, loss is 0.6657098005255078\n",
      " for epoch : 384/500, loss is 0.6656866336369766\n",
      " for epoch : 385/500, loss is 0.665663608131239\n",
      " for epoch : 386/500, loss is 0.6656407229621029\n",
      " for epoch : 387/500, loss is 0.6656179770927905\n",
      " for epoch : 388/500, loss is 0.6655953694958163\n",
      " for epoch : 389/500, loss is 0.6655728991528688\n",
      " for epoch : 390/500, loss is 0.6655505650546928\n",
      " for epoch : 391/500, loss is 0.6655283662009769\n",
      " for epoch : 392/500, loss is 0.6655063016002397\n",
      " for epoch : 393/500, loss is 0.6654843702697207\n",
      " for epoch : 394/500, loss is 0.6654625712352719\n",
      " for epoch : 395/500, loss is 0.6654409035312515\n",
      " for epoch : 396/500, loss is 0.6654193662004213\n",
      " for epoch : 397/500, loss is 0.6653979582938432\n",
      " for epoch : 398/500, loss is 0.6653766788707791\n",
      " for epoch : 399/500, loss is 0.6653555269985943\n",
      " for epoch : 400/500, loss is 0.6653345017526585\n",
      " for epoch : 401/500, loss is 0.6653136022162524\n",
      " for epoch : 402/500, loss is 0.6652928274804748\n",
      " for epoch : 403/500, loss is 0.6652721766441502\n",
      " for epoch : 404/500, loss is 0.6652516488137399\n",
      " for epoch : 405/500, loss is 0.6652312431032533\n",
      " for epoch : 406/500, loss is 0.6652109586341612\n",
      " for epoch : 407/500, loss is 0.6651907945353113\n",
      " for epoch : 408/500, loss is 0.6651707499428431\n",
      " for epoch : 409/500, loss is 0.6651508240001076\n",
      " for epoch : 410/500, loss is 0.665131015857585\n",
      " for epoch : 411/500, loss is 0.6651113246728058\n",
      " for epoch : 412/500, loss is 0.6650917496102728\n",
      " for epoch : 413/500, loss is 0.6650722898413846\n",
      " for epoch : 414/500, loss is 0.6650529445443589\n",
      " for epoch : 415/500, loss is 0.6650337129041604\n",
      " for epoch : 416/500, loss is 0.6650145941124251\n",
      " for epoch : 417/500, loss is 0.6649955873673914\n",
      " for epoch : 418/500, loss is 0.6649766918738277\n",
      " for epoch : 419/500, loss is 0.6649579068429626\n",
      " for epoch : 420/500, loss is 0.6649392314924185\n",
      " for epoch : 421/500, loss is 0.6649206650461423\n",
      " for epoch : 422/500, loss is 0.6649022067343405\n",
      " for epoch : 423/500, loss is 0.6648838557934139\n",
      " for epoch : 424/500, loss is 0.664865611465893\n",
      " for epoch : 425/500, loss is 0.6648474730003754\n",
      " for epoch : 426/500, loss is 0.6648294396514632\n",
      " for epoch : 427/500, loss is 0.6648115106797029\n",
      " for epoch : 428/500, loss is 0.6647936853515243\n",
      " for epoch : 429/500, loss is 0.6647759629391808\n",
      " for epoch : 430/500, loss is 0.6647583427206926\n",
      " for epoch : 431/500, loss is 0.6647408239797876\n",
      " for epoch : 432/500, loss is 0.6647234060058456\n",
      " for epoch : 433/500, loss is 0.6647060880938422\n",
      " for epoch : 434/500, loss is 0.6646888695442943\n",
      " for epoch : 435/500, loss is 0.664671749663205\n",
      " for epoch : 436/500, loss is 0.6646547277620111\n",
      " for epoch : 437/500, loss is 0.6646378031575304\n",
      " for epoch : 438/500, loss is 0.664620975171909\n",
      " for epoch : 439/500, loss is 0.6646042431325713\n",
      " for epoch : 440/500, loss is 0.664587606372169\n",
      " for epoch : 441/500, loss is 0.6645710642285314\n",
      " for epoch : 442/500, loss is 0.6645546160446167\n",
      " for epoch : 443/500, loss is 0.6645382611684639\n",
      " for epoch : 444/500, loss is 0.6645219989531438\n",
      " for epoch : 445/500, loss is 0.6645058287567144\n",
      " for epoch : 446/500, loss is 0.664489749942172\n",
      " for epoch : 447/500, loss is 0.6644737618774074\n",
      " for epoch : 448/500, loss is 0.6644578639351603\n",
      " for epoch : 449/500, loss is 0.6644420554929737\n",
      " for epoch : 450/500, loss is 0.6644263359331521\n",
      " for epoch : 451/500, loss is 0.664410704642716\n",
      " for epoch : 452/500, loss is 0.6643951610133607\n",
      " for epoch : 453/500, loss is 0.664379704441413\n",
      " for epoch : 454/500, loss is 0.664364334327791\n",
      " for epoch : 455/500, loss is 0.664349050077961\n",
      " for epoch : 456/500, loss is 0.6643338511018987\n",
      " for epoch : 457/500, loss is 0.6643187368140481\n",
      " for epoch : 458/500, loss is 0.6643037066332818\n",
      " for epoch : 459/500, loss is 0.6642887599828633\n",
      " for epoch : 460/500, loss is 0.6642738962904065\n",
      " for epoch : 461/500, loss is 0.6642591149878386\n",
      " for epoch : 462/500, loss is 0.664244415511363\n",
      " for epoch : 463/500, loss is 0.6642297973014207\n",
      " for epoch : 464/500, loss is 0.6642152598026546\n",
      " for epoch : 465/500, loss is 0.6642008024638728\n",
      " for epoch : 466/500, loss is 0.6641864247380128\n",
      " for epoch : 467/500, loss is 0.664172126082106\n",
      " for epoch : 468/500, loss is 0.6641579059572431\n",
      " for epoch : 469/500, loss is 0.6641437638285383\n",
      " for epoch : 470/500, loss is 0.6641296991650969\n",
      " for epoch : 471/500, loss is 0.6641157114399796\n",
      " for epoch : 472/500, loss is 0.6641018001301703\n",
      " for epoch : 473/500, loss is 0.6640879647165427\n",
      " for epoch : 474/500, loss is 0.6640742046838273\n",
      " for epoch : 475/500, loss is 0.6640605195205801\n",
      " for epoch : 476/500, loss is 0.6640469087191488\n",
      " for epoch : 477/500, loss is 0.6640333717756434\n",
      " for epoch : 478/500, loss is 0.6640199081899035\n",
      " for epoch : 479/500, loss is 0.6640065174654675\n",
      " for epoch : 480/500, loss is 0.6639931991095429\n",
      " for epoch : 481/500, loss is 0.663979952632975\n",
      " for epoch : 482/500, loss is 0.663966777550218\n",
      " for epoch : 483/500, loss is 0.6639536733793044\n",
      " for epoch : 484/500, loss is 0.663940639641817\n",
      " for epoch : 485/500, loss is 0.6639276758628582\n",
      " for epoch : 486/500, loss is 0.6639147815710231\n",
      " for epoch : 487/500, loss is 0.6639019562983705\n",
      " for epoch : 488/500, loss is 0.663889199580394\n",
      " for epoch : 489/500, loss is 0.663876510955996\n",
      " for epoch : 490/500, loss is 0.6638638899674579\n",
      " for epoch : 491/500, loss is 0.6638513361604156\n",
      " for epoch : 492/500, loss is 0.6638388490838305\n",
      " for epoch : 493/500, loss is 0.6638264282899634\n",
      " for epoch : 494/500, loss is 0.6638140733343489\n",
      " for epoch : 495/500, loss is 0.6638017837757683\n",
      " for epoch : 496/500, loss is 0.6637895591762235\n",
      " for epoch : 497/500, loss is 0.6637773991009135\n",
      " for epoch : 498/500, loss is 0.6637653031182064\n",
      " for epoch : 499/500, loss is 0.6637532707996158\n"
     ]
    }
   ],
   "source": [
    "# creating model\n",
    "model = SimpleNN(X_train_tensor)\n",
    "\n",
    "print(model.bias)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # forward pass\n",
    "    \n",
    "    y_pred = model.forward_pass(X_train_tensor)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = model.loss_calculation(y_pred, y_train_tensor)\n",
    "    print(f\" for epoch : {epoch}/{epochs}, loss is {loss.item()}\")\n",
    "    \n",
    "    # backward pass\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    # parameters update\n",
    "    with torch.no_grad():\n",
    "        model.weights -= learning_rate * model.weights.grad\n",
    "        model.bias -= learning_rate * model.bias.grad\n",
    "\n",
    "    # zero gradientsy_train_tensor\n",
    "    model.weights.grad.zero_()\n",
    "    model.bias.grad.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe591585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 62.28%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "   #oss is 0.9898135662078857\n",
    " #for epoch : 3/500, loss is 0.989813566207885\n",
    "   y_test_pred = model.forward_pass(X_test_tensor)\n",
    "   y_test_pred_labels = (y_test_pred >= 0.5).float()\n",
    "   \n",
    "   accuracy = (y_test_pred_labels.squeeze() == y_test_tensor).float().mean()\n",
    "   \n",
    "   print(f\"Test Accuracy: {accuracy.item()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eace08d",
   "metadata": {},
   "source": [
    "# Using Torch's NN Module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dd598aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b70e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features, hidden_layer1_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.hidden_layer1_size = hidden_layer1_size\n",
    "        \n",
    "        self.neural_network = nn.Sequential(\n",
    "            nn.Linear(num_features, hidden_layer1_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer1_size, 1),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, features):\n",
    "       \n",
    "        activation2_out = self.neural_network(features)\n",
    "\n",
    "        return activation2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd329772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([455, 30])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b86e97e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5323176383972168\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hidden_layer_size = 15\n",
    "learning_rate = 0.05\n",
    "epochs = 5000\n",
    "\n",
    "#loss_function = nn.BCELoss() # binary cross entropy loss\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = Model(X_train_tensor.shape[1], hidden_layer_size)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "# model\n",
    "\n",
    "    # internally NN Module calls forward method\n",
    "    y_pred = model(X_train_tensor.float()) # same as model.forward(features)\n",
    "    loss = loss_function(y_pred, y_train_tensor.float().unsqueeze(1))\n",
    "\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #print(f\" for epoch : {epoch}/{epochs}, loss is {loss.item()}\")\n",
    "\n",
    "\n",
    "y_pred_f = model(X_test_tensor.float())\n",
    "\n",
    "y_pred_f = (y_pred_f >= 0.5).float()\n",
    "\n",
    "accuracy = (y_pred_f == y_test_tensor).float().mean()\n",
    "\n",
    "print(accuracy.item())\n",
    "\n",
    "\n",
    "    # print(y_pred)\n",
    "# 0.5323176383972168\n",
    "\n",
    "# 0.5280086398124695 - ADAM\n",
    "# 0.5344721674919128 - SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85c11451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5323176383972168\n"
     ]
    }
   ],
   "source": [
    "y_pred = model(X_test_tensor.float())\n",
    "\n",
    "y_pred = (y_pred >= 0.5).float()\n",
    "\n",
    "accuracy = (y_pred == y_test_tensor).float().mean()\n",
    "\n",
    "print(accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab156d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
