{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "53989a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello WOrld!!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello WOrld!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8ff00b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f918527b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 33)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5d492549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-70d7ed19-61e1-4a2f-a824-fd5fa12cb5bd\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>texture_se</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>smoothness_se</th>\n",
       "      <th>compactness_se</th>\n",
       "      <th>concavity_se</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70d7ed19-61e1-4a2f-a824-fd5fa12cb5bd')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-70d7ed19-61e1-4a2f-a824-fd5fa12cb5bd button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-70d7ed19-61e1-4a2f-a824-fd5fa12cb5bd');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
       "0    842302         M  ...                  0.11890          NaN\n",
       "1    842517         M  ...                  0.08902          NaN\n",
       "2  84300903         M  ...                  0.08758          NaN\n",
       "3  84348301         M  ...                  0.17300          NaN\n",
       "4  84358402         M  ...                  0.07678          NaN\n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d556c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['id', 'Unnamed: 32'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f6590d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-2dad64ee-6ccd-4f5d-a2b7-e258409ea8cd\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>texture_se</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>smoothness_se</th>\n",
       "      <th>compactness_se</th>\n",
       "      <th>concavity_se</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2dad64ee-6ccd-4f5d-a2b7-e258409ea8cd')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-2dad64ee-6ccd-4f5d-a2b7-e258409ea8cd button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-2dad64ee-6ccd-4f5d-a2b7-e258409ea8cd');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "  diagnosis  radius_mean  ...  symmetry_worst  fractal_dimension_worst\n",
       "0         M        17.99  ...          0.4601                  0.11890\n",
       "1         M        20.57  ...          0.2750                  0.08902\n",
       "2         M        19.69  ...          0.3613                  0.08758\n",
       "3         M        11.42  ...          0.6638                  0.17300\n",
       "4         M        20.29  ...          0.2364                  0.07678\n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f608f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:], df.iloc[:, 0], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7afd1563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30)\n",
      "(569, 31)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ce8489b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7bcb38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_test_encoded = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ec4fe",
   "metadata": {},
   "source": [
    "Numpy Arrays to PyTorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "379c2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train_scaled)\n",
    "X_test_tensor = torch.from_numpy(X_test_scaled)\n",
    "y_train_tensor = torch.from_numpy(y_train_encoded)\n",
    "y_test_tensor = torch.from_numpy(y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "96dde298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4408, -0.4353, -1.3621,  ...,  0.9320,  2.0972,  1.8865],\n",
       "        [ 1.9741,  1.7330,  2.0917,  ...,  2.6989,  1.8912,  2.4978],\n",
       "        [-1.4000, -1.2496, -1.3452,  ..., -0.9702,  0.5976,  0.0579],\n",
       "        ...,\n",
       "        [ 0.0488, -0.5550, -0.0651,  ..., -1.2390, -0.7086, -1.2715],\n",
       "        [-0.0390,  0.1021, -0.0314,  ...,  1.0500,  0.4343,  1.2134],\n",
       "        [-0.5486,  0.3133, -0.6035,  ..., -0.6110, -0.3345, -0.8463]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd18c07f",
   "metadata": {},
   "source": [
    "Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a373a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN():\n",
    "    \n",
    "    def __init__(self, X):\n",
    "        self.weights = torch.rand(X.shape[1], 1, dtype=torch.float64, requires_grad=True)\n",
    "        self.bias = torch.zeros(1, dtype=torch.float64, requires_grad=True)\n",
    "        \n",
    "    def forward_pass(self, X):\n",
    "        y = torch.matmul(X, self.weights) + self.bias\n",
    "        \n",
    "        y_pred = torch.sigmoid(y)\n",
    "        return y_pred\n",
    "        \n",
    "        \n",
    "    def loss_calculation(self, y_pred, y_true):\n",
    "        epsilon = 1e-7\n",
    "        \n",
    "        y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        loss = -(y_true*torch.log(y_pred) + (1-y_true)*torch.log(1-y_pred)).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    " \n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc47a8",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a873ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4581e765",
   "metadata": {},
   "source": [
    "Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1a0f5571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], dtype=torch.float64, requires_grad=True)\n",
      " for epoch : 0/500, loss is 3.6306229215516495\n",
      " for epoch : 1/500, loss is 3.5662298223307407\n",
      " for epoch : 2/500, loss is 3.5014954091290655\n",
      " for epoch : 3/500, loss is 3.436138208947765\n",
      " for epoch : 4/500, loss is 3.370685947332795\n",
      " for epoch : 5/500, loss is 3.3046441335624364\n",
      " for epoch : 6/500, loss is 3.2386742713521124\n",
      " for epoch : 7/500, loss is 3.1712739273818302\n",
      " for epoch : 8/500, loss is 3.1022161542917557\n",
      " for epoch : 9/500, loss is 3.031878380952748\n",
      " for epoch : 10/500, loss is 2.958865018694129\n",
      " for epoch : 11/500, loss is 2.8853356899423668\n",
      " for epoch : 12/500, loss is 2.8093998923495525\n",
      " for epoch : 13/500, loss is 2.7321901642064437\n",
      " for epoch : 14/500, loss is 2.6556848470181227\n",
      " for epoch : 15/500, loss is 2.578708865290636\n",
      " for epoch : 16/500, loss is 2.502721556514821\n",
      " for epoch : 17/500, loss is 2.4238778749511094\n",
      " for epoch : 18/500, loss is 2.3455019170785363\n",
      " for epoch : 19/500, loss is 2.268175914922609\n",
      " for epoch : 20/500, loss is 2.1905329913257927\n",
      " for epoch : 21/500, loss is 2.1135846084051346\n",
      " for epoch : 22/500, loss is 2.0364971110138343\n",
      " for epoch : 23/500, loss is 1.9609106172188537\n",
      " for epoch : 24/500, loss is 1.885491137381742\n",
      " for epoch : 25/500, loss is 1.8104535213502617\n",
      " for epoch : 26/500, loss is 1.73805845198275\n",
      " for epoch : 27/500, loss is 1.6684730533034826\n",
      " for epoch : 28/500, loss is 1.6018586960616308\n",
      " for epoch : 29/500, loss is 1.5376307682597234\n",
      " for epoch : 30/500, loss is 1.475923982860308\n",
      " for epoch : 31/500, loss is 1.4176505053334891\n",
      " for epoch : 32/500, loss is 1.36291196210636\n",
      " for epoch : 33/500, loss is 1.311783187171111\n",
      " for epoch : 34/500, loss is 1.2643083563765027\n",
      " for epoch : 35/500, loss is 1.22049669198973\n",
      " for epoch : 36/500, loss is 1.1803175812412827\n",
      " for epoch : 37/500, loss is 1.1436956516702934\n",
      " for epoch : 38/500, loss is 1.110507514053889\n",
      " for epoch : 39/500, loss is 1.08058268714694\n",
      " for epoch : 40/500, loss is 1.0537102953976836\n",
      " for epoch : 41/500, loss is 1.0296505442677186\n",
      " for epoch : 42/500, loss is 1.008147912815848\n",
      " for epoch : 43/500, loss is 0.9889431570505328\n",
      " for epoch : 44/500, loss is 0.9717828066847329\n",
      " for epoch : 45/500, loss is 0.9564261350078463\n",
      " for epoch : 46/500, loss is 0.9426499600999294\n",
      " for epoch : 47/500, loss is 0.9302515462374193\n",
      " for epoch : 48/500, loss is 0.9190498280227231\n",
      " for epoch : 49/500, loss is 0.908885271273198\n",
      " for epoch : 50/500, loss is 0.8996187755768018\n",
      " for epoch : 51/500, loss is 0.8911300183182418\n",
      " for epoch : 52/500, loss is 0.8833155536865882\n",
      " for epoch : 53/500, loss is 0.8760868710789845\n",
      " for epoch : 54/500, loss is 0.8693685273911365\n",
      " for epoch : 55/500, loss is 0.8630964093682573\n",
      " for epoch : 56/500, loss is 0.8572161500439489\n",
      " for epoch : 57/500, loss is 0.851681707166701\n",
      " for epoch : 58/500, loss is 0.8464541032999529\n",
      " for epoch : 59/500, loss is 0.8415003224071977\n",
      " for epoch : 60/500, loss is 0.8367923542792141\n",
      " for epoch : 61/500, loss is 0.8323063755939554\n",
      " for epoch : 62/500, loss is 0.828022054679081\n",
      " for epoch : 63/500, loss is 0.8239219661812707\n",
      " for epoch : 64/500, loss is 0.8199911017588783\n",
      " for epoch : 65/500, loss is 0.8162164634496057\n",
      " for epoch : 66/500, loss is 0.8125867273348871\n",
      " for epoch : 67/500, loss is 0.8090919663482227\n",
      " for epoch : 68/500, loss is 0.8057234224082689\n",
      " for epoch : 69/500, loss is 0.8024733193907534\n",
      " for epoch : 70/500, loss is 0.7993347097147243\n",
      " for epoch : 71/500, loss is 0.7963013484669431\n",
      " for epoch : 72/500, loss is 0.7933675900042916\n",
      " for epoch : 73/500, loss is 0.790528302854028\n",
      " for epoch : 74/500, loss is 0.7877787994812733\n",
      " for epoch : 75/500, loss is 0.7851147781232437\n",
      " for epoch : 76/500, loss is 0.782532274414022\n",
      " for epoch : 77/500, loss is 0.7800276209561675\n",
      " for epoch : 78/500, loss is 0.7775974133498439\n",
      " for epoch : 79/500, loss is 0.7752384814788682\n",
      " for epoch : 80/500, loss is 0.7729478650872201\n",
      " for epoch : 81/500, loss is 0.7707227928686899\n",
      " for epoch : 82/500, loss is 0.7685606644446455\n",
      " for epoch : 83/500, loss is 0.7664590347272064\n",
      " for epoch : 84/500, loss is 0.7644156002631354\n",
      " for epoch : 85/500, loss is 0.7624281872321577\n",
      " for epoch : 86/500, loss is 0.7604947408360792\n",
      " for epoch : 87/500, loss is 0.7586133158650642\n",
      " for epoch : 88/500, loss is 0.7567820682673407\n",
      " for epoch : 89/500, loss is 0.7549992475804232\n",
      " for epoch : 90/500, loss is 0.7532631901073534\n",
      " for epoch : 91/500, loss is 0.7515723127417466\n",
      " for epoch : 92/500, loss is 0.749925107361683\n",
      " for epoch : 93/500, loss is 0.7483201357254943\n",
      " for epoch : 94/500, loss is 0.7467560248129806\n",
      " for epoch : 95/500, loss is 0.7452314625640448\n",
      " for epoch : 96/500, loss is 0.7437451939735866\n",
      " for epoch : 97/500, loss is 0.7422960175071006\n",
      " for epoch : 98/500, loss is 0.7408827818060016\n",
      " for epoch : 99/500, loss is 0.7395043826555026\n",
      " for epoch : 100/500, loss is 0.7381597601910344\n",
      " for epoch : 101/500, loss is 0.7368478963218439\n",
      " for epoch : 102/500, loss is 0.7355678123526757\n",
      " for epoch : 103/500, loss is 0.7343185667863662\n",
      " for epoch : 104/500, loss is 0.7330992532918473\n",
      " for epoch : 105/500, loss is 0.7319089988235195\n",
      " for epoch : 106/500, loss is 0.7307469618792253\n",
      " for epoch : 107/500, loss is 0.7296123308852044\n",
      " for epoch : 108/500, loss is 0.7285043226974144\n",
      " for epoch : 109/500, loss is 0.7274221812095191\n",
      " for epoch : 110/500, loss is 0.7263651760586773\n",
      " for epoch : 111/500, loss is 0.7253326014210044\n",
      " for epoch : 112/500, loss is 0.724323774889271\n",
      " for epoch : 113/500, loss is 0.7233380364260247\n",
      " for epoch : 114/500, loss is 0.7223747473858935\n",
      " for epoch : 115/500, loss is 0.7214332896013577\n",
      " for epoch : 116/500, loss is 0.7205130645267559\n",
      " for epoch : 117/500, loss is 0.7196134924357418\n",
      " for epoch : 118/500, loss is 0.7187340116678087\n",
      " for epoch : 119/500, loss is 0.7178740779198847\n",
      " for epoch : 120/500, loss is 0.7170331635793369\n",
      " for epoch : 121/500, loss is 0.7162107570950504\n",
      " for epoch : 122/500, loss is 0.7154063623835312\n",
      " for epoch : 123/500, loss is 0.714619498267258\n",
      " for epoch : 124/500, loss is 0.7138496979427464\n",
      " for epoch : 125/500, loss is 0.7130965084760215\n",
      " for epoch : 126/500, loss is 0.7123594903233913\n",
      " for epoch : 127/500, loss is 0.711638216875615\n",
      " for epoch : 128/500, loss is 0.7109322740237164\n",
      " for epoch : 129/500, loss is 0.7102412597448649\n",
      " for epoch : 130/500, loss is 0.7095647837068783\n",
      " for epoch : 131/500, loss is 0.7089024668900388\n",
      " for epoch : 132/500, loss is 0.7082539412250276\n",
      " for epoch : 133/500, loss is 0.7076188492458945\n",
      " for epoch : 134/500, loss is 0.7069968437570741\n",
      " for epoch : 135/500, loss is 0.7063875875135504\n",
      " for epoch : 136/500, loss is 0.7057907529133524\n",
      " for epoch : 137/500, loss is 0.7052060217016302\n",
      " for epoch : 138/500, loss is 0.7046330846856397\n",
      " for epoch : 139/500, loss is 0.7040716414600051\n",
      " for epoch : 140/500, loss is 0.7035214001416998\n",
      " for epoch : 141/500, loss is 0.7029820771142251\n",
      " for epoch : 142/500, loss is 0.7024533967805064\n",
      " for epoch : 143/500, loss is 0.7019350913240808\n",
      " for epoch : 144/500, loss is 0.7014269004781641\n",
      " for epoch : 145/500, loss is 0.7009285713022402\n",
      " for epoch : 146/500, loss is 0.7004398579658229\n",
      " for epoch : 147/500, loss is 0.6999605215390855\n",
      " for epoch : 148/500, loss is 0.6994903297900585\n",
      " for epoch : 149/500, loss is 0.6990290569881358\n",
      " for epoch : 150/500, loss is 0.6985764837136277\n",
      " for epoch : 151/500, loss is 0.6981323966731375\n",
      " for epoch : 152/500, loss is 0.6976965885205366\n",
      " for epoch : 153/500, loss is 0.6972688576833355\n",
      " for epoch : 154/500, loss is 0.6968490081942599\n",
      " for epoch : 155/500, loss is 0.6964368495278495\n",
      " for epoch : 156/500, loss is 0.6960321964419094\n",
      " for epoch : 157/500, loss is 0.6956348688236547\n",
      " for epoch : 158/500, loss is 0.6952446915403916\n",
      " for epoch : 159/500, loss is 0.694861494294594\n",
      " for epoch : 160/500, loss is 0.6944851114832331\n",
      " for epoch : 161/500, loss is 0.6941153820612334\n",
      " for epoch : 162/500, loss is 0.693752149408921\n",
      " for epoch : 163/500, loss is 0.693395261203353\n",
      " for epoch : 164/500, loss is 0.6930445692934042\n",
      " for epoch : 165/500, loss is 0.6926999295785011\n",
      " for epoch : 166/500, loss is 0.692361201890902\n",
      " for epoch : 167/500, loss is 0.6920282498814093\n",
      " for epoch : 168/500, loss is 0.6917009409084253\n",
      " for epoch : 169/500, loss is 0.6913791459302443\n",
      " for epoch : 170/500, loss is 0.6910627394004993\n",
      " for epoch : 171/500, loss is 0.6907515991666605\n",
      " for epoch : 172/500, loss is 0.6904456063715092\n",
      " for epoch : 173/500, loss is 0.6901446453574953\n",
      " for epoch : 174/500, loss is 0.6898486035738981\n",
      " for epoch : 175/500, loss is 0.6895573714867128\n",
      " for epoch : 176/500, loss is 0.6892708424911802\n",
      " for epoch : 177/500, loss is 0.6889889128268892\n",
      " for epoch : 178/500, loss is 0.6887114814953758\n",
      " for epoch : 179/500, loss is 0.6884384501801477\n",
      " for epoch : 180/500, loss is 0.6881697231690663\n",
      " for epoch : 181/500, loss is 0.6879052072790184\n",
      " for epoch : 182/500, loss is 0.6876448117828099\n",
      " for epoch : 183/500, loss is 0.6873884483382211\n",
      " for epoch : 184/500, loss is 0.6871360309191601\n",
      " for epoch : 185/500, loss is 0.6868874757488515\n",
      " for epoch : 186/500, loss is 0.6866427012350046\n",
      " for epoch : 187/500, loss is 0.6864016279069053\n",
      " for epoch : 188/500, loss is 0.6861641783543696\n",
      " for epoch : 189/500, loss is 0.6859302771685111\n",
      " for epoch : 190/500, loss is 0.6856998508842669\n",
      " for epoch : 191/500, loss is 0.6854728279246308\n",
      " for epoch : 192/500, loss is 0.6852491385465435\n",
      " for epoch : 193/500, loss is 0.6850287147883923\n",
      " for epoch : 194/500, loss is 0.6848114904190725\n",
      " for epoch : 195/500, loss is 0.6845974008885644\n",
      " for epoch : 196/500, loss is 0.6843863832799811\n",
      " for epoch : 197/500, loss is 0.6841783762630453\n",
      " for epoch : 198/500, loss is 0.6839733200489493\n",
      " for epoch : 199/500, loss is 0.683771156346561\n",
      " for epoch : 200/500, loss is 0.6835718283199351\n",
      " for epoch : 201/500, loss is 0.6833752805470871\n",
      " for epoch : 202/500, loss is 0.6831814589799977\n",
      " for epoch : 203/500, loss is 0.6829903109058078\n",
      " for epoch : 204/500, loss is 0.6828017849091692\n",
      " for epoch : 205/500, loss is 0.6826158308357168\n",
      " for epoch : 206/500, loss is 0.6824323997566261\n",
      " for epoch : 207/500, loss is 0.6822514439342302\n",
      " for epoch : 208/500, loss is 0.682072916788655\n",
      " for epoch : 209/500, loss is 0.68189677286545\n",
      " for epoch : 210/500, loss is 0.6817229678041813\n",
      " for epoch : 211/500, loss is 0.6815514583079566\n",
      " for epoch : 212/500, loss is 0.6813822021138569\n",
      " for epoch : 213/500, loss is 0.681215157964247\n",
      " for epoch : 214/500, loss is 0.6810502855789353\n",
      " for epoch : 215/500, loss is 0.6808875456281617\n",
      " for epoch : 216/500, loss is 0.6807268997063846\n",
      " for epoch : 217/500, loss is 0.6805683103068454\n",
      " for epoch : 218/500, loss is 0.6804117407968865\n",
      " for epoch : 219/500, loss is 0.6802571553939972\n",
      " for epoch : 220/500, loss is 0.6801045191425716\n",
      " for epoch : 221/500, loss is 0.6799537978913482\n",
      " for epoch : 222/500, loss is 0.6798049582715205\n",
      " for epoch : 223/500, loss is 0.6796579676754887\n",
      " for epoch : 224/500, loss is 0.6795127942362399\n",
      " for epoch : 225/500, loss is 0.6793694068073346\n",
      " for epoch : 226/500, loss is 0.6792277749434817\n",
      " for epoch : 227/500, loss is 0.679087868881684\n",
      " for epoch : 228/500, loss is 0.6789496595229372\n",
      " for epoch : 229/500, loss is 0.6788131184144653\n",
      " for epoch : 230/500, loss is 0.6786782177324766\n",
      " for epoch : 231/500, loss is 0.6785449302654238\n",
      " for epoch : 232/500, loss is 0.6784132293977521\n",
      " for epoch : 233/500, loss is 0.6782830890941245\n",
      " for epoch : 234/500, loss is 0.6781544838841032\n",
      " for epoch : 235/500, loss is 0.67802738884728\n",
      " for epoch : 236/500, loss is 0.6779017795988375\n",
      " for epoch : 237/500, loss is 0.6777776322755297\n",
      " for epoch : 238/500, loss is 0.6776549235220694\n",
      " for epoch : 239/500, loss is 0.6775336304779095\n",
      " for epoch : 240/500, loss is 0.6774137307644078\n",
      " for epoch : 241/500, loss is 0.6772952024723597\n",
      " for epoch : 242/500, loss is 0.6771780241498938\n",
      " for epoch : 243/500, loss is 0.6770621747907135\n",
      " for epoch : 244/500, loss is 0.676947633822678\n",
      " for epoch : 245/500, loss is 0.6768343810967097\n",
      " for epoch : 246/500, loss is 0.6767223968760229\n",
      " for epoch : 247/500, loss is 0.6766116618256554\n",
      " for epoch : 248/500, loss is 0.6765021570023045\n",
      " for epoch : 249/500, loss is 0.6763938638444495\n",
      " for epoch : 250/500, loss is 0.6762867641627573\n",
      " for epoch : 251/500, loss is 0.6761808401307596\n",
      " for epoch : 252/500, loss is 0.6760760742757952\n",
      " for epoch : 253/500, loss is 0.67597244947021\n",
      " for epoch : 254/500, loss is 0.6758699489228033\n",
      " for epoch : 255/500, loss is 0.6757685561705186\n",
      " for epoch : 256/500, loss is 0.6756682550703658\n",
      " for epoch : 257/500, loss is 0.6755690297915731\n",
      " for epoch : 258/500, loss is 0.6754708648079574\n",
      " for epoch : 259/500, loss is 0.6753737448905096\n",
      " for epoch : 260/500, loss is 0.6752776551001863\n",
      " for epoch : 261/500, loss is 0.6751825807809037\n",
      " for epoch : 262/500, loss is 0.6750885075527265\n",
      " for epoch : 263/500, loss is 0.6749954213052453\n",
      " for epoch : 264/500, loss is 0.6749033081911386\n",
      " for epoch : 265/500, loss is 0.6748121546199132\n",
      " for epoch : 266/500, loss is 0.6747219472518171\n",
      " for epoch : 267/500, loss is 0.6746326729919198\n",
      " for epoch : 268/500, loss is 0.6745443189843565\n",
      " for epoch : 269/500, loss is 0.6744568726067299\n",
      " for epoch : 270/500, loss is 0.6743703214646645\n",
      " for epoch : 271/500, loss is 0.6742846533865114\n",
      " for epoch : 272/500, loss is 0.6741998564181962\n",
      " for epoch : 273/500, loss is 0.6741159188182071\n",
      " for epoch : 274/500, loss is 0.6740328290527197\n",
      " for epoch : 275/500, loss is 0.673950575790854\n",
      " for epoch : 276/500, loss is 0.6738691479000571\n",
      " for epoch : 277/500, loss is 0.6737885344416149\n",
      " for epoch : 278/500, loss is 0.6737087246662794\n",
      " for epoch : 279/500, loss is 0.6736297080100169\n",
      " for epoch : 280/500, loss is 0.6735514740898683\n",
      " for epoch : 281/500, loss is 0.6734740126999201\n",
      " for epoch : 282/500, loss is 0.6733973138073829\n",
      " for epoch : 283/500, loss is 0.6733213675487734\n",
      " for epoch : 284/500, loss is 0.6732461642261977\n",
      " for epoch : 285/500, loss is 0.6731716943037336\n",
      " for epoch : 286/500, loss is 0.6730979484039068\n",
      " for epoch : 287/500, loss is 0.6730249173042604\n",
      " for epoch : 288/500, loss is 0.6729525919340131\n",
      " for epoch : 289/500, loss is 0.6728809633708077\n",
      " for epoch : 290/500, loss is 0.6728100228375392\n",
      " for epoch : 291/500, loss is 0.6727397616992712\n",
      " for epoch : 292/500, loss is 0.6726701714602266\n",
      " for epoch : 293/500, loss is 0.6726012437608595\n",
      " for epoch : 294/500, loss is 0.6725329703750017\n",
      " for epoch : 295/500, loss is 0.6724653432070811\n",
      " for epoch : 296/500, loss is 0.6723983542894132\n",
      " for epoch : 297/500, loss is 0.6723319957795599\n",
      " for epoch : 298/500, loss is 0.6722662599577569\n",
      " for epoch : 299/500, loss is 0.6722011392244047\n",
      " for epoch : 300/500, loss is 0.6721366260976247\n",
      " for epoch : 301/500, loss is 0.6720727132108755\n",
      " for epoch : 302/500, loss is 0.6720093933106298\n",
      " for epoch : 303/500, loss is 0.6719466592541087\n",
      " for epoch : 304/500, loss is 0.6718845040070732\n",
      " for epoch : 305/500, loss is 0.6718229206416699\n",
      " for epoch : 306/500, loss is 0.6717619023343311\n",
      " for epoch : 307/500, loss is 0.671701442363725\n",
      " for epoch : 308/500, loss is 0.671641534108758\n",
      " for epoch : 309/500, loss is 0.6715821710466251\n",
      " for epoch : 310/500, loss is 0.6715233467509077\n",
      " for epoch : 311/500, loss is 0.6714650548897186\n",
      " for epoch : 312/500, loss is 0.671407289223891\n",
      " for epoch : 313/500, loss is 0.6713500436052116\n",
      " for epoch : 314/500, loss is 0.6712933119746965\n",
      " for epoch : 315/500, loss is 0.671237088360908\n",
      " for epoch : 316/500, loss is 0.6711813668783116\n",
      " for epoch : 317/500, loss is 0.6711261417256732\n",
      " for epoch : 318/500, loss is 0.6710714071844918\n",
      " for epoch : 319/500, loss is 0.6710171576174723\n",
      " for epoch : 320/500, loss is 0.670963387467032\n",
      " for epoch : 321/500, loss is 0.6709100912538425\n",
      " for epoch : 322/500, loss is 0.6708572635754065\n",
      " for epoch : 323/500, loss is 0.6708048991046672\n",
      " for epoch : 324/500, loss is 0.6707529925886488\n",
      " for epoch : 325/500, loss is 0.6707015388471305\n",
      " for epoch : 326/500, loss is 0.6706505327713489\n",
      " for epoch : 327/500, loss is 0.6705999693227318\n",
      " for epoch : 328/500, loss is 0.6705498435316586\n",
      " for epoch : 329/500, loss is 0.6705001504962518\n",
      " for epoch : 330/500, loss is 0.6704508853811932\n",
      " for epoch : 331/500, loss is 0.6704020434165681\n",
      " for epoch : 332/500, loss is 0.670353619896734\n",
      " for epoch : 333/500, loss is 0.6703056101792174\n",
      " for epoch : 334/500, loss is 0.670258009683631\n",
      " for epoch : 335/500, loss is 0.6702108138906183\n",
      " for epoch : 336/500, loss is 0.6701640183408201\n",
      " for epoch : 337/500, loss is 0.6701176186338627\n",
      " for epoch : 338/500, loss is 0.6700716104273705\n",
      " for epoch : 339/500, loss is 0.6700259894359968\n",
      " for epoch : 340/500, loss is 0.6699807514304793\n",
      " for epoch : 341/500, loss is 0.6699358922367128\n",
      " for epoch : 342/500, loss is 0.669891407734843\n",
      " for epoch : 343/500, loss is 0.6698472938583799\n",
      " for epoch : 344/500, loss is 0.6698035465933295\n",
      " for epoch : 345/500, loss is 0.6697601619773441\n",
      " for epoch : 346/500, loss is 0.6697171360988902\n",
      " for epoch : 347/500, loss is 0.6696744650964335\n",
      " for epoch : 348/500, loss is 0.6696321451576422\n",
      " for epoch : 349/500, loss is 0.6695901725186059\n",
      " for epoch : 350/500, loss is 0.6695485434630689\n",
      " for epoch : 351/500, loss is 0.6695072543216836\n",
      " for epoch : 352/500, loss is 0.6694663014712744\n",
      " for epoch : 353/500, loss is 0.6694256813341193\n",
      " for epoch : 354/500, loss is 0.6693853903772463\n",
      " for epoch : 355/500, loss is 0.6693454251117407\n",
      " for epoch : 356/500, loss is 0.6693057820920716\n",
      " for epoch : 357/500, loss is 0.6692664579154263\n",
      " for epoch : 358/500, loss is 0.6692274492210618\n",
      " for epoch : 359/500, loss is 0.6691887526896676\n",
      " for epoch : 360/500, loss is 0.6691503650427402\n",
      " for epoch : 361/500, loss is 0.669112283041972\n",
      " for epoch : 362/500, loss is 0.6690745034886498\n",
      " for epoch : 363/500, loss is 0.6690370232230669\n",
      " for epoch : 364/500, loss is 0.6689998391239448\n",
      " for epoch : 365/500, loss is 0.6689629481078665\n",
      " for epoch : 366/500, loss is 0.6689263471287221\n",
      " for epoch : 367/500, loss is 0.6688900331771623\n",
      " for epoch : 368/500, loss is 0.6688540032800645\n",
      " for epoch : 369/500, loss is 0.6688182545000069\n",
      " for epoch : 370/500, loss is 0.6687827839347548\n",
      " for epoch : 371/500, loss is 0.6687475887167539\n",
      " for epoch : 372/500, loss is 0.668712666012635\n",
      " for epoch : 373/500, loss is 0.6686780130227264\n",
      " for epoch : 374/500, loss is 0.6686436269805764\n",
      " for epoch : 375/500, loss is 0.6686095051524835\n",
      " for epoch : 376/500, loss is 0.668575644837035\n",
      " for epoch : 377/500, loss is 0.6685420433646554\n",
      " for epoch : 378/500, loss is 0.6685086980971615\n",
      " for epoch : 379/500, loss is 0.6684756064273253\n",
      " for epoch : 380/500, loss is 0.668442765778446\n",
      " for epoch : 381/500, loss is 0.668410173603928\n",
      " for epoch : 382/500, loss is 0.6683778273868676\n",
      " for epoch : 383/500, loss is 0.6683457246396459\n",
      " for epoch : 384/500, loss is 0.6683138629035295\n",
      " for epoch : 385/500, loss is 0.668282239748278\n",
      " for epoch : 386/500, loss is 0.6682508527717574\n",
      " for epoch : 387/500, loss is 0.6682196995995615\n",
      " for epoch : 388/500, loss is 0.668188777884639\n",
      " for epoch : 389/500, loss is 0.6681580853069266\n",
      " for epoch : 390/500, loss is 0.6681276195729878\n",
      " for epoch : 391/500, loss is 0.6680973784156616\n",
      " for epoch : 392/500, loss is 0.6680673595937104\n",
      " for epoch : 393/500, loss is 0.6680375608914799\n",
      " for epoch : 394/500, loss is 0.6680079801185611\n",
      " for epoch : 395/500, loss is 0.6679786151094594\n",
      " for epoch : 396/500, loss is 0.6679494637232681\n",
      " for epoch : 397/500, loss is 0.667920523843348\n",
      " for epoch : 398/500, loss is 0.667891793377012\n",
      " for epoch : 399/500, loss is 0.6678632702552146\n",
      " for epoch : 400/500, loss is 0.6678349524322469\n",
      " for epoch : 401/500, loss is 0.6678068378854348\n",
      " for epoch : 402/500, loss is 0.6677789246148449\n",
      " for epoch : 403/500, loss is 0.6677512106429925\n",
      " for epoch : 404/500, loss is 0.6677236940145549\n",
      " for epoch : 405/500, loss is 0.6676963727960902\n",
      " for epoch : 406/500, loss is 0.6676692450757588\n",
      " for epoch : 407/500, loss is 0.667642308963051\n",
      " for epoch : 408/500, loss is 0.667615562588517\n",
      " for epoch : 409/500, loss is 0.6675890041035025\n",
      " for epoch : 410/500, loss is 0.6675626316798882\n",
      " for epoch : 411/500, loss is 0.6675364435098314\n",
      " for epoch : 412/500, loss is 0.6675104378055151\n",
      " for epoch : 413/500, loss is 0.6674846127988964\n",
      " for epoch : 414/500, loss is 0.6674589667414634\n",
      " for epoch : 415/500, loss is 0.667433497903992\n",
      " for epoch : 416/500, loss is 0.6674082045763077\n",
      " for epoch : 417/500, loss is 0.6673830850670518\n",
      " for epoch : 418/500, loss is 0.6673581377034492\n",
      " for epoch : 419/500, loss is 0.6673333608310814\n",
      " for epoch : 420/500, loss is 0.667308752813662\n",
      " for epoch : 421/500, loss is 0.6672843120328142\n",
      " for epoch : 422/500, loss is 0.6672600368878557\n",
      " for epoch : 423/500, loss is 0.6672359257955804\n",
      " for epoch : 424/500, loss is 0.6672119771900494\n",
      " for epoch : 425/500, loss is 0.6671881895223805\n",
      " for epoch : 426/500, loss is 0.6671645612605438\n",
      " for epoch : 427/500, loss is 0.6671410908891582\n",
      " for epoch : 428/500, loss is 0.667117776909291\n",
      " for epoch : 429/500, loss is 0.6670946178382626\n",
      " for epoch : 430/500, loss is 0.6670716122094499\n",
      " for epoch : 431/500, loss is 0.6670487585720963\n",
      " for epoch : 432/500, loss is 0.6670260554911218\n",
      " for epoch : 433/500, loss is 0.6670035015469379\n",
      " for epoch : 434/500, loss is 0.6669810953352618\n",
      " for epoch : 435/500, loss is 0.6669588354669376\n",
      " for epoch : 436/500, loss is 0.6669367205677555\n",
      " for epoch : 437/500, loss is 0.6669147492782768\n",
      " for epoch : 438/500, loss is 0.6668929202536599\n",
      " for epoch : 439/500, loss is 0.6668712321634875\n",
      " for epoch : 440/500, loss is 0.6668496836915998\n",
      " for epoch : 441/500, loss is 0.6668282735359248\n",
      " for epoch : 442/500, loss is 0.6668070004083159\n",
      " for epoch : 443/500, loss is 0.6667858630343881\n",
      " for epoch : 444/500, loss is 0.6667648601533582\n",
      " for epoch : 445/500, loss is 0.6667439905178867\n",
      " for epoch : 446/500, loss is 0.666723252893921\n",
      " for epoch : 447/500, loss is 0.6667026460605426\n",
      " for epoch : 448/500, loss is 0.6666821688098139\n",
      " for epoch : 449/500, loss is 0.6666618199466291\n",
      " for epoch : 450/500, loss is 0.6666415982885653\n",
      " for epoch : 451/500, loss is 0.6666215026657377\n",
      " for epoch : 452/500, loss is 0.6666015319206535\n",
      " for epoch : 453/500, loss is 0.6665816849080716\n",
      " for epoch : 454/500, loss is 0.6665619604948605\n",
      " for epoch : 455/500, loss is 0.6665423575598601\n",
      " for epoch : 456/500, loss is 0.666522874993746\n",
      " for epoch : 457/500, loss is 0.6665035116988919\n",
      " for epoch : 458/500, loss is 0.6664842665892384\n",
      " for epoch : 459/500, loss is 0.6664651385901599\n",
      " for epoch : 460/500, loss is 0.6664461266383356\n",
      " for epoch : 461/500, loss is 0.6664272296816194\n",
      " for epoch : 462/500, loss is 0.6664084466789155\n",
      " for epoch : 463/500, loss is 0.6663897766000514\n",
      " for epoch : 464/500, loss is 0.6663712184256545\n",
      " for epoch : 465/500, loss is 0.6663527711470304\n",
      " for epoch : 466/500, loss is 0.6663344337660424\n",
      " for epoch : 467/500, loss is 0.6663162052949922\n",
      " for epoch : 468/500, loss is 0.6662980847565027\n",
      " for epoch : 469/500, loss is 0.6662800711834008\n",
      " for epoch : 470/500, loss is 0.6662621636186042\n",
      " for epoch : 471/500, loss is 0.6662443611150073\n",
      " for epoch : 472/500, loss is 0.6662266627353692\n",
      " for epoch : 473/500, loss is 0.6662090675522038\n",
      " for epoch : 474/500, loss is 0.6661915746476702\n",
      " for epoch : 475/500, loss is 0.6661741831134649\n",
      " for epoch : 476/500, loss is 0.6661568920507152\n",
      " for epoch : 477/500, loss is 0.6661397005698747\n",
      " for epoch : 478/500, loss is 0.6661226077906186\n",
      " for epoch : 479/500, loss is 0.6661056128417411\n",
      " for epoch : 480/500, loss is 0.6660887148610545\n",
      " for epoch : 481/500, loss is 0.6660719129952886\n",
      " for epoch : 482/500, loss is 0.6660552063999915\n",
      " for epoch : 483/500, loss is 0.6660385942394326\n",
      " for epoch : 484/500, loss is 0.6660220756865046\n",
      " for epoch : 485/500, loss is 0.6660056499226299\n",
      " for epoch : 486/500, loss is 0.6659893161376641\n",
      " for epoch : 487/500, loss is 0.6659730735298048\n",
      " for epoch : 488/500, loss is 0.6659569213054983\n",
      " for epoch : 489/500, loss is 0.6659408586793486\n",
      " for epoch : 490/500, loss is 0.6659248848740279\n",
      " for epoch : 491/500, loss is 0.6659089991201878\n",
      " for epoch : 492/500, loss is 0.6658932006563708\n",
      " for epoch : 493/500, loss is 0.6658774887289247\n",
      " for epoch : 494/500, loss is 0.6658618625919152\n",
      " for epoch : 495/500, loss is 0.6658463215070428\n",
      " for epoch : 496/500, loss is 0.665830864743558\n",
      " for epoch : 497/500, loss is 0.6658154915781788\n",
      " for epoch : 498/500, loss is 0.6658002012950088\n",
      " for epoch : 499/500, loss is 0.6657849931854569\n"
     ]
    }
   ],
   "source": [
    "# creating model\n",
    "model = SimpleNN(X_train_tensor)\n",
    "\n",
    "print(model.bias)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # forward pass\n",
    "    \n",
    "    y_pred = model.forward_pass(X_train_tensor)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = model.loss_calculation(y_pred, y_train_tensor)\n",
    "    print(f\" for epoch : {epoch}/{epochs}, loss is {loss.item()}\")\n",
    "    \n",
    "    # backward pass\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    # parameters update\n",
    "    with torch.no_grad():\n",
    "        model.weights -= learning_rate * model.weights.grad\n",
    "        model.bias -= learning_rate * model.bias.grad\n",
    "\n",
    "    # zero gradients\n",
    "    model.weights.grad.zero_()\n",
    "    model.bias.grad.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fe591585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 63.16%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_test_pred = model.forward_pass(X_test_tensor)\n",
    "    y_test_pred_labels = (y_test_pred >= 0.5).float()\n",
    "    \n",
    "    accuracy = (y_test_pred_labels.squeeze() == y_test_tensor).float().mean()\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy.item()*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
