{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34f220fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install kagglehub\n",
    "# ! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "665254d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da07b9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Colab cache for faster access to the 'fashionmnist' dataset.\n",
      "Path to dataset files: /kaggle/input/fashionmnist\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"zalando-research/fashionmnist\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "458f0287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-0eb9dff3-f2a6-4150-9fc2-820d816fd3aa\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0eb9dff3-f2a6-4150-9fc2-820d816fd3aa')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-0eb9dff3-f2a6-4150-9fc2-820d816fd3aa button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-0eb9dff3-f2a6-4150-9fc2-820d816fd3aa');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      2       0       0       0       0       0       0       0       0   \n",
       "1      9       0       0       0       0       0       0       0       0   \n",
       "2      6       0       0       0       0       0       0       0       5   \n",
       "3      0       0       0       0       1       2       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0        30        43         0   \n",
       "3       0  ...         3         0         0         0         0         1   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path + \"/fashion-mnist_train.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed3d9f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.sample(n=30000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91d1fd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 785)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f902ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample_df.iloc[:, 1:].values\n",
    "y = sample_df.iloc[:, :1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0281f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8069da1b",
   "metadata": {},
   "source": [
    "# Pre - Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0dee91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "161b236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = st_scaler.fit_transform(X_train)\n",
    "X_test = st_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "570c1302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0100328 , -0.03207572, -0.04945422, ..., -0.16188057,\n",
       "        -0.09292675, -0.03794802],\n",
       "       [-0.0100328 , -0.03207572, -0.04945422, ..., -0.16188057,\n",
       "        -0.09292675, -0.03794802],\n",
       "       [-0.0100328 , -0.03207572, -0.04945422, ..., -0.16188057,\n",
       "        -0.09292675, -0.03794802],\n",
       "       ...,\n",
       "       [-0.0100328 , -0.03207572, -0.04945422, ..., -0.16188057,\n",
       "        -0.09292675, -0.03794802],\n",
       "       [-0.0100328 , -0.03207572, -0.04945422, ..., -0.16188057,\n",
       "        -0.09292675, -0.03794802],\n",
       "       [-0.0100328 , -0.03207572, -0.04945422, ..., -0.16188057,\n",
       "        -0.09292675, -0.03794802]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58818cd",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc1b2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom dataset class\n",
    "\n",
    "class FashionMNISTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, features, labels):\n",
    "        print(type(features))\n",
    "        print(type(labels))\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "241b9d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FashionMNISTDataset(X_train, y_train)\n",
    "test_dataset = FashionMNISTDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7751f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955f25f",
   "metadata": {},
   "source": [
    "# OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61dcb9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # objective function\n",
    "\n",
    "# def objective_function(trial):\n",
    "\n",
    "#     # next hyperparameter values\n",
    "#     num_hidden_layers = trial.suggest_int(\"num_hidden_layers\", 1, 5, step=1, log=False)\n",
    "#     neurons_per_layer = trial.suggest_int(\"neurons_per_layer\", 8, 128, step=8, log=False)\n",
    "\n",
    "\n",
    "#     # model init\n",
    "#     input_dim = 784\n",
    "#     output_dim = 10\n",
    "\n",
    "#     class \n",
    "\n",
    "\n",
    "\n",
    "#     # params init\n",
    "\n",
    "#     # training loop\n",
    "\n",
    "#     # evaluation\n",
    "\n",
    "\n",
    "#     # return accuracy\n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac48a366",
   "metadata": {},
   "source": [
    "# NN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faab5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.BatchNorm1d(128), # batch normaliza\n",
    "        cnt +=1tion layer to stabilize and accelerate training, applied before activation functions, \n",
    "                                 #       128 - is the number of features from the previous layer, 1d as we have 1 dimensional data\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2), # using dropout to prevent overfitting, dropout rate of 20%, used to prevent overfitting, applied after activation functions\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(16, 10),\n",
    "            # nn.Softmax(dim=1) - softmax is not required in PyTorch as it is included in the CrossEntropyLoss\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5bcc291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set learning rate and epochs\n",
    "epochs = 300\n",
    "\n",
    "learning_rate = 0.03\n",
    "\n",
    "\n",
    "# instantiate the model\n",
    "\n",
    "model = MyNN(X_train.shape[1])\n",
    "\n",
    "# define the loss function\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.0001) # weight decay is mathematically equivalent to L2 regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4adc7cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0e7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Epoch 1/300 -----------\n",
      " avg epoch loss: 1.173630410194397 \n",
      "----------- Epoch 2/300 -----------\n",
      " avg epoch loss: 0.74077583471934 \n",
      "----------- Epoch 3/300 -----------\n",
      " avg epoch loss: 0.6506297153631846 \n",
      "----------- Epoch 4/300 -----------\n",
      " avg epoch loss: 0.6076108044783274 \n",
      "----------- Epoch 5/300 -----------\n",
      " avg epoch loss: 0.5721842919985454 \n",
      "----------- Epoch 6/300 -----------\n",
      " avg epoch loss: 0.5444732427597045 \n",
      "----------- Epoch 7/300 -----------\n",
      " avg epoch loss: 0.5141473467350006 \n",
      "----------- Epoch 8/300 -----------\n",
      " avg epoch loss: 0.5005755435625712 \n",
      "----------- Epoch 9/300 -----------\n",
      " avg epoch loss: 0.4842878180742264 \n",
      "----------- Epoch 10/300 -----------\n",
      " avg epoch loss: 0.46880221549669904 \n",
      "----------- Epoch 11/300 -----------\n",
      " avg epoch loss: 0.4562778458595276 \n",
      "----------- Epoch 12/300 -----------\n",
      " avg epoch loss: 0.44742696424325307 \n",
      "----------- Epoch 13/300 -----------\n",
      " avg epoch loss: 0.43250371384620667 \n",
      "----------- Epoch 14/300 -----------\n",
      " avg epoch loss: 0.4312198567787806 \n",
      "----------- Epoch 15/300 -----------\n",
      " avg epoch loss: 0.4208492325146993 \n",
      "----------- Epoch 16/300 -----------\n",
      " avg epoch loss: 0.41713675634066266 \n",
      "----------- Epoch 17/300 -----------\n",
      " avg epoch loss: 0.4049936782916387 \n",
      "----------- Epoch 18/300 -----------\n",
      " avg epoch loss: 0.3977154717842738 \n",
      "----------- Epoch 19/300 -----------\n",
      " avg epoch loss: 0.3939205524126689 \n",
      "----------- Epoch 20/300 -----------\n",
      " avg epoch loss: 0.3793084791103999 \n",
      "----------- Epoch 21/300 -----------\n",
      " avg epoch loss: 0.38387585039933525 \n",
      "----------- Epoch 22/300 -----------\n",
      " avg epoch loss: 0.3745390410423279 \n",
      "----------- Epoch 23/300 -----------\n",
      " avg epoch loss: 0.3667033805052439 \n",
      "----------- Epoch 24/300 -----------\n",
      " avg epoch loss: 0.362771092971166 \n",
      "----------- Epoch 25/300 -----------\n",
      " avg epoch loss: 0.35819142309824625 \n",
      "----------- Epoch 26/300 -----------\n",
      " avg epoch loss: 0.35881432429949445 \n",
      "----------- Epoch 27/300 -----------\n",
      " avg epoch loss: 0.3477569110393524 \n",
      "----------- Epoch 28/300 -----------\n",
      " avg epoch loss: 0.3417886253197988 \n",
      "----------- Epoch 29/300 -----------\n",
      " avg epoch loss: 0.3368401359319687 \n",
      "----------- Epoch 30/300 -----------\n",
      " avg epoch loss: 0.3382191673318545 \n",
      "----------- Epoch 31/300 -----------\n",
      " avg epoch loss: 0.3283833547830582 \n",
      "----------- Epoch 32/300 -----------\n",
      " avg epoch loss: 0.32949133882919945 \n",
      "----------- Epoch 33/300 -----------\n",
      " avg epoch loss: 0.3255674882729848 \n",
      "----------- Epoch 34/300 -----------\n",
      " avg epoch loss: 0.3197273401816686 \n",
      "----------- Epoch 35/300 -----------\n",
      " avg epoch loss: 0.31467060720920564 \n",
      "----------- Epoch 36/300 -----------\n",
      " avg epoch loss: 0.31033048353592557 \n",
      "----------- Epoch 37/300 -----------\n",
      " avg epoch loss: 0.3159527703921 \n",
      "----------- Epoch 38/300 -----------\n",
      " avg epoch loss: 0.3168892371455828 \n",
      "----------- Epoch 39/300 -----------\n",
      " avg epoch loss: 0.30616149155298866 \n",
      "----------- Epoch 40/300 -----------\n",
      " avg epoch loss: 0.29902834645907084 \n",
      "----------- Epoch 41/300 -----------\n",
      " avg epoch loss: 0.2988466256260872 \n",
      "----------- Epoch 42/300 -----------\n",
      " avg epoch loss: 0.29009547611077624 \n",
      "----------- Epoch 43/300 -----------\n",
      " avg epoch loss: 0.29525400944550834 \n",
      "----------- Epoch 44/300 -----------\n",
      " avg epoch loss: 0.2985952219963074 \n",
      "----------- Epoch 45/300 -----------\n",
      " avg epoch loss: 0.2949548244277636 \n",
      "----------- Epoch 46/300 -----------\n",
      " avg epoch loss: 0.28021380615234376 \n",
      "----------- Epoch 47/300 -----------\n",
      " avg epoch loss: 0.2871488377451897 \n",
      "----------- Epoch 48/300 -----------\n",
      " avg epoch loss: 0.27439500335852307 \n",
      "----------- Epoch 49/300 -----------\n",
      " avg epoch loss: 0.27056094153722127 \n",
      "----------- Epoch 50/300 -----------\n",
      " avg epoch loss: 0.27392672846714655 \n",
      "----------- Epoch 51/300 -----------\n",
      " avg epoch loss: 0.2719037035306295 \n",
      "----------- Epoch 52/300 -----------\n",
      " avg epoch loss: 0.26823765285809836 \n",
      "----------- Epoch 53/300 -----------\n",
      " avg epoch loss: 0.264680513938268 \n",
      "----------- Epoch 54/300 -----------\n",
      " avg epoch loss: 0.25741934327284494 \n",
      "----------- Epoch 55/300 -----------\n",
      " avg epoch loss: 0.26330117203791936 \n",
      "----------- Epoch 56/300 -----------\n",
      " avg epoch loss: 0.26092230345805484 \n",
      "----------- Epoch 57/300 -----------\n",
      " avg epoch loss: 0.2535987001856168 \n",
      "----------- Epoch 58/300 -----------\n",
      " avg epoch loss: 0.2576828494469325 \n",
      "----------- Epoch 59/300 -----------\n",
      " avg epoch loss: 0.2580698548555374 \n",
      "----------- Epoch 60/300 -----------\n",
      " avg epoch loss: 0.2525914132197698 \n",
      "----------- Epoch 61/300 -----------\n",
      " avg epoch loss: 0.24413273054361342 \n",
      "----------- Epoch 62/300 -----------\n",
      " avg epoch loss: 0.24715453565120696 \n",
      "----------- Epoch 63/300 -----------\n",
      " avg epoch loss: 0.23976459883650145 \n",
      "----------- Epoch 64/300 -----------\n",
      " avg epoch loss: 0.2489710608522097 \n",
      "----------- Epoch 65/300 -----------\n",
      " avg epoch loss: 0.23557434250911077 \n",
      "----------- Epoch 66/300 -----------\n",
      " avg epoch loss: 0.23645588518182437 \n",
      "----------- Epoch 67/300 -----------\n",
      " avg epoch loss: 0.23840346606572468 \n",
      "----------- Epoch 68/300 -----------\n",
      " avg epoch loss: 0.23085238736867905 \n",
      "----------- Epoch 69/300 -----------\n",
      " avg epoch loss: 0.23738419449329376 \n",
      "----------- Epoch 70/300 -----------\n",
      " avg epoch loss: 0.22723139613866805 \n",
      "----------- Epoch 71/300 -----------\n",
      " avg epoch loss: 0.2293081617554029 \n",
      "----------- Epoch 72/300 -----------\n",
      " avg epoch loss: 0.22697486280401546 \n",
      "----------- Epoch 73/300 -----------\n",
      " avg epoch loss: 0.21940247434377672 \n",
      "----------- Epoch 74/300 -----------\n",
      " avg epoch loss: 0.22227608876427016 \n",
      "----------- Epoch 75/300 -----------\n",
      " avg epoch loss: 0.22095918688178062 \n",
      "----------- Epoch 76/300 -----------\n",
      " avg epoch loss: 0.22394173511862756 \n",
      "----------- Epoch 77/300 -----------\n",
      " avg epoch loss: 0.21688177889585494 \n",
      "----------- Epoch 78/300 -----------\n",
      " avg epoch loss: 0.21618131123979886 \n",
      "----------- Epoch 79/300 -----------\n",
      " avg epoch loss: 0.22056198089321455 \n",
      "----------- Epoch 80/300 -----------\n",
      " avg epoch loss: 0.21362905517220498 \n",
      "----------- Epoch 81/300 -----------\n",
      " avg epoch loss: 0.21344455507397653 \n",
      "----------- Epoch 82/300 -----------\n",
      " avg epoch loss: 0.21209704809387525 \n",
      "----------- Epoch 83/300 -----------\n",
      " avg epoch loss: 0.2106457272271315 \n",
      "----------- Epoch 84/300 -----------\n",
      " avg epoch loss: 0.20273219497998554 \n",
      "----------- Epoch 85/300 -----------\n",
      " avg epoch loss: 0.20540918617447218 \n",
      "----------- Epoch 86/300 -----------\n",
      " avg epoch loss: 0.21164877238869667 \n",
      "----------- Epoch 87/300 -----------\n",
      " avg epoch loss: 0.2041349456210931 \n",
      "----------- Epoch 88/300 -----------\n",
      " avg epoch loss: 0.2013595467209816 \n",
      "----------- Epoch 89/300 -----------\n",
      " avg epoch loss: 0.20143170341849326 \n",
      "----------- Epoch 90/300 -----------\n",
      " avg epoch loss: 0.19493634102741877 \n",
      "----------- Epoch 91/300 -----------\n",
      " avg epoch loss: 0.20721260803937913 \n",
      "----------- Epoch 92/300 -----------\n",
      " avg epoch loss: 0.20065208169817925 \n",
      "----------- Epoch 93/300 -----------\n",
      " avg epoch loss: 0.19219257604579132 \n",
      "----------- Epoch 94/300 -----------\n",
      " avg epoch loss: 0.19607048718134562 \n",
      "----------- Epoch 95/300 -----------\n",
      " avg epoch loss: 0.20065918206175168 \n",
      "----------- Epoch 96/300 -----------\n",
      " avg epoch loss: 0.197203835606575 \n",
      "----------- Epoch 97/300 -----------\n",
      " avg epoch loss: 0.19421068692207336 \n",
      "----------- Epoch 98/300 -----------\n",
      " avg epoch loss: 0.18823162627220152 \n",
      "----------- Epoch 99/300 -----------\n",
      " avg epoch loss: 0.1847846303085486 \n",
      "----------- Epoch 100/300 -----------\n",
      " avg epoch loss: 0.1861831408639749 \n",
      "----------- Epoch 101/300 -----------\n",
      " avg epoch loss: 0.19052246866126854 \n",
      "----------- Epoch 102/300 -----------\n",
      " avg epoch loss: 0.18578772672017416 \n",
      "----------- Epoch 103/300 -----------\n",
      " avg epoch loss: 0.19142778482039768 \n",
      "----------- Epoch 104/300 -----------\n",
      " avg epoch loss: 0.1841625231007735 \n",
      "----------- Epoch 105/300 -----------\n",
      " avg epoch loss: 0.18940812018016975 \n",
      "----------- Epoch 106/300 -----------\n",
      " avg epoch loss: 0.18756769904494286 \n",
      "----------- Epoch 107/300 -----------\n",
      " avg epoch loss: 0.18307670105497043 \n",
      "----------- Epoch 108/300 -----------\n",
      " avg epoch loss: 0.17578216056029003 \n",
      "----------- Epoch 109/300 -----------\n",
      " avg epoch loss: 0.1845427078107993 \n",
      "----------- Epoch 110/300 -----------\n",
      " avg epoch loss: 0.179108571767807 \n",
      "----------- Epoch 111/300 -----------\n",
      " avg epoch loss: 0.17547067613899708 \n",
      "----------- Epoch 112/300 -----------\n",
      " avg epoch loss: 0.17647490528225898 \n",
      "----------- Epoch 113/300 -----------\n",
      " avg epoch loss: 0.1771825249195099 \n",
      "----------- Epoch 114/300 -----------\n",
      " avg epoch loss: 0.17328049804270268 \n",
      "----------- Epoch 115/300 -----------\n",
      " avg epoch loss: 0.172729122141997 \n",
      "----------- Epoch 116/300 -----------\n",
      " avg epoch loss: 0.16982863596081735 \n",
      "----------- Epoch 117/300 -----------\n",
      " avg epoch loss: 0.17679007047911485 \n",
      "----------- Epoch 118/300 -----------\n",
      " avg epoch loss: 0.1666420495013396 \n",
      "----------- Epoch 119/300 -----------\n",
      " avg epoch loss: 0.1689662440220515 \n",
      "----------- Epoch 120/300 -----------\n",
      " avg epoch loss: 0.17001288305719695 \n",
      "----------- Epoch 121/300 -----------\n",
      " avg epoch loss: 0.17045250517129898 \n",
      "----------- Epoch 122/300 -----------\n",
      " avg epoch loss: 0.1663624464770158 \n",
      "----------- Epoch 123/300 -----------\n",
      " avg epoch loss: 0.16978445437550543 \n",
      "----------- Epoch 124/300 -----------\n",
      " avg epoch loss: 0.17456180215875308 \n",
      "----------- Epoch 125/300 -----------\n",
      " avg epoch loss: 0.16538260115186373 \n",
      "----------- Epoch 126/300 -----------\n",
      " avg epoch loss: 0.17248557104667028 \n",
      "----------- Epoch 127/300 -----------\n",
      " avg epoch loss: 0.16151298723121485 \n",
      "----------- Epoch 128/300 -----------\n",
      " avg epoch loss: 0.1711314535140991 \n",
      "----------- Epoch 129/300 -----------\n",
      " avg epoch loss: 0.16546012846628824 \n",
      "----------- Epoch 130/300 -----------\n",
      " avg epoch loss: 0.1669249365925789 \n",
      "----------- Epoch 131/300 -----------\n",
      " avg epoch loss: 0.160557315270106 \n",
      "----------- Epoch 132/300 -----------\n",
      " avg epoch loss: 0.15590042531490325 \n",
      "----------- Epoch 133/300 -----------\n",
      " avg epoch loss: 0.16215352069338163 \n",
      "----------- Epoch 134/300 -----------\n",
      " avg epoch loss: 0.1618084841420253 \n",
      "----------- Epoch 135/300 -----------\n",
      " avg epoch loss: 0.1569810712983211 \n",
      "----------- Epoch 136/300 -----------\n",
      " avg epoch loss: 0.1601935292482376 \n",
      "----------- Epoch 137/300 -----------\n",
      " avg epoch loss: 0.1586886857151985 \n",
      "----------- Epoch 138/300 -----------\n",
      " avg epoch loss: 0.15786662779251734 \n",
      "----------- Epoch 139/300 -----------\n",
      " avg epoch loss: 0.15807946556806565 \n",
      "----------- Epoch 140/300 -----------\n",
      " avg epoch loss: 0.1532093282888333 \n",
      "----------- Epoch 141/300 -----------\n",
      " avg epoch loss: 0.15485348087549208 \n",
      "----------- Epoch 142/300 -----------\n",
      " avg epoch loss: 0.15384632292886574 \n",
      "----------- Epoch 143/300 -----------\n",
      " avg epoch loss: 0.15598236625393233 \n",
      "----------- Epoch 144/300 -----------\n",
      " avg epoch loss: 0.15111178591847418 \n",
      "----------- Epoch 145/300 -----------\n",
      " avg epoch loss: 0.15636922026177247 \n",
      "----------- Epoch 146/300 -----------\n",
      " avg epoch loss: 0.14568174283703167 \n",
      "----------- Epoch 147/300 -----------\n",
      " avg epoch loss: 0.1467300523320834 \n",
      "----------- Epoch 148/300 -----------\n",
      " avg epoch loss: 0.15059557150801023 \n",
      "----------- Epoch 149/300 -----------\n",
      " avg epoch loss: 0.14635523563126723 \n",
      "----------- Epoch 150/300 -----------\n",
      " avg epoch loss: 0.143380917151769 \n",
      "----------- Epoch 151/300 -----------\n",
      " avg epoch loss: 0.15061080825328826 \n",
      "----------- Epoch 152/300 -----------\n",
      " avg epoch loss: 0.14904273427526157 \n",
      "----------- Epoch 153/300 -----------\n",
      " avg epoch loss: 0.14965037331481776 \n",
      "----------- Epoch 154/300 -----------\n",
      " avg epoch loss: 0.14994972016414007 \n",
      "----------- Epoch 155/300 -----------\n",
      " avg epoch loss: 0.1380996432552735 \n",
      "----------- Epoch 156/300 -----------\n",
      " avg epoch loss: 0.1441925404469172 \n",
      "----------- Epoch 157/300 -----------\n",
      " avg epoch loss: 0.14552833521366118 \n",
      "----------- Epoch 158/300 -----------\n",
      " avg epoch loss: 0.14754143111407758 \n",
      "----------- Epoch 159/300 -----------\n",
      " avg epoch loss: 0.14296168849865595 \n",
      "----------- Epoch 160/300 -----------\n",
      " avg epoch loss: 0.1468183067838351 \n",
      "----------- Epoch 161/300 -----------\n",
      " avg epoch loss: 0.14108958405752978 \n",
      "----------- Epoch 162/300 -----------\n",
      " avg epoch loss: 0.1411926611463229 \n",
      "----------- Epoch 163/300 -----------\n",
      " avg epoch loss: 0.1455481231013934 \n",
      "----------- Epoch 164/300 -----------\n",
      " avg epoch loss: 0.1445536433905363 \n",
      "----------- Epoch 165/300 -----------\n",
      " avg epoch loss: 0.1465281415830056 \n",
      "----------- Epoch 166/300 -----------\n",
      " avg epoch loss: 0.1448728586783012 \n",
      "----------- Epoch 167/300 -----------\n",
      " avg epoch loss: 0.14103903215875227 \n",
      "----------- Epoch 168/300 -----------\n",
      " avg epoch loss: 0.13545704693595567 \n",
      "----------- Epoch 169/300 -----------\n",
      " avg epoch loss: 0.13865871794025103 \n",
      "----------- Epoch 170/300 -----------\n",
      " avg epoch loss: 0.13658940842250983 \n",
      "----------- Epoch 171/300 -----------\n",
      " avg epoch loss: 0.14169546430806318 \n",
      "----------- Epoch 172/300 -----------\n",
      " avg epoch loss: 0.1386073920528094 \n",
      "----------- Epoch 173/300 -----------\n",
      " avg epoch loss: 0.13863262728353343 \n",
      "----------- Epoch 174/300 -----------\n",
      " avg epoch loss: 0.1363586365779241 \n",
      "----------- Epoch 175/300 -----------\n",
      " avg epoch loss: 0.1422048969666163 \n",
      "----------- Epoch 176/300 -----------\n",
      " avg epoch loss: 0.1445489696909984 \n",
      "----------- Epoch 177/300 -----------\n",
      " avg epoch loss: 0.1309743782778581 \n",
      "----------- Epoch 178/300 -----------\n",
      " avg epoch loss: 0.1292630485743284 \n",
      "----------- Epoch 179/300 -----------\n",
      " avg epoch loss: 0.13432502027849355 \n",
      "----------- Epoch 180/300 -----------\n",
      " avg epoch loss: 0.13478126469254495 \n",
      "----------- Epoch 181/300 -----------\n",
      " avg epoch loss: 0.12727668579419454 \n",
      "----------- Epoch 182/300 -----------\n",
      " avg epoch loss: 0.1356107399960359 \n",
      "----------- Epoch 183/300 -----------\n",
      " avg epoch loss: 0.1341196943943699 \n",
      "----------- Epoch 184/300 -----------\n",
      " avg epoch loss: 0.14056551099320252 \n",
      "----------- Epoch 185/300 -----------\n",
      " avg epoch loss: 0.13141831985364358 \n",
      "----------- Epoch 186/300 -----------\n",
      " avg epoch loss: 0.13219730265935262 \n",
      "----------- Epoch 187/300 -----------\n",
      " avg epoch loss: 0.1313790001620849 \n",
      "----------- Epoch 188/300 -----------\n",
      " avg epoch loss: 0.13591321636736392 \n",
      "----------- Epoch 189/300 -----------\n",
      " avg epoch loss: 0.1392285323292017 \n",
      "----------- Epoch 190/300 -----------\n",
      " avg epoch loss: 0.13693695789078872 \n",
      "----------- Epoch 191/300 -----------\n",
      " avg epoch loss: 0.1301868105828762 \n",
      "----------- Epoch 192/300 -----------\n",
      " avg epoch loss: 0.13308239937822025 \n",
      "----------- Epoch 193/300 -----------\n",
      " avg epoch loss: 0.1374873909552892 \n",
      "----------- Epoch 194/300 -----------\n",
      " avg epoch loss: 0.1283980952600638 \n",
      "----------- Epoch 195/300 -----------\n",
      " avg epoch loss: 0.12766641588509084 \n",
      "----------- Epoch 196/300 -----------\n",
      " avg epoch loss: 0.13309416836003463 \n",
      "----------- Epoch 197/300 -----------\n",
      " avg epoch loss: 0.13281549075245858 \n",
      "----------- Epoch 198/300 -----------\n",
      " avg epoch loss: 0.13513853319485983 \n",
      "----------- Epoch 199/300 -----------\n",
      " avg epoch loss: 0.13544907875855763 \n",
      "----------- Epoch 200/300 -----------\n",
      " avg epoch loss: 0.12483490040153265 \n",
      "----------- Epoch 201/300 -----------\n",
      " avg epoch loss: 0.12575288763642312 \n",
      "----------- Epoch 202/300 -----------\n",
      " avg epoch loss: 0.12405766269067924 \n",
      "----------- Epoch 203/300 -----------\n",
      " avg epoch loss: 0.12049178480605284 \n",
      "----------- Epoch 204/300 -----------\n",
      " avg epoch loss: 0.12486130333940187 \n",
      "----------- Epoch 205/300 -----------\n",
      " avg epoch loss: 0.12751353740195434 \n",
      "----------- Epoch 206/300 -----------\n",
      " avg epoch loss: 0.12475802957763274 \n",
      "----------- Epoch 207/300 -----------\n",
      " avg epoch loss: 0.122527183547616 \n",
      "----------- Epoch 208/300 -----------\n",
      " avg epoch loss: 0.1245090684046348 \n",
      "----------- Epoch 209/300 -----------\n",
      " avg epoch loss: 0.12938798591742912 \n",
      "----------- Epoch 210/300 -----------\n",
      " avg epoch loss: 0.12938041473428408 \n",
      "----------- Epoch 211/300 -----------\n",
      " avg epoch loss: 0.11966316431760787 \n",
      "----------- Epoch 212/300 -----------\n",
      " avg epoch loss: 0.12292430879672368 \n",
      "----------- Epoch 213/300 -----------\n",
      " avg epoch loss: 0.12514440231770277 \n",
      "----------- Epoch 214/300 -----------\n",
      " avg epoch loss: 0.12121725130081176 \n",
      "----------- Epoch 215/300 -----------\n",
      " avg epoch loss: 0.11898120939731598 \n",
      "----------- Epoch 216/300 -----------\n",
      " avg epoch loss: 0.1253325233658155 \n",
      "----------- Epoch 217/300 -----------\n",
      " avg epoch loss: 0.12422722267607848 \n",
      "----------- Epoch 218/300 -----------\n",
      " avg epoch loss: 0.11980056876440844 \n",
      "----------- Epoch 219/300 -----------\n",
      " avg epoch loss: 0.12624220428367455 \n",
      "----------- Epoch 220/300 -----------\n",
      " avg epoch loss: 0.12115902219216029 \n",
      "----------- Epoch 221/300 -----------\n",
      " avg epoch loss: 0.12343450658520062 \n",
      "----------- Epoch 222/300 -----------\n",
      " avg epoch loss: 0.12449239839116732 \n",
      "----------- Epoch 223/300 -----------\n",
      " avg epoch loss: 0.120032552977403 \n",
      "----------- Epoch 224/300 -----------\n",
      " avg epoch loss: 0.12050610092530648 \n",
      "----------- Epoch 225/300 -----------\n",
      " avg epoch loss: 0.12761879145105678 \n",
      "----------- Epoch 226/300 -----------\n",
      " avg epoch loss: 0.1246965466439724 \n",
      "----------- Epoch 227/300 -----------\n",
      " avg epoch loss: 0.11636043132841586 \n",
      "----------- Epoch 228/300 -----------\n",
      " avg epoch loss: 0.11458393830060959 \n",
      "----------- Epoch 229/300 -----------\n",
      " avg epoch loss: 0.11364639469981194 \n",
      "----------- Epoch 230/300 -----------\n",
      " avg epoch loss: 0.12172049075861772 \n",
      "----------- Epoch 231/300 -----------\n",
      " avg epoch loss: 0.11435810781394441 \n",
      "----------- Epoch 232/300 -----------\n",
      " avg epoch loss: 0.11935485903173686 \n",
      "----------- Epoch 233/300 -----------\n",
      " avg epoch loss: 0.11656947880486647 \n",
      "----------- Epoch 234/300 -----------\n",
      " avg epoch loss: 0.11881167899568876 \n",
      "----------- Epoch 235/300 -----------\n",
      " avg epoch loss: 0.12371796280890703 \n",
      "----------- Epoch 236/300 -----------\n",
      " avg epoch loss: 0.11947226103643577 \n",
      "----------- Epoch 237/300 -----------\n",
      " avg epoch loss: 0.10747691841920216 \n",
      "----------- Epoch 238/300 -----------\n",
      " avg epoch loss: 0.11779026988148689 \n",
      "----------- Epoch 239/300 -----------\n",
      " avg epoch loss: 0.1132929810633262 \n",
      "----------- Epoch 240/300 -----------\n",
      " avg epoch loss: 0.11186992611487706 \n",
      "----------- Epoch 241/300 -----------\n",
      " avg epoch loss: 0.11146943847338359 \n",
      "----------- Epoch 242/300 -----------\n",
      " avg epoch loss: 0.11523829818765323 \n",
      "----------- Epoch 243/300 -----------\n",
      " avg epoch loss: 0.11548052534212669 \n",
      "----------- Epoch 244/300 -----------\n",
      " avg epoch loss: 0.12125999530156453 \n",
      "----------- Epoch 245/300 -----------\n",
      " avg epoch loss: 0.11637358107914528 \n",
      "----------- Epoch 246/300 -----------\n",
      " avg epoch loss: 0.11817852466056744 \n",
      "----------- Epoch 247/300 -----------\n",
      " avg epoch loss: 0.11083144913365443 \n",
      "----------- Epoch 248/300 -----------\n",
      " avg epoch loss: 0.12268340600033602 \n",
      "----------- Epoch 249/300 -----------\n",
      " avg epoch loss: 0.1133872680614392 \n",
      "----------- Epoch 250/300 -----------\n",
      " avg epoch loss: 0.12058025661607584 \n",
      "----------- Epoch 251/300 -----------\n",
      " avg epoch loss: 0.11516226261854172 \n",
      "----------- Epoch 252/300 -----------\n",
      " avg epoch loss: 0.11959165707727273 \n",
      "----------- Epoch 253/300 -----------\n",
      " avg epoch loss: 0.11920963485787313 \n",
      "----------- Epoch 254/300 -----------\n",
      " avg epoch loss: 0.11619912274430196 \n",
      "----------- Epoch 255/300 -----------\n",
      " avg epoch loss: 0.1134566793491443 \n",
      "----------- Epoch 256/300 -----------\n",
      " avg epoch loss: 0.11400296523918708 \n",
      "----------- Epoch 257/300 -----------\n",
      " avg epoch loss: 0.1147837360004584 \n",
      "----------- Epoch 258/300 -----------\n",
      " avg epoch loss: 0.11224892094482979 \n",
      "----------- Epoch 259/300 -----------\n",
      " avg epoch loss: 0.11300098129858573 \n",
      "----------- Epoch 260/300 -----------\n",
      " avg epoch loss: 0.11297697905202707 \n",
      "----------- Epoch 261/300 -----------\n",
      " avg epoch loss: 0.11023653320223092 \n",
      "----------- Epoch 262/300 -----------\n",
      " avg epoch loss: 0.1112285133128365 \n",
      "----------- Epoch 263/300 -----------\n",
      " avg epoch loss: 0.11038064721723398 \n",
      "----------- Epoch 264/300 -----------\n",
      " avg epoch loss: 0.10089595109472672 \n",
      "----------- Epoch 265/300 -----------\n",
      " avg epoch loss: 0.10840760476638873 \n",
      "----------- Epoch 266/300 -----------\n",
      " avg epoch loss: 0.10509025776634613 \n",
      "----------- Epoch 267/300 -----------\n",
      " avg epoch loss: 0.10493153575062751 \n",
      "----------- Epoch 268/300 -----------\n",
      " avg epoch loss: 0.10856460469216109 \n",
      "----------- Epoch 269/300 -----------\n",
      " avg epoch loss: 0.11011586459477743 \n",
      "----------- Epoch 270/300 -----------\n",
      " avg epoch loss: 0.11399967769285044 \n",
      "----------- Epoch 271/300 -----------\n",
      " avg epoch loss: 0.11207052188615005 \n",
      "----------- Epoch 272/300 -----------\n",
      " avg epoch loss: 0.10598812959094843 \n",
      "----------- Epoch 273/300 -----------\n",
      " avg epoch loss: 0.10987821864585082 \n",
      "----------- Epoch 274/300 -----------\n",
      " avg epoch loss: 0.10625830902407567 \n",
      "----------- Epoch 275/300 -----------\n",
      " avg epoch loss: 0.10006600938985745 \n",
      "----------- Epoch 276/300 -----------\n",
      " avg epoch loss: 0.10971899015208085 \n",
      "----------- Epoch 277/300 -----------\n",
      " avg epoch loss: 0.11138646604120732 \n",
      "----------- Epoch 278/300 -----------\n",
      " avg epoch loss: 0.10277657729138931 \n",
      "----------- Epoch 279/300 -----------\n",
      " avg epoch loss: 0.09937782382965088 \n",
      "----------- Epoch 280/300 -----------\n",
      " avg epoch loss: 0.10973229523872335 \n",
      "----------- Epoch 281/300 -----------\n",
      " avg epoch loss: 0.11059645354251067 \n",
      "----------- Epoch 282/300 -----------\n",
      " avg epoch loss: 0.10636061886449655 \n",
      "----------- Epoch 283/300 -----------\n",
      " avg epoch loss: 0.1068202836389343 \n",
      "----------- Epoch 284/300 -----------\n",
      " avg epoch loss: 0.10469867200404405 \n",
      "----------- Epoch 285/300 -----------\n",
      " avg epoch loss: 0.10082753391563892 \n",
      "----------- Epoch 286/300 -----------\n",
      " avg epoch loss: 0.10487388594945272 \n",
      "----------- Epoch 287/300 -----------\n",
      " avg epoch loss: 0.10960530207306146 \n",
      "----------- Epoch 288/300 -----------\n",
      " avg epoch loss: 0.10396603426337242 \n",
      "----------- Epoch 289/300 -----------\n",
      " avg epoch loss: 0.10645975692073505 \n",
      "----------- Epoch 290/300 -----------\n",
      " avg epoch loss: 0.10679518917699656 \n",
      "----------- Epoch 291/300 -----------\n",
      " avg epoch loss: 0.10348076199243467 \n",
      "----------- Epoch 292/300 -----------\n",
      " avg epoch loss: 0.1071807241688172 \n",
      "----------- Epoch 293/300 -----------\n",
      " avg epoch loss: 0.10132764557749033 \n",
      "----------- Epoch 294/300 -----------\n",
      " avg epoch loss: 0.10787581433107456 \n",
      "----------- Epoch 295/300 -----------\n",
      " avg epoch loss: 0.10635352043062449 \n",
      "----------- Epoch 296/300 -----------\n",
      " avg epoch loss: 0.11172936388601859 \n",
      "----------- Epoch 297/300 -----------\n",
      " avg epoch loss: 0.10885258099685112 \n",
      "----------- Epoch 298/300 -----------\n",
      " avg epoch loss: 0.10409070122490327 \n",
      "----------- Epoch 299/300 -----------\n",
      " avg epoch loss: 0.10018822111686071 \n",
      "----------- Epoch 300/300 -----------\n",
      " avg epoch loss: 0.10244773784528176 \n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "i = 1\n",
    "total_epoch_loss = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"----------- Epoch {epoch+1}/{epochs} -----------\")\n",
    "    epoch_loss = []\n",
    "\n",
    "    for batch_features, batch_labels in train_loader:\n",
    "\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(batch_features)\n",
    "\n",
    "      \n",
    "        \n",
    "\n",
    "        # loss calculation\n",
    "        loss = loss_function(outputs, batch_labels.squeeze())\n",
    "        total_epoch_loss.append(loss.item())\n",
    "\n",
    "        # backprop\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "    print(f\" avg epoch loss: {sum(epoch_loss)/len(epoch_loss)} \")\n",
    "\n",
    "        # update gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc9d282f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110bc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_: tensor([22.7425, 18.8435, 19.0904, 19.5416, 22.1023,  5.3151, 29.7748,  3.8748,\n",
      "        15.3527, 24.0269,  6.4164,  9.6461,  9.0416, 20.6596,  6.8374, 17.8770,\n",
      "         8.6189, 15.0795,  9.8777, 22.4432,  8.3527,  8.4552, 20.9496, 12.2945,\n",
      "        15.6015,  6.1589, 12.2478, 27.4259, 20.5952, 11.1070, 14.0889, 15.5195,\n",
      "         7.2379, 11.3217, 13.8385, 10.9708, 23.9063, 15.1887, 13.9677, 14.5753,\n",
      "         6.3640,  9.9296, 11.5687, 17.7541, 14.5306,  6.9519, 20.4176, 11.4977,\n",
      "        18.2846, 14.2363, 12.6667, 20.7846,  6.4981, 21.2807, 17.6593, 13.9336,\n",
      "        25.2330, 27.3003, 16.1608, 15.1575, 12.6086, 19.3762, 22.1285,  6.8764]), len: 64\n",
      "Predicted: tensor([1, 1, 5, 1, 5, 6, 9, 6, 2, 1, 4, 7, 0, 1, 3, 6, 4, 3, 0, 7, 4, 2, 7, 4,\n",
      "        7, 4, 4, 5, 8, 2, 7, 2, 6, 9, 0, 2, 8, 8, 4, 7, 6, 0, 3, 2, 3, 3, 3, 2,\n",
      "        9, 0, 4, 3, 6, 9, 3, 6, 1, 8, 7, 8, 1, 9, 3, 8])\n",
      "Actual   : tensor([1, 1, 5, 1, 5, 2, 9, 4, 2, 1, 2, 7, 0, 1, 3, 6, 6, 3, 0, 7, 4, 2, 7, 4,\n",
      "        9, 2, 4, 5, 8, 2, 7, 2, 6, 9, 0, 2, 8, 8, 4, 7, 0, 0, 3, 2, 3, 3, 3, 2,\n",
      "        9, 0, 4, 3, 6, 9, 3, 0, 1, 8, 7, 8, 1, 9, 3, 8])\n",
      " total cnt : 94\n",
      "Accuracy of the model on the test set: 88.32%\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "model.eval()\n",
    "\n",
    "\n",
    "total = 0\n",
    "correct = 0 \n",
    "i = 1\n",
    "cnt =0\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_labels in test_loader:\n",
    "\n",
    "        cnt +=1\n",
    "            \n",
    "        test_outputs = model(batch_features)\n",
    "\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        \n",
    "        if i <2:\n",
    "            print(f\"_: {_}, len: {len(_)}\")\n",
    "            print(\"Predicted:\", predicted)\n",
    "            print(\"Actual   :\", batch_labels.squeeze())\n",
    "            i += 1\n",
    "        \n",
    "        total += batch_labels.shape[0]\n",
    "        correct += (predicted == batch_labels.squeeze()).sum().item()\n",
    "\n",
    "print(f\" total cnt : {cnt}\")\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy of the model on the test set: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c629b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of the model on the test set: 84.45%\n",
    "# Accuracy of the model on the test set: 84.70%\n",
    "# Accuracy of the model on the test set: 84.30%\n",
    "# Accuracy of the model on the test set: 84.90%\n",
    "# Accuracy of the model on the test set: 87.45%\n",
    "# Accuracy of the model on the test set: 85.92% - increasing the batch size to 100 \n",
    "# Accuracy of the model on the test set: 87.08% - batch size 100, learning rate 0.05, hidden layers: 128, 64, 16, 10,  dropout layers added\n",
    "# Accuracy of the model on the test set: 87.67% - batch size 64, learning rate 0.05, hidden layers: 128, 64, 16, 10,  dropout layers added, batch normalization layers added\n",
    "# Accuracy of the model on the test set: 86.30% - batch size 32, learning rate 0.05, hidden layers: 128, 64, 16, 10,  dropout layers added, batch normalization layers added\n",
    "# Accuracy of the model on the test set: 88.32% - batch size 64, learning rate 0.05, hidden layers: 128, 64, 16, 10, decay: .0001, epochs- 300, dropout layers added, batch normalization layers added\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d0f5b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
