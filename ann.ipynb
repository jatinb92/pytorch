{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34f220fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install kagglehub\n",
    "# ! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "665254d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da07b9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/codespace/.cache/kagglehub/datasets/zalando-research/fashionmnist/versions/4\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"zalando-research/fashionmnist\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "458f0287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      2       0       0       0       0       0       0       0       0   \n",
       "1      9       0       0       0       0       0       0       0       0   \n",
       "2      6       0       0       0       0       0       0       0       5   \n",
       "3      0       0       0       0       1       2       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0        30        43         0   \n",
       "3       0  ...         3         0         0         0         0         1   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path + \"/fashion-mnist_train.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d9f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.sample(n=30000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91d1fd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 785)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0f902ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample_df.iloc[:, 1:].values\n",
    "y = sample_df.iloc[:, :1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a0281f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8069da1b",
   "metadata": {},
   "source": [
    "# Pre - Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b0dee91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "161b236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = st_scaler.fit_transform(X_train)\n",
    "X_test = st_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "570c1302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01348522, -0.03527256, -0.0498121 , ...,  2.38657474,\n",
       "         0.01416161, -0.04111855],\n",
       "       [-0.01348522, -0.03527256, -0.0498121 , ..., -0.15810555,\n",
       "        -0.09040037, -0.04111855],\n",
       "       [-0.01348522, -0.03527256, -0.0498121 , ..., -0.15810555,\n",
       "        -0.09040037, -0.04111855],\n",
       "       ...,\n",
       "       [-0.01348522, -0.03527256, -0.0498121 , ..., -0.15810555,\n",
       "        -0.09040037, -0.04111855],\n",
       "       [-0.01348522, -0.03527256, -0.0498121 , ..., -0.15810555,\n",
       "        -0.09040037, -0.04111855],\n",
       "       [-0.01348522, -0.03527256, -0.0498121 , ..., -0.15810555,\n",
       "        -0.09040037, -0.04111855]], shape=(16000, 784))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58818cd",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dc1b2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom dataset class\n",
    "\n",
    "class FashionMNISTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, features, labels):\n",
    "        print(type(features))\n",
    "        print(type(labels))\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "241b9d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FashionMNISTDataset(X_train, y_train)\n",
    "test_dataset = FashionMNISTDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7751f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955f25f",
   "metadata": {},
   "source": [
    "# OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dcb9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective function\n",
    "\n",
    "def objective_function(trial):\n",
    "\n",
    "    # next hyperparameter values\n",
    "    num_hidden_layers = trial.suggest_int(\"num_hidden_layers\", 1, 5, step=1, log=False)\n",
    "    neurons_per_layer = trial.suggest_int(\"neurons_per_layer\", 8, 128, step=8, log=False)\n",
    "\n",
    "\n",
    "    # model init\n",
    "    input_dim = 784\n",
    "    output_dim = 10\n",
    "\n",
    "    class \n",
    "\n",
    "\n",
    "\n",
    "    # params init\n",
    "\n",
    "    # training loop\n",
    "\n",
    "    # evaluation\n",
    "\n",
    "\n",
    "    # return accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac48a366",
   "metadata": {},
   "source": [
    "# NN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6faab5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.BatchNorm1d(128), # batch normalization layer to stabilize and accelerate training, applied before activation functions, \n",
    "                                 #       128 - is the number of features from the previous layer, 1d as we have 1 dimensional data\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2), # using dropout to prevent overfitting, dropout rate of 20%, used to prevent overfitting, applied after activation functions\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(16, 10),\n",
    "            # nn.Softmax(dim=1) - softmax is not required in PyTorch as it is included in the CrossEntropyLoss\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bcc291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set learning rate and epochs\n",
    "epochs = 300\n",
    "\n",
    "learning_rate = 0.03\n",
    "\n",
    "\n",
    "# instantiate the model\n",
    "\n",
    "model = MyNN(X_train.shape[1])\n",
    "\n",
    "# define the loss function\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.0001) # weight decay is mathematically equivalent to L2 regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4adc7cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a1d0e7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Epoch 1/700 -----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " avg epoch loss: 1.2202758622169494 \n",
      "----------- Epoch 2/700 -----------\n",
      " avg epoch loss: 0.7627844249010086 \n",
      "----------- Epoch 3/700 -----------\n",
      " avg epoch loss: 0.6708186898231506 \n",
      "----------- Epoch 4/700 -----------\n",
      " avg epoch loss: 0.6181969386935234 \n",
      "----------- Epoch 5/700 -----------\n",
      " avg epoch loss: 0.5801185286343098 \n",
      "----------- Epoch 6/700 -----------\n",
      " avg epoch loss: 0.5541691065728664 \n",
      "----------- Epoch 7/700 -----------\n",
      " avg epoch loss: 0.5313997372090816 \n",
      "----------- Epoch 8/700 -----------\n",
      " avg epoch loss: 0.5168785937428474 \n",
      "----------- Epoch 9/700 -----------\n",
      " avg epoch loss: 0.4951062305569649 \n",
      "----------- Epoch 10/700 -----------\n",
      " avg epoch loss: 0.4924792742431164 \n",
      "----------- Epoch 11/700 -----------\n",
      " avg epoch loss: 0.4793404828608036 \n",
      "----------- Epoch 12/700 -----------\n",
      " avg epoch loss: 0.46812558647990227 \n",
      "----------- Epoch 13/700 -----------\n",
      " avg epoch loss: 0.44879987913370134 \n",
      "----------- Epoch 14/700 -----------\n",
      " avg epoch loss: 0.4467666516005993 \n",
      "----------- Epoch 15/700 -----------\n",
      " avg epoch loss: 0.43452028124034403 \n",
      "----------- Epoch 16/700 -----------\n",
      " avg epoch loss: 0.42218465442955494 \n",
      "----------- Epoch 17/700 -----------\n",
      " avg epoch loss: 0.4275183292031288 \n",
      "----------- Epoch 18/700 -----------\n",
      " avg epoch loss: 0.4103262887597084 \n",
      "----------- Epoch 19/700 -----------\n",
      " avg epoch loss: 0.3966603331267834 \n",
      "----------- Epoch 20/700 -----------\n",
      " avg epoch loss: 0.39605681851506236 \n",
      "----------- Epoch 21/700 -----------\n",
      " avg epoch loss: 0.392262477517128 \n",
      "----------- Epoch 22/700 -----------\n",
      " avg epoch loss: 0.3730764185637236 \n",
      "----------- Epoch 23/700 -----------\n",
      " avg epoch loss: 0.3919961188137531 \n",
      "----------- Epoch 24/700 -----------\n",
      " avg epoch loss: 0.3832221813350916 \n",
      "----------- Epoch 25/700 -----------\n",
      " avg epoch loss: 0.37133950215578077 \n",
      "----------- Epoch 26/700 -----------\n",
      " avg epoch loss: 0.36962560141086576 \n",
      "----------- Epoch 27/700 -----------\n",
      " avg epoch loss: 0.3615714780539274 \n",
      "----------- Epoch 28/700 -----------\n",
      " avg epoch loss: 0.36049291414022444 \n",
      "----------- Epoch 29/700 -----------\n",
      " avg epoch loss: 0.345889695122838 \n",
      "----------- Epoch 30/700 -----------\n",
      " avg epoch loss: 0.34466677233576776 \n",
      "----------- Epoch 31/700 -----------\n",
      " avg epoch loss: 0.349387236610055 \n",
      "----------- Epoch 32/700 -----------\n",
      " avg epoch loss: 0.3381523643285036 \n",
      "----------- Epoch 33/700 -----------\n",
      " avg epoch loss: 0.33769706811010836 \n",
      "----------- Epoch 34/700 -----------\n",
      " avg epoch loss: 0.33546315398812293 \n",
      "----------- Epoch 35/700 -----------\n",
      " avg epoch loss: 0.33087970834970476 \n",
      "----------- Epoch 36/700 -----------\n",
      " avg epoch loss: 0.32689458741247657 \n",
      "----------- Epoch 37/700 -----------\n",
      " avg epoch loss: 0.3186352905780077 \n",
      "----------- Epoch 38/700 -----------\n",
      " avg epoch loss: 0.3270132799595594 \n",
      "----------- Epoch 39/700 -----------\n",
      " avg epoch loss: 0.3199486862421036 \n",
      "----------- Epoch 40/700 -----------\n",
      " avg epoch loss: 0.31545156398415564 \n",
      "----------- Epoch 41/700 -----------\n",
      " avg epoch loss: 0.31412752625346185 \n",
      "----------- Epoch 42/700 -----------\n",
      " avg epoch loss: 0.30606422389298676 \n",
      "----------- Epoch 43/700 -----------\n",
      " avg epoch loss: 0.3057669643461704 \n",
      "----------- Epoch 44/700 -----------\n",
      " avg epoch loss: 0.29957402230799196 \n",
      "----------- Epoch 45/700 -----------\n",
      " avg epoch loss: 0.29206770891696215 \n",
      "----------- Epoch 46/700 -----------\n",
      " avg epoch loss: 0.2993786056265235 \n",
      "----------- Epoch 47/700 -----------\n",
      " avg epoch loss: 0.29873264279961587 \n",
      "----------- Epoch 48/700 -----------\n",
      " avg epoch loss: 0.2884995959028602 \n",
      "----------- Epoch 49/700 -----------\n",
      " avg epoch loss: 0.29460711766779424 \n",
      "----------- Epoch 50/700 -----------\n",
      " avg epoch loss: 0.2896100978925824 \n",
      "----------- Epoch 51/700 -----------\n",
      " avg epoch loss: 0.29532199167460205 \n",
      "----------- Epoch 52/700 -----------\n",
      " avg epoch loss: 0.2801889598146081 \n",
      "----------- Epoch 53/700 -----------\n",
      " avg epoch loss: 0.27678909359127285 \n",
      "----------- Epoch 54/700 -----------\n",
      " avg epoch loss: 0.27355109447985887 \n",
      "----------- Epoch 55/700 -----------\n",
      " avg epoch loss: 0.28896254022419454 \n",
      "----------- Epoch 56/700 -----------\n",
      " avg epoch loss: 0.28056259763240815 \n",
      "----------- Epoch 57/700 -----------\n",
      " avg epoch loss: 0.2757970700711012 \n",
      "----------- Epoch 58/700 -----------\n",
      " avg epoch loss: 0.27469596941024066 \n",
      "----------- Epoch 59/700 -----------\n",
      " avg epoch loss: 0.27550100737810135 \n",
      "----------- Epoch 60/700 -----------\n",
      " avg epoch loss: 0.27847480099648236 \n",
      "----------- Epoch 61/700 -----------\n",
      " avg epoch loss: 0.2603130995631218 \n",
      "----------- Epoch 62/700 -----------\n",
      " avg epoch loss: 0.2730287923440337 \n",
      "----------- Epoch 63/700 -----------\n",
      " avg epoch loss: 0.2626693611443043 \n",
      "----------- Epoch 64/700 -----------\n",
      " avg epoch loss: 0.2587493615746498 \n",
      "----------- Epoch 65/700 -----------\n",
      " avg epoch loss: 0.2532490867003798 \n",
      "----------- Epoch 66/700 -----------\n",
      " avg epoch loss: 0.2549770869016647 \n",
      "----------- Epoch 67/700 -----------\n",
      " avg epoch loss: 0.257521317794919 \n",
      "----------- Epoch 68/700 -----------\n",
      " avg epoch loss: 0.2579485915452242 \n",
      "----------- Epoch 69/700 -----------\n",
      " avg epoch loss: 0.253749034717679 \n",
      "----------- Epoch 70/700 -----------\n",
      " avg epoch loss: 0.25597764787822963 \n",
      "----------- Epoch 71/700 -----------\n",
      " avg epoch loss: 0.25011483614891766 \n",
      "----------- Epoch 72/700 -----------\n",
      " avg epoch loss: 0.24728623455762863 \n",
      "----------- Epoch 73/700 -----------\n",
      " avg epoch loss: 0.2323780998568982 \n",
      "----------- Epoch 74/700 -----------\n",
      " avg epoch loss: 0.25404717509448527 \n",
      "----------- Epoch 75/700 -----------\n",
      " avg epoch loss: 0.24339668365567924 \n",
      "----------- Epoch 76/700 -----------\n",
      " avg epoch loss: 0.2448079907298088 \n",
      "----------- Epoch 77/700 -----------\n",
      " avg epoch loss: 0.2376662702783942 \n",
      "----------- Epoch 78/700 -----------\n",
      " avg epoch loss: 0.24402582263574005 \n",
      "----------- Epoch 79/700 -----------\n",
      " avg epoch loss: 0.23682780243456364 \n",
      "----------- Epoch 80/700 -----------\n",
      " avg epoch loss: 0.23662863616272808 \n",
      "----------- Epoch 81/700 -----------\n",
      " avg epoch loss: 0.23459873870015144 \n",
      "----------- Epoch 82/700 -----------\n",
      " avg epoch loss: 0.22792014629766344 \n",
      "----------- Epoch 83/700 -----------\n",
      " avg epoch loss: 0.2288833301216364 \n",
      "----------- Epoch 84/700 -----------\n",
      " avg epoch loss: 0.23186957801878452 \n",
      "----------- Epoch 85/700 -----------\n",
      " avg epoch loss: 0.23435113982856273 \n",
      "----------- Epoch 86/700 -----------\n",
      " avg epoch loss: 0.23439863147959114 \n",
      "----------- Epoch 87/700 -----------\n",
      " avg epoch loss: 0.233365676805377 \n",
      "----------- Epoch 88/700 -----------\n",
      " avg epoch loss: 0.22139566150307655 \n",
      "----------- Epoch 89/700 -----------\n",
      " avg epoch loss: 0.22402443565055727 \n",
      "----------- Epoch 90/700 -----------\n",
      " avg epoch loss: 0.21667698815464972 \n",
      "----------- Epoch 91/700 -----------\n",
      " avg epoch loss: 0.22650253381580115 \n",
      "----------- Epoch 92/700 -----------\n",
      " avg epoch loss: 0.22803740230202674 \n",
      "----------- Epoch 93/700 -----------\n",
      " avg epoch loss: 0.22444755825027823 \n",
      "----------- Epoch 94/700 -----------\n",
      " avg epoch loss: 0.21829444701969625 \n",
      "----------- Epoch 95/700 -----------\n",
      " avg epoch loss: 0.22333166149631142 \n",
      "----------- Epoch 96/700 -----------\n",
      " avg epoch loss: 0.22006957187876106 \n",
      "----------- Epoch 97/700 -----------\n",
      " avg epoch loss: 0.21347599352523686 \n",
      "----------- Epoch 98/700 -----------\n",
      " avg epoch loss: 0.21750710773095489 \n",
      "----------- Epoch 99/700 -----------\n",
      " avg epoch loss: 0.22821832321584223 \n",
      "----------- Epoch 100/700 -----------\n",
      " avg epoch loss: 0.20550034299492836 \n",
      "----------- Epoch 101/700 -----------\n",
      " avg epoch loss: 0.20864644043892622 \n",
      "----------- Epoch 102/700 -----------\n",
      " avg epoch loss: 0.21008801535144447 \n",
      "----------- Epoch 103/700 -----------\n",
      " avg epoch loss: 0.2156423797234893 \n",
      "----------- Epoch 104/700 -----------\n",
      " avg epoch loss: 0.2094693837687373 \n",
      "----------- Epoch 105/700 -----------\n",
      " avg epoch loss: 0.20815939826145768 \n",
      "----------- Epoch 106/700 -----------\n",
      " avg epoch loss: 0.21227141856774687 \n",
      "----------- Epoch 107/700 -----------\n",
      " avg epoch loss: 0.20671048614755272 \n",
      "----------- Epoch 108/700 -----------\n",
      " avg epoch loss: 0.1971870858706534 \n",
      "----------- Epoch 109/700 -----------\n",
      " avg epoch loss: 0.21007538847438992 \n",
      "----------- Epoch 110/700 -----------\n",
      " avg epoch loss: 0.20978735882788896 \n",
      "----------- Epoch 111/700 -----------\n",
      " avg epoch loss: 0.2063946262523532 \n",
      "----------- Epoch 112/700 -----------\n",
      " avg epoch loss: 0.20999211843311788 \n",
      "----------- Epoch 113/700 -----------\n",
      " avg epoch loss: 0.20100923090055584 \n",
      "----------- Epoch 114/700 -----------\n",
      " avg epoch loss: 0.20905421110242606 \n",
      "----------- Epoch 115/700 -----------\n",
      " avg epoch loss: 0.1929846459850669 \n",
      "----------- Epoch 116/700 -----------\n",
      " avg epoch loss: 0.18871096062660217 \n",
      "----------- Epoch 117/700 -----------\n",
      " avg epoch loss: 0.20889033558219672 \n",
      "----------- Epoch 118/700 -----------\n",
      " avg epoch loss: 0.19844959118217229 \n",
      "----------- Epoch 119/700 -----------\n",
      " avg epoch loss: 0.19271683798357844 \n",
      "----------- Epoch 120/700 -----------\n",
      " avg epoch loss: 0.20176975597441196 \n",
      "----------- Epoch 121/700 -----------\n",
      " avg epoch loss: 0.19657833306118846 \n",
      "----------- Epoch 122/700 -----------\n",
      " avg epoch loss: 0.20441075662523508 \n",
      "----------- Epoch 123/700 -----------\n",
      " avg epoch loss: 0.19510338923707604 \n",
      "----------- Epoch 124/700 -----------\n",
      " avg epoch loss: 0.19065188211575151 \n",
      "----------- Epoch 125/700 -----------\n",
      " avg epoch loss: 0.190397978650406 \n",
      "----------- Epoch 126/700 -----------\n",
      " avg epoch loss: 0.18860689543560147 \n",
      "----------- Epoch 127/700 -----------\n",
      " avg epoch loss: 0.18110855366848408 \n",
      "----------- Epoch 128/700 -----------\n",
      " avg epoch loss: 0.1881317477710545 \n",
      "----------- Epoch 129/700 -----------\n",
      " avg epoch loss: 0.1880354655329138 \n",
      "----------- Epoch 130/700 -----------\n",
      " avg epoch loss: 0.19736364977806806 \n",
      "----------- Epoch 131/700 -----------\n",
      " avg epoch loss: 0.1987816436253488 \n",
      "----------- Epoch 132/700 -----------\n",
      " avg epoch loss: 0.18442622537910938 \n",
      "----------- Epoch 133/700 -----------\n",
      " avg epoch loss: 0.19187397054582833 \n",
      "----------- Epoch 134/700 -----------\n",
      " avg epoch loss: 0.19090793629549443 \n",
      "----------- Epoch 135/700 -----------\n",
      " avg epoch loss: 0.1820331256184727 \n",
      "----------- Epoch 136/700 -----------\n",
      " avg epoch loss: 0.175359800696373 \n",
      "----------- Epoch 137/700 -----------\n",
      " avg epoch loss: 0.18656225118786096 \n",
      "----------- Epoch 138/700 -----------\n",
      " avg epoch loss: 0.18535057452321052 \n",
      "----------- Epoch 139/700 -----------\n",
      " avg epoch loss: 0.1765098468903452 \n",
      "----------- Epoch 140/700 -----------\n",
      " avg epoch loss: 0.18658650521934034 \n",
      "----------- Epoch 141/700 -----------\n",
      " avg epoch loss: 0.18370928014814852 \n",
      "----------- Epoch 142/700 -----------\n",
      " avg epoch loss: 0.17583362948521972 \n",
      "----------- Epoch 143/700 -----------\n",
      " avg epoch loss: 0.18611646232753992 \n",
      "----------- Epoch 144/700 -----------\n",
      " avg epoch loss: 0.1850737715177238 \n",
      "----------- Epoch 145/700 -----------\n",
      " avg epoch loss: 0.1838457082118839 \n",
      "----------- Epoch 146/700 -----------\n",
      " avg epoch loss: 0.1898619050104171 \n",
      "----------- Epoch 147/700 -----------\n",
      " avg epoch loss: 0.17699411425739528 \n",
      "----------- Epoch 148/700 -----------\n",
      " avg epoch loss: 0.1708789990544319 \n",
      "----------- Epoch 149/700 -----------\n",
      " avg epoch loss: 0.169661168217659 \n",
      "----------- Epoch 150/700 -----------\n",
      " avg epoch loss: 0.1803181101344526 \n",
      "----------- Epoch 151/700 -----------\n",
      " avg epoch loss: 0.17659177101403475 \n",
      "----------- Epoch 152/700 -----------\n",
      " avg epoch loss: 0.17619335243664683 \n",
      "----------- Epoch 153/700 -----------\n",
      " avg epoch loss: 0.17442520462349057 \n",
      "----------- Epoch 154/700 -----------\n",
      " avg epoch loss: 0.1763263363484293 \n",
      "----------- Epoch 155/700 -----------\n",
      " avg epoch loss: 0.18157852621003986 \n",
      "----------- Epoch 156/700 -----------\n",
      " avg epoch loss: 0.1753511062413454 \n",
      "----------- Epoch 157/700 -----------\n",
      " avg epoch loss: 0.17926017681136727 \n",
      "----------- Epoch 158/700 -----------\n",
      " avg epoch loss: 0.17790787824243307 \n",
      "----------- Epoch 159/700 -----------\n",
      " avg epoch loss: 0.17083058100007475 \n",
      "----------- Epoch 160/700 -----------\n",
      " avg epoch loss: 0.17069087715633213 \n",
      "----------- Epoch 161/700 -----------\n",
      " avg epoch loss: 0.18156090478226541 \n",
      "----------- Epoch 162/700 -----------\n",
      " avg epoch loss: 0.17329559317976237 \n",
      "----------- Epoch 163/700 -----------\n",
      " avg epoch loss: 0.17122591997683048 \n",
      "----------- Epoch 164/700 -----------\n",
      " avg epoch loss: 0.16444494954496622 \n",
      "----------- Epoch 165/700 -----------\n",
      " avg epoch loss: 0.16302977600507437 \n",
      "----------- Epoch 166/700 -----------\n",
      " avg epoch loss: 0.16854351647384463 \n",
      "----------- Epoch 167/700 -----------\n",
      " avg epoch loss: 0.1622247114740312 \n",
      "----------- Epoch 168/700 -----------\n",
      " avg epoch loss: 0.16987511345185338 \n",
      "----------- Epoch 169/700 -----------\n",
      " avg epoch loss: 0.17041968558728696 \n",
      "----------- Epoch 170/700 -----------\n",
      " avg epoch loss: 0.17174431877210736 \n",
      "----------- Epoch 171/700 -----------\n",
      " avg epoch loss: 0.17308835036307574 \n",
      "----------- Epoch 172/700 -----------\n",
      " avg epoch loss: 0.16511782325059177 \n",
      "----------- Epoch 173/700 -----------\n",
      " avg epoch loss: 0.1673468144647777 \n",
      "----------- Epoch 174/700 -----------\n",
      " avg epoch loss: 0.16416867055743933 \n",
      "----------- Epoch 175/700 -----------\n",
      " avg epoch loss: 0.15844855935685337 \n",
      "----------- Epoch 176/700 -----------\n",
      " avg epoch loss: 0.16507376272417604 \n",
      "----------- Epoch 177/700 -----------\n",
      " avg epoch loss: 0.1580922496858984 \n",
      "----------- Epoch 178/700 -----------\n",
      " avg epoch loss: 0.15714206286892296 \n",
      "----------- Epoch 179/700 -----------\n",
      " avg epoch loss: 0.15731955965422093 \n",
      "----------- Epoch 180/700 -----------\n",
      " avg epoch loss: 0.16755226933583617 \n",
      "----------- Epoch 181/700 -----------\n",
      " avg epoch loss: 0.16072492204234004 \n",
      "----------- Epoch 182/700 -----------\n",
      " avg epoch loss: 0.16242346205562352 \n",
      "----------- Epoch 183/700 -----------\n",
      " avg epoch loss: 0.16186953374743462 \n",
      "----------- Epoch 184/700 -----------\n",
      " avg epoch loss: 0.16636670457944275 \n",
      "----------- Epoch 185/700 -----------\n",
      " avg epoch loss: 0.16140560393221676 \n",
      "----------- Epoch 186/700 -----------\n",
      " avg epoch loss: 0.1515281506627798 \n",
      "----------- Epoch 187/700 -----------\n",
      " avg epoch loss: 0.15968443986214698 \n",
      "----------- Epoch 188/700 -----------\n",
      " avg epoch loss: 0.15856338266655803 \n",
      "----------- Epoch 189/700 -----------\n",
      " avg epoch loss: 0.16337494376674294 \n",
      "----------- Epoch 190/700 -----------\n",
      " avg epoch loss: 0.16633496334776282 \n",
      "----------- Epoch 191/700 -----------\n",
      " avg epoch loss: 0.1644390589110553 \n",
      "----------- Epoch 192/700 -----------\n",
      " avg epoch loss: 0.16173536343872547 \n",
      "----------- Epoch 193/700 -----------\n",
      " avg epoch loss: 0.14962304067611695 \n",
      "----------- Epoch 194/700 -----------\n",
      " avg epoch loss: 0.17611308570578693 \n",
      "----------- Epoch 195/700 -----------\n",
      " avg epoch loss: 0.16047636429406703 \n",
      "----------- Epoch 196/700 -----------\n",
      " avg epoch loss: 0.1447443227339536 \n",
      "----------- Epoch 197/700 -----------\n",
      " avg epoch loss: 0.1482514567822218 \n",
      "----------- Epoch 198/700 -----------\n",
      " avg epoch loss: 0.15802304377034307 \n",
      "----------- Epoch 199/700 -----------\n",
      " avg epoch loss: 0.1556687983945012 \n",
      "----------- Epoch 200/700 -----------\n",
      " avg epoch loss: 0.1465408944170922 \n",
      "----------- Epoch 201/700 -----------\n",
      " avg epoch loss: 0.16613894300721585 \n",
      "----------- Epoch 202/700 -----------\n",
      " avg epoch loss: 0.15455892611853778 \n",
      "----------- Epoch 203/700 -----------\n",
      " avg epoch loss: 0.15409786382317542 \n",
      "----------- Epoch 204/700 -----------\n",
      " avg epoch loss: 0.14554417449980975 \n",
      "----------- Epoch 205/700 -----------\n",
      " avg epoch loss: 0.1524335378371179 \n",
      "----------- Epoch 206/700 -----------\n",
      " avg epoch loss: 0.15204202118329704 \n",
      "----------- Epoch 207/700 -----------\n",
      " avg epoch loss: 0.15043382496759294 \n",
      "----------- Epoch 208/700 -----------\n",
      " avg epoch loss: 0.14797315563075245 \n",
      "----------- Epoch 209/700 -----------\n",
      " avg epoch loss: 0.15820045336708427 \n",
      "----------- Epoch 210/700 -----------\n",
      " avg epoch loss: 0.14732205128110945 \n",
      "----------- Epoch 211/700 -----------\n",
      " avg epoch loss: 0.1538469939008355 \n",
      "----------- Epoch 212/700 -----------\n",
      " avg epoch loss: 0.15057720736786723 \n",
      "----------- Epoch 213/700 -----------\n",
      " avg epoch loss: 0.14685178425349296 \n",
      "----------- Epoch 214/700 -----------\n",
      " avg epoch loss: 0.15300910778529941 \n",
      "----------- Epoch 215/700 -----------\n",
      " avg epoch loss: 0.14508610654436052 \n",
      "----------- Epoch 216/700 -----------\n",
      " avg epoch loss: 0.14676112225092947 \n",
      "----------- Epoch 217/700 -----------\n",
      " avg epoch loss: 0.13804489065427333 \n",
      "----------- Epoch 218/700 -----------\n",
      " avg epoch loss: 0.14583477144129575 \n",
      "----------- Epoch 219/700 -----------\n",
      " avg epoch loss: 0.14809259107336403 \n",
      "----------- Epoch 220/700 -----------\n",
      " avg epoch loss: 0.1472060157544911 \n",
      "----------- Epoch 221/700 -----------\n",
      " avg epoch loss: 0.15786292845197022 \n",
      "----------- Epoch 222/700 -----------\n",
      " avg epoch loss: 0.14128151878714562 \n",
      "----------- Epoch 223/700 -----------\n",
      " avg epoch loss: 0.13926037088781595 \n",
      "----------- Epoch 224/700 -----------\n",
      " avg epoch loss: 0.14702798319933935 \n",
      "----------- Epoch 225/700 -----------\n",
      " avg epoch loss: 0.1485264965649694 \n",
      "----------- Epoch 226/700 -----------\n",
      " avg epoch loss: 0.14217665668390692 \n",
      "----------- Epoch 227/700 -----------\n",
      " avg epoch loss: 0.14364891997911036 \n",
      "----------- Epoch 228/700 -----------\n",
      " avg epoch loss: 0.14018091974779964 \n",
      "----------- Epoch 229/700 -----------\n",
      " avg epoch loss: 0.13795616790466012 \n",
      "----------- Epoch 230/700 -----------\n",
      " avg epoch loss: 0.14528719115396962 \n",
      "----------- Epoch 231/700 -----------\n",
      " avg epoch loss: 0.13329735917411745 \n",
      "----------- Epoch 232/700 -----------\n",
      " avg epoch loss: 0.13845841808523981 \n",
      "----------- Epoch 233/700 -----------\n",
      " avg epoch loss: 0.13602905554976313 \n",
      "----------- Epoch 234/700 -----------\n",
      " avg epoch loss: 0.1455288599077612 \n",
      "----------- Epoch 235/700 -----------\n",
      " avg epoch loss: 0.1357928391844034 \n",
      "----------- Epoch 236/700 -----------\n",
      " avg epoch loss: 0.13796899023279546 \n",
      "----------- Epoch 237/700 -----------\n",
      " avg epoch loss: 0.13635715615842492 \n",
      "----------- Epoch 238/700 -----------\n",
      " avg epoch loss: 0.13805294526927173 \n",
      "----------- Epoch 239/700 -----------\n",
      " avg epoch loss: 0.14056156215071677 \n",
      "----------- Epoch 240/700 -----------\n",
      " avg epoch loss: 0.1340311244558543 \n",
      "----------- Epoch 241/700 -----------\n",
      " avg epoch loss: 0.14774873804673552 \n",
      "----------- Epoch 242/700 -----------\n",
      " avg epoch loss: 0.13922393357381224 \n",
      "----------- Epoch 243/700 -----------\n",
      " avg epoch loss: 0.14510095650702715 \n",
      "----------- Epoch 244/700 -----------\n",
      " avg epoch loss: 0.13853955334797502 \n",
      "----------- Epoch 245/700 -----------\n",
      " avg epoch loss: 0.14024334066547453 \n",
      "----------- Epoch 246/700 -----------\n",
      " avg epoch loss: 0.13115517720393838 \n",
      "----------- Epoch 247/700 -----------\n",
      " avg epoch loss: 0.13017810192517937 \n",
      "----------- Epoch 248/700 -----------\n",
      " avg epoch loss: 0.1370519834589213 \n",
      "----------- Epoch 249/700 -----------\n",
      " avg epoch loss: 0.13697729670628905 \n",
      "----------- Epoch 250/700 -----------\n",
      " avg epoch loss: 0.1367508239117451 \n",
      "----------- Epoch 251/700 -----------\n",
      " avg epoch loss: 0.14005050668679178 \n",
      "----------- Epoch 252/700 -----------\n",
      " avg epoch loss: 0.1413075160160661 \n",
      "----------- Epoch 253/700 -----------\n",
      " avg epoch loss: 0.12795371142961084 \n",
      "----------- Epoch 254/700 -----------\n",
      " avg epoch loss: 0.13228517958521843 \n",
      "----------- Epoch 255/700 -----------\n",
      " avg epoch loss: 0.1335763944759965 \n",
      "----------- Epoch 256/700 -----------\n",
      " avg epoch loss: 0.141137652550824 \n",
      "----------- Epoch 257/700 -----------\n",
      " avg epoch loss: 0.1382179743424058 \n",
      "----------- Epoch 258/700 -----------\n",
      " avg epoch loss: 0.13257616548612713 \n",
      "----------- Epoch 259/700 -----------\n",
      " avg epoch loss: 0.12618823685403913 \n",
      "----------- Epoch 260/700 -----------\n",
      " avg epoch loss: 0.13189582734927535 \n",
      "----------- Epoch 261/700 -----------\n",
      " avg epoch loss: 0.14488721406646074 \n",
      "----------- Epoch 262/700 -----------\n",
      " avg epoch loss: 0.13204490498173982 \n",
      "----------- Epoch 263/700 -----------\n",
      " avg epoch loss: 0.12523594107292593 \n",
      "----------- Epoch 264/700 -----------\n",
      " avg epoch loss: 0.1292508902773261 \n",
      "----------- Epoch 265/700 -----------\n",
      " avg epoch loss: 0.14593713274225592 \n",
      "----------- Epoch 266/700 -----------\n",
      " avg epoch loss: 0.1342427419759333 \n",
      "----------- Epoch 267/700 -----------\n",
      " avg epoch loss: 0.13005325237102808 \n",
      "----------- Epoch 268/700 -----------\n",
      " avg epoch loss: 0.13481487976759673 \n",
      "----------- Epoch 269/700 -----------\n",
      " avg epoch loss: 0.12906505526788534 \n",
      "----------- Epoch 270/700 -----------\n",
      " avg epoch loss: 0.13708873580023645 \n",
      "----------- Epoch 271/700 -----------\n",
      " avg epoch loss: 0.13970075998874382 \n",
      "----------- Epoch 272/700 -----------\n",
      " avg epoch loss: 0.13565800548903645 \n",
      "----------- Epoch 273/700 -----------\n",
      " avg epoch loss: 0.12608741287421435 \n",
      "----------- Epoch 274/700 -----------\n",
      " avg epoch loss: 0.12955627539753914 \n",
      "----------- Epoch 275/700 -----------\n",
      " avg epoch loss: 0.13153018673323094 \n",
      "----------- Epoch 276/700 -----------\n",
      " avg epoch loss: 0.12478789939545094 \n",
      "----------- Epoch 277/700 -----------\n",
      " avg epoch loss: 0.12649298952240498 \n",
      "----------- Epoch 278/700 -----------\n",
      " avg epoch loss: 0.13224085390008986 \n",
      "----------- Epoch 279/700 -----------\n",
      " avg epoch loss: 0.1341422057058662 \n",
      "----------- Epoch 280/700 -----------\n",
      " avg epoch loss: 0.1356609653076157 \n",
      "----------- Epoch 281/700 -----------\n",
      " avg epoch loss: 0.13521779549308122 \n",
      "----------- Epoch 282/700 -----------\n",
      " avg epoch loss: 0.1263058988135308 \n",
      "----------- Epoch 283/700 -----------\n",
      " avg epoch loss: 0.12763871821388603 \n",
      "----------- Epoch 284/700 -----------\n",
      " avg epoch loss: 0.13388355745654554 \n",
      "----------- Epoch 285/700 -----------\n",
      " avg epoch loss: 0.12700335919857025 \n",
      "----------- Epoch 286/700 -----------\n",
      " avg epoch loss: 0.13443737678788603 \n",
      "----------- Epoch 287/700 -----------\n",
      " avg epoch loss: 0.13081866635009645 \n",
      "----------- Epoch 288/700 -----------\n",
      " avg epoch loss: 0.12724342502839864 \n",
      "----------- Epoch 289/700 -----------\n",
      " avg epoch loss: 0.12643629951030017 \n",
      "----------- Epoch 290/700 -----------\n",
      " avg epoch loss: 0.12976957160700112 \n",
      "----------- Epoch 291/700 -----------\n",
      " avg epoch loss: 0.12832611337024719 \n",
      "----------- Epoch 292/700 -----------\n",
      " avg epoch loss: 0.12778402713593096 \n",
      "----------- Epoch 293/700 -----------\n",
      " avg epoch loss: 0.1227416180036962 \n",
      "----------- Epoch 294/700 -----------\n",
      " avg epoch loss: 0.12833470713160933 \n",
      "----------- Epoch 295/700 -----------\n",
      " avg epoch loss: 0.1339801330473274 \n",
      "----------- Epoch 296/700 -----------\n",
      " avg epoch loss: 0.12049335026741027 \n",
      "----------- Epoch 297/700 -----------\n",
      " avg epoch loss: 0.12601596128661186 \n",
      "----------- Epoch 298/700 -----------\n",
      " avg epoch loss: 0.12389235489629209 \n",
      "----------- Epoch 299/700 -----------\n",
      " avg epoch loss: 0.12386779227852822 \n",
      "----------- Epoch 300/700 -----------\n",
      " avg epoch loss: 0.1295059953071177 \n",
      "----------- Epoch 301/700 -----------\n",
      " avg epoch loss: 0.12888180913589894 \n",
      "----------- Epoch 302/700 -----------\n",
      " avg epoch loss: 0.12799772216845304 \n",
      "----------- Epoch 303/700 -----------\n",
      " avg epoch loss: 0.12011569484695792 \n",
      "----------- Epoch 304/700 -----------\n",
      " avg epoch loss: 0.127208358861506 \n",
      "----------- Epoch 305/700 -----------\n",
      " avg epoch loss: 0.12497426024731248 \n",
      "----------- Epoch 306/700 -----------\n",
      " avg epoch loss: 0.12197422977723181 \n",
      "----------- Epoch 307/700 -----------\n",
      " avg epoch loss: 0.12070439395587891 \n",
      "----------- Epoch 308/700 -----------\n",
      " avg epoch loss: 0.13272695746272803 \n",
      "----------- Epoch 309/700 -----------\n",
      " avg epoch loss: 0.1229579828158021 \n",
      "----------- Epoch 310/700 -----------\n",
      " avg epoch loss: 0.12876653093658388 \n",
      "----------- Epoch 311/700 -----------\n",
      " avg epoch loss: 0.11927531012706459 \n",
      "----------- Epoch 312/700 -----------\n",
      " avg epoch loss: 0.12044772721268236 \n",
      "----------- Epoch 313/700 -----------\n",
      " avg epoch loss: 0.11316918968502432 \n",
      "----------- Epoch 314/700 -----------\n",
      " avg epoch loss: 0.12745387103781103 \n",
      "----------- Epoch 315/700 -----------\n",
      " avg epoch loss: 0.12476642451435327 \n",
      "----------- Epoch 316/700 -----------\n",
      " avg epoch loss: 0.12452433252800256 \n",
      "----------- Epoch 317/700 -----------\n",
      " avg epoch loss: 0.12014547046460211 \n",
      "----------- Epoch 318/700 -----------\n",
      " avg epoch loss: 0.11975705580879002 \n",
      "----------- Epoch 319/700 -----------\n",
      " avg epoch loss: 0.12013531160634011 \n",
      "----------- Epoch 320/700 -----------\n",
      " avg epoch loss: 0.1285925150439143 \n",
      "----------- Epoch 321/700 -----------\n",
      " avg epoch loss: 0.11346863162890077 \n",
      "----------- Epoch 322/700 -----------\n",
      " avg epoch loss: 0.11342019186355173 \n",
      "----------- Epoch 323/700 -----------\n",
      " avg epoch loss: 0.1209993407735601 \n",
      "----------- Epoch 324/700 -----------\n",
      " avg epoch loss: 0.1178817623462528 \n",
      "----------- Epoch 325/700 -----------\n",
      " avg epoch loss: 0.12558543267659844 \n",
      "----------- Epoch 326/700 -----------\n",
      " avg epoch loss: 0.12757841044291854 \n",
      "----------- Epoch 327/700 -----------\n",
      " avg epoch loss: 0.12397045324370265 \n",
      "----------- Epoch 328/700 -----------\n",
      " avg epoch loss: 0.12205518029071391 \n",
      "----------- Epoch 329/700 -----------\n",
      " avg epoch loss: 0.11791173828486354 \n",
      "----------- Epoch 330/700 -----------\n",
      " avg epoch loss: 0.12365876295045018 \n",
      "----------- Epoch 331/700 -----------\n",
      " avg epoch loss: 0.12112961775436998 \n",
      "----------- Epoch 332/700 -----------\n",
      " avg epoch loss: 0.11568635914567858 \n",
      "----------- Epoch 333/700 -----------\n",
      " avg epoch loss: 0.11594399062776937 \n",
      "----------- Epoch 334/700 -----------\n",
      " avg epoch loss: 0.11999886330589651 \n",
      "----------- Epoch 335/700 -----------\n",
      " avg epoch loss: 0.12942159936018288 \n",
      "----------- Epoch 336/700 -----------\n",
      " avg epoch loss: 0.11998110205680132 \n",
      "----------- Epoch 337/700 -----------\n",
      " avg epoch loss: 0.1099600860113278 \n",
      "----------- Epoch 338/700 -----------\n",
      " avg epoch loss: 0.11216504423320293 \n",
      "----------- Epoch 339/700 -----------\n",
      " avg epoch loss: 0.11469648407306522 \n",
      "----------- Epoch 340/700 -----------\n",
      " avg epoch loss: 0.11569497396983207 \n",
      "----------- Epoch 341/700 -----------\n",
      " avg epoch loss: 0.12331536919763311 \n",
      "----------- Epoch 342/700 -----------\n",
      " avg epoch loss: 0.12145238585956394 \n",
      "----------- Epoch 343/700 -----------\n",
      " avg epoch loss: 0.11460197194572538 \n",
      "----------- Epoch 344/700 -----------\n",
      " avg epoch loss: 0.12191339712031185 \n",
      "----------- Epoch 345/700 -----------\n",
      " avg epoch loss: 0.11475946085527539 \n",
      "----------- Epoch 346/700 -----------\n",
      " avg epoch loss: 0.11597125566378236 \n",
      "----------- Epoch 347/700 -----------\n",
      " avg epoch loss: 0.12922112274169922 \n",
      "----------- Epoch 348/700 -----------\n",
      " avg epoch loss: 0.11221880284696817 \n",
      "----------- Epoch 349/700 -----------\n",
      " avg epoch loss: 0.12029222131613641 \n",
      "----------- Epoch 350/700 -----------\n",
      " avg epoch loss: 0.11552842340897769 \n",
      "----------- Epoch 351/700 -----------\n",
      " avg epoch loss: 0.12484648636355997 \n",
      "----------- Epoch 352/700 -----------\n",
      " avg epoch loss: 0.11363911284971982 \n",
      "----------- Epoch 353/700 -----------\n",
      " avg epoch loss: 0.10743625093344598 \n",
      "----------- Epoch 354/700 -----------\n",
      " avg epoch loss: 0.1171012827269733 \n",
      "----------- Epoch 355/700 -----------\n",
      " avg epoch loss: 0.11438428252097219 \n",
      "----------- Epoch 356/700 -----------\n",
      " avg epoch loss: 0.12456198281608521 \n",
      "----------- Epoch 357/700 -----------\n",
      " avg epoch loss: 0.11592882187105716 \n",
      "----------- Epoch 358/700 -----------\n",
      " avg epoch loss: 0.11040046142227948 \n",
      "----------- Epoch 359/700 -----------\n",
      " avg epoch loss: 0.11958758458774536 \n",
      "----------- Epoch 360/700 -----------\n",
      " avg epoch loss: 0.11248492911830545 \n",
      "----------- Epoch 361/700 -----------\n",
      " avg epoch loss: 0.11306341276224703 \n",
      "----------- Epoch 362/700 -----------\n",
      " avg epoch loss: 0.1189774754224345 \n",
      "----------- Epoch 363/700 -----------\n",
      " avg epoch loss: 0.12123695048782974 \n",
      "----------- Epoch 364/700 -----------\n",
      " avg epoch loss: 0.11451671350561082 \n",
      "----------- Epoch 365/700 -----------\n",
      " avg epoch loss: 0.11014946450758725 \n",
      "----------- Epoch 366/700 -----------\n",
      " avg epoch loss: 0.11203687810711563 \n",
      "----------- Epoch 367/700 -----------\n",
      " avg epoch loss: 0.1172068177293986 \n",
      "----------- Epoch 368/700 -----------\n",
      " avg epoch loss: 0.12029047002829611 \n",
      "----------- Epoch 369/700 -----------\n",
      " avg epoch loss: 0.11003033343469724 \n",
      "----------- Epoch 370/700 -----------\n",
      " avg epoch loss: 0.11254874215275049 \n",
      "----------- Epoch 371/700 -----------\n",
      " avg epoch loss: 0.121019552170299 \n",
      "----------- Epoch 372/700 -----------\n",
      " avg epoch loss: 0.11071197428833693 \n",
      "----------- Epoch 373/700 -----------\n",
      " avg epoch loss: 0.11230193673912436 \n",
      "----------- Epoch 374/700 -----------\n",
      " avg epoch loss: 0.11001908992882818 \n",
      "----------- Epoch 375/700 -----------\n",
      " avg epoch loss: 0.11548098466172814 \n",
      "----------- Epoch 376/700 -----------\n",
      " avg epoch loss: 0.11719821454491466 \n",
      "----------- Epoch 377/700 -----------\n",
      " avg epoch loss: 0.11114899433217942 \n",
      "----------- Epoch 378/700 -----------\n",
      " avg epoch loss: 0.11350364990718663 \n",
      "----------- Epoch 379/700 -----------\n",
      " avg epoch loss: 0.11890801312727854 \n",
      "----------- Epoch 380/700 -----------\n",
      " avg epoch loss: 0.1126264091199264 \n",
      "----------- Epoch 381/700 -----------\n",
      " avg epoch loss: 0.11775265123508871 \n",
      "----------- Epoch 382/700 -----------\n",
      " avg epoch loss: 0.10831662978138774 \n",
      "----------- Epoch 383/700 -----------\n",
      " avg epoch loss: 0.11485657298564911 \n",
      "----------- Epoch 384/700 -----------\n",
      " avg epoch loss: 0.11891157477535308 \n",
      "----------- Epoch 385/700 -----------\n",
      " avg epoch loss: 0.106053207334131 \n",
      "----------- Epoch 386/700 -----------\n",
      " avg epoch loss: 0.10559645019844174 \n",
      "----------- Epoch 387/700 -----------\n",
      " avg epoch loss: 0.11092197647038847 \n",
      "----------- Epoch 388/700 -----------\n",
      " avg epoch loss: 0.11417728673387319 \n",
      "----------- Epoch 389/700 -----------\n",
      " avg epoch loss: 0.11746166228037327 \n",
      "----------- Epoch 390/700 -----------\n",
      " avg epoch loss: 0.10621467757225037 \n",
      "----------- Epoch 391/700 -----------\n",
      " avg epoch loss: 0.11625160402059555 \n",
      "----------- Epoch 392/700 -----------\n",
      " avg epoch loss: 0.11058990374486893 \n",
      "----------- Epoch 393/700 -----------\n",
      " avg epoch loss: 0.11707217239495367 \n",
      "----------- Epoch 394/700 -----------\n",
      " avg epoch loss: 0.10920697940327227 \n",
      "----------- Epoch 395/700 -----------\n",
      " avg epoch loss: 0.10901478271186352 \n",
      "----------- Epoch 396/700 -----------\n",
      " avg epoch loss: 0.11368027306627482 \n",
      "----------- Epoch 397/700 -----------\n",
      " avg epoch loss: 0.11290038206335157 \n",
      "----------- Epoch 398/700 -----------\n",
      " avg epoch loss: 0.10730656412709505 \n",
      "----------- Epoch 399/700 -----------\n",
      " avg epoch loss: 0.11125620163418352 \n",
      "----------- Epoch 400/700 -----------\n",
      " avg epoch loss: 0.11337918362021446 \n",
      "----------- Epoch 401/700 -----------\n",
      " avg epoch loss: 0.11315666816011072 \n",
      "----------- Epoch 402/700 -----------\n",
      " avg epoch loss: 0.1139837114168331 \n",
      "----------- Epoch 403/700 -----------\n",
      " avg epoch loss: 0.10082139488589019 \n",
      "----------- Epoch 404/700 -----------\n",
      " avg epoch loss: 0.1033284478476271 \n",
      "----------- Epoch 405/700 -----------\n",
      " avg epoch loss: 0.11994606668222696 \n",
      "----------- Epoch 406/700 -----------\n",
      " avg epoch loss: 0.10972095916001126 \n",
      "----------- Epoch 407/700 -----------\n",
      " avg epoch loss: 0.1166219315258786 \n",
      "----------- Epoch 408/700 -----------\n",
      " avg epoch loss: 0.11224644475523382 \n",
      "----------- Epoch 409/700 -----------\n",
      " avg epoch loss: 0.11241891165915877 \n",
      "----------- Epoch 410/700 -----------\n",
      " avg epoch loss: 0.10869727262295782 \n",
      "----------- Epoch 411/700 -----------\n",
      " avg epoch loss: 0.11164961850177496 \n",
      "----------- Epoch 412/700 -----------\n",
      " avg epoch loss: 0.11169464029185473 \n",
      "----------- Epoch 413/700 -----------\n",
      " avg epoch loss: 0.1111671016360633 \n",
      "----------- Epoch 414/700 -----------\n",
      " avg epoch loss: 0.10955722806230188 \n",
      "----------- Epoch 415/700 -----------\n",
      " avg epoch loss: 0.1119406920298934 \n",
      "----------- Epoch 416/700 -----------\n",
      " avg epoch loss: 0.10029241076717153 \n",
      "----------- Epoch 417/700 -----------\n",
      " avg epoch loss: 0.11475370396580548 \n",
      "----------- Epoch 418/700 -----------\n",
      " avg epoch loss: 0.11254144316725433 \n",
      "----------- Epoch 419/700 -----------\n",
      " avg epoch loss: 0.1129684064751491 \n",
      "----------- Epoch 420/700 -----------\n",
      " avg epoch loss: 0.09713567799143494 \n",
      "----------- Epoch 421/700 -----------\n",
      " avg epoch loss: 0.11213416930567473 \n",
      "----------- Epoch 422/700 -----------\n",
      " avg epoch loss: 0.11166575204022229 \n",
      "----------- Epoch 423/700 -----------\n",
      " avg epoch loss: 0.11573747083451599 \n",
      "----------- Epoch 424/700 -----------\n",
      " avg epoch loss: 0.11068024819158018 \n",
      "----------- Epoch 425/700 -----------\n",
      " avg epoch loss: 0.10623825564049184 \n",
      "----------- Epoch 426/700 -----------\n",
      " avg epoch loss: 0.1077363197375089 \n",
      "----------- Epoch 427/700 -----------\n",
      " avg epoch loss: 0.10990592391975224 \n",
      "----------- Epoch 428/700 -----------\n",
      " avg epoch loss: 0.1103832445545122 \n",
      "----------- Epoch 429/700 -----------\n",
      " avg epoch loss: 0.11034984383545816 \n",
      "----------- Epoch 430/700 -----------\n",
      " avg epoch loss: 0.11030556635558605 \n",
      "----------- Epoch 431/700 -----------\n",
      " avg epoch loss: 0.10693298820778728 \n",
      "----------- Epoch 432/700 -----------\n",
      " avg epoch loss: 0.10646017694007605 \n",
      "----------- Epoch 433/700 -----------\n",
      " avg epoch loss: 0.10409952972922475 \n",
      "----------- Epoch 434/700 -----------\n",
      " avg epoch loss: 0.11048893052875064 \n",
      "----------- Epoch 435/700 -----------\n",
      " avg epoch loss: 0.1043605177141726 \n",
      "----------- Epoch 436/700 -----------\n",
      " avg epoch loss: 0.10259229753073305 \n",
      "----------- Epoch 437/700 -----------\n",
      " avg epoch loss: 0.11253226194810122 \n",
      "----------- Epoch 438/700 -----------\n",
      " avg epoch loss: 0.1085968583514914 \n",
      "----------- Epoch 439/700 -----------\n",
      " avg epoch loss: 0.10941098809242249 \n",
      "----------- Epoch 440/700 -----------\n",
      " avg epoch loss: 0.10805322255194187 \n",
      "----------- Epoch 441/700 -----------\n",
      " avg epoch loss: 0.10850188004598021 \n",
      "----------- Epoch 442/700 -----------\n",
      " avg epoch loss: 0.11287827691063285 \n",
      "----------- Epoch 443/700 -----------\n",
      " avg epoch loss: 0.11586513161100447 \n",
      "----------- Epoch 444/700 -----------\n",
      " avg epoch loss: 0.10850258349999785 \n",
      "----------- Epoch 445/700 -----------\n",
      " avg epoch loss: 0.10313390089385212 \n",
      "----------- Epoch 446/700 -----------\n",
      " avg epoch loss: 0.10842156079271809 \n",
      "----------- Epoch 447/700 -----------\n",
      " avg epoch loss: 0.09831704529281705 \n",
      "----------- Epoch 448/700 -----------\n",
      " avg epoch loss: 0.10606000169320032 \n",
      "----------- Epoch 449/700 -----------\n",
      " avg epoch loss: 0.10179162634769455 \n",
      "----------- Epoch 450/700 -----------\n",
      " avg epoch loss: 0.11455970910005271 \n",
      "----------- Epoch 451/700 -----------\n",
      " avg epoch loss: 0.10883115408290178 \n",
      "----------- Epoch 452/700 -----------\n",
      " avg epoch loss: 0.09851965762255713 \n",
      "----------- Epoch 453/700 -----------\n",
      " avg epoch loss: 0.10902999951411038 \n",
      "----------- Epoch 454/700 -----------\n",
      " avg epoch loss: 0.10818866735510528 \n",
      "----------- Epoch 455/700 -----------\n",
      " avg epoch loss: 0.099309372685384 \n",
      "----------- Epoch 456/700 -----------\n",
      " avg epoch loss: 0.10453747831191867 \n",
      "----------- Epoch 457/700 -----------\n",
      " avg epoch loss: 0.10095629022363574 \n",
      "----------- Epoch 458/700 -----------\n",
      " avg epoch loss: 0.10718704177904874 \n",
      "----------- Epoch 459/700 -----------\n",
      " avg epoch loss: 0.1037008705670014 \n",
      "----------- Epoch 460/700 -----------\n",
      " avg epoch loss: 0.10676979352906346 \n",
      "----------- Epoch 461/700 -----------\n",
      " avg epoch loss: 0.10630169258732348 \n",
      "----------- Epoch 462/700 -----------\n",
      " avg epoch loss: 0.09951810427196324 \n",
      "----------- Epoch 463/700 -----------\n",
      " avg epoch loss: 0.10560601670853793 \n",
      "----------- Epoch 464/700 -----------\n",
      " avg epoch loss: 0.10404238244472072 \n",
      "----------- Epoch 465/700 -----------\n",
      " avg epoch loss: 0.11817794621922076 \n",
      "----------- Epoch 466/700 -----------\n",
      " avg epoch loss: 0.11327611723449081 \n",
      "----------- Epoch 467/700 -----------\n",
      " avg epoch loss: 0.11101258656941354 \n",
      "----------- Epoch 468/700 -----------\n",
      " avg epoch loss: 0.10709708309732377 \n",
      "----------- Epoch 469/700 -----------\n",
      " avg epoch loss: 0.1007836841950193 \n",
      "----------- Epoch 470/700 -----------\n",
      " avg epoch loss: 0.10963266095984728 \n",
      "----------- Epoch 471/700 -----------\n",
      " avg epoch loss: 0.1145799764348194 \n",
      "----------- Epoch 472/700 -----------\n",
      " avg epoch loss: 0.1119600925645791 \n",
      "----------- Epoch 473/700 -----------\n",
      " avg epoch loss: 0.1032852510213852 \n",
      "----------- Epoch 474/700 -----------\n",
      " avg epoch loss: 0.10426472113188356 \n",
      "----------- Epoch 475/700 -----------\n",
      " avg epoch loss: 0.10957909665256739 \n",
      "----------- Epoch 476/700 -----------\n",
      " avg epoch loss: 0.099919606635347 \n",
      "----------- Epoch 477/700 -----------\n",
      " avg epoch loss: 0.10733387191127985 \n",
      "----------- Epoch 478/700 -----------\n",
      " avg epoch loss: 0.11585152149386704 \n",
      "----------- Epoch 479/700 -----------\n",
      " avg epoch loss: 0.10375484696403146 \n",
      "----------- Epoch 480/700 -----------\n",
      " avg epoch loss: 0.10647298806952313 \n",
      "----------- Epoch 481/700 -----------\n",
      " avg epoch loss: 0.0984114434113726 \n",
      "----------- Epoch 482/700 -----------\n",
      " avg epoch loss: 0.10732215793430805 \n",
      "----------- Epoch 483/700 -----------\n",
      " avg epoch loss: 0.10806335233431309 \n",
      "----------- Epoch 484/700 -----------\n",
      " avg epoch loss: 0.09545715825725347 \n",
      "----------- Epoch 485/700 -----------\n",
      " avg epoch loss: 0.10681149570830166 \n",
      "----------- Epoch 486/700 -----------\n",
      " avg epoch loss: 0.11423251257278025 \n",
      "----------- Epoch 487/700 -----------\n",
      " avg epoch loss: 0.10800850671436638 \n",
      "----------- Epoch 488/700 -----------\n",
      " avg epoch loss: 0.10779446268593892 \n",
      "----------- Epoch 489/700 -----------\n",
      " avg epoch loss: 0.09432385257259011 \n",
      "----------- Epoch 490/700 -----------\n",
      " avg epoch loss: 0.10389831751724705 \n",
      "----------- Epoch 491/700 -----------\n",
      " avg epoch loss: 0.0954605482800398 \n",
      "----------- Epoch 492/700 -----------\n",
      " avg epoch loss: 0.10386324449349195 \n",
      "----------- Epoch 493/700 -----------\n",
      " avg epoch loss: 0.10810273443162441 \n",
      "----------- Epoch 494/700 -----------\n",
      " avg epoch loss: 0.10465724734449759 \n",
      "----------- Epoch 495/700 -----------\n",
      " avg epoch loss: 0.10964425988867879 \n",
      "----------- Epoch 496/700 -----------\n",
      " avg epoch loss: 0.11610019961744547 \n",
      "----------- Epoch 497/700 -----------\n",
      " avg epoch loss: 0.10388385640038178 \n",
      "----------- Epoch 498/700 -----------\n",
      " avg epoch loss: 0.109022336024791 \n",
      "----------- Epoch 499/700 -----------\n",
      " avg epoch loss: 0.10929692708607763 \n",
      "----------- Epoch 500/700 -----------\n",
      " avg epoch loss: 0.1027499497514218 \n",
      "----------- Epoch 501/700 -----------\n",
      " avg epoch loss: 0.09930906585510821 \n",
      "----------- Epoch 502/700 -----------\n",
      " avg epoch loss: 0.09903205451648682 \n",
      "----------- Epoch 503/700 -----------\n",
      " avg epoch loss: 0.11028403034061193 \n",
      "----------- Epoch 504/700 -----------\n",
      " avg epoch loss: 0.09958612378500402 \n",
      "----------- Epoch 505/700 -----------\n",
      " avg epoch loss: 0.10277732611447572 \n",
      "----------- Epoch 506/700 -----------\n",
      " avg epoch loss: 0.10173732499033213 \n",
      "----------- Epoch 507/700 -----------\n",
      " avg epoch loss: 0.11105217034742236 \n",
      "----------- Epoch 508/700 -----------\n",
      " avg epoch loss: 0.10277851649932564 \n",
      "----------- Epoch 509/700 -----------\n",
      " avg epoch loss: 0.10095277858851477 \n",
      "----------- Epoch 510/700 -----------\n",
      " avg epoch loss: 0.1031327377408743 \n",
      "----------- Epoch 511/700 -----------\n",
      " avg epoch loss: 0.10838498675567097 \n",
      "----------- Epoch 512/700 -----------\n",
      " avg epoch loss: 0.09962659266125411 \n",
      "----------- Epoch 513/700 -----------\n",
      " avg epoch loss: 0.100078667094931 \n",
      "----------- Epoch 514/700 -----------\n",
      " avg epoch loss: 0.1018715175241232 \n",
      "----------- Epoch 515/700 -----------\n",
      " avg epoch loss: 0.11174805661663413 \n",
      "----------- Epoch 516/700 -----------\n",
      " avg epoch loss: 0.10011670914385468 \n",
      "----------- Epoch 517/700 -----------\n",
      " avg epoch loss: 0.10023595787212253 \n",
      "----------- Epoch 518/700 -----------\n",
      " avg epoch loss: 0.10008105361927301 \n",
      "----------- Epoch 519/700 -----------\n",
      " avg epoch loss: 0.10191927592409775 \n",
      "----------- Epoch 520/700 -----------\n",
      " avg epoch loss: 0.10459162308461964 \n",
      "----------- Epoch 521/700 -----------\n",
      " avg epoch loss: 0.09636565656354651 \n",
      "----------- Epoch 522/700 -----------\n",
      " avg epoch loss: 0.10291424332698808 \n",
      "----------- Epoch 523/700 -----------\n",
      " avg epoch loss: 0.10028762954939156 \n",
      "----------- Epoch 524/700 -----------\n",
      " avg epoch loss: 0.09429704916663467 \n",
      "----------- Epoch 525/700 -----------\n",
      " avg epoch loss: 0.09630380553100258 \n",
      "----------- Epoch 526/700 -----------\n",
      " avg epoch loss: 0.09985736981686205 \n",
      "----------- Epoch 527/700 -----------\n",
      " avg epoch loss: 0.1086770120728761 \n",
      "----------- Epoch 528/700 -----------\n",
      " avg epoch loss: 0.0976027187211439 \n",
      "----------- Epoch 529/700 -----------\n",
      " avg epoch loss: 0.0970311365481466 \n",
      "----------- Epoch 530/700 -----------\n",
      " avg epoch loss: 0.10027548723015935 \n",
      "----------- Epoch 531/700 -----------\n",
      " avg epoch loss: 0.09929856607271359 \n",
      "----------- Epoch 532/700 -----------\n",
      " avg epoch loss: 0.10455772029282526 \n",
      "----------- Epoch 533/700 -----------\n",
      " avg epoch loss: 0.09233655155356973 \n",
      "----------- Epoch 534/700 -----------\n",
      " avg epoch loss: 0.09743192115984857 \n",
      "----------- Epoch 535/700 -----------\n",
      " avg epoch loss: 0.09843590252846479 \n",
      "----------- Epoch 536/700 -----------\n",
      " avg epoch loss: 0.10423602589499205 \n",
      "----------- Epoch 537/700 -----------\n",
      " avg epoch loss: 0.10120695716701449 \n",
      "----------- Epoch 538/700 -----------\n",
      " avg epoch loss: 0.09597345965821295 \n",
      "----------- Epoch 539/700 -----------\n",
      " avg epoch loss: 0.09859507634444162 \n",
      "----------- Epoch 540/700 -----------\n",
      " avg epoch loss: 0.09935816102242097 \n",
      "----------- Epoch 541/700 -----------\n",
      " avg epoch loss: 0.10516249676514416 \n",
      "----------- Epoch 542/700 -----------\n",
      " avg epoch loss: 0.09753536128718406 \n",
      "----------- Epoch 543/700 -----------\n",
      " avg epoch loss: 0.09671994539815933 \n",
      "----------- Epoch 544/700 -----------\n",
      " avg epoch loss: 0.10235165728256107 \n",
      "----------- Epoch 545/700 -----------\n",
      " avg epoch loss: 0.09519697872269899 \n",
      "----------- Epoch 546/700 -----------\n",
      " avg epoch loss: 0.11755336942942814 \n",
      "----------- Epoch 547/700 -----------\n",
      " avg epoch loss: 0.10059777522832156 \n",
      "----------- Epoch 548/700 -----------\n",
      " avg epoch loss: 0.09913682515826076 \n",
      "----------- Epoch 549/700 -----------\n",
      " avg epoch loss: 0.1080999685479328 \n",
      "----------- Epoch 550/700 -----------\n",
      " avg epoch loss: 0.0967148324456066 \n",
      "----------- Epoch 551/700 -----------\n",
      " avg epoch loss: 0.09407392631005496 \n",
      "----------- Epoch 552/700 -----------\n",
      " avg epoch loss: 0.09933282737154514 \n",
      "----------- Epoch 553/700 -----------\n",
      " avg epoch loss: 0.10328498535044492 \n",
      "----------- Epoch 554/700 -----------\n",
      " avg epoch loss: 0.0942278365935199 \n",
      "----------- Epoch 555/700 -----------\n",
      " avg epoch loss: 0.09560243060346693 \n",
      "----------- Epoch 556/700 -----------\n",
      " avg epoch loss: 0.10117963956296444 \n",
      "----------- Epoch 557/700 -----------\n",
      " avg epoch loss: 0.10122128452826291 \n",
      "----------- Epoch 558/700 -----------\n",
      " avg epoch loss: 0.1025640602549538 \n",
      "----------- Epoch 559/700 -----------\n",
      " avg epoch loss: 0.10815309842675924 \n",
      "----------- Epoch 560/700 -----------\n",
      " avg epoch loss: 0.0944451600778848 \n",
      "----------- Epoch 561/700 -----------\n",
      " avg epoch loss: 0.1011944311503321 \n",
      "----------- Epoch 562/700 -----------\n",
      " avg epoch loss: 0.09838787343818695 \n",
      "----------- Epoch 563/700 -----------\n",
      " avg epoch loss: 0.09974636355461552 \n",
      "----------- Epoch 564/700 -----------\n",
      " avg epoch loss: 0.10863287766743451 \n",
      "----------- Epoch 565/700 -----------\n",
      " avg epoch loss: 0.10067053474485874 \n",
      "----------- Epoch 566/700 -----------\n",
      " avg epoch loss: 0.09686781065538526 \n",
      "----------- Epoch 567/700 -----------\n",
      " avg epoch loss: 0.09789952584728599 \n",
      "----------- Epoch 568/700 -----------\n",
      " avg epoch loss: 0.09338105290010572 \n",
      "----------- Epoch 569/700 -----------\n",
      " avg epoch loss: 0.09723397907242179 \n",
      "----------- Epoch 570/700 -----------\n",
      " avg epoch loss: 0.09486439909692854 \n",
      "----------- Epoch 571/700 -----------\n",
      " avg epoch loss: 0.0908527324735187 \n",
      "----------- Epoch 572/700 -----------\n",
      " avg epoch loss: 0.10897224150644615 \n",
      "----------- Epoch 573/700 -----------\n",
      " avg epoch loss: 0.09354595597321168 \n",
      "----------- Epoch 574/700 -----------\n",
      " avg epoch loss: 0.10287971805501729 \n",
      "----------- Epoch 575/700 -----------\n",
      " avg epoch loss: 0.0988425463931635 \n",
      "----------- Epoch 576/700 -----------\n",
      " avg epoch loss: 0.10126878767088056 \n",
      "----------- Epoch 577/700 -----------\n",
      " avg epoch loss: 0.09857094403821975 \n",
      "----------- Epoch 578/700 -----------\n",
      " avg epoch loss: 0.103744119820185 \n",
      "----------- Epoch 579/700 -----------\n",
      " avg epoch loss: 0.09781198983732611 \n",
      "----------- Epoch 580/700 -----------\n",
      " avg epoch loss: 0.10205727239698172 \n",
      "----------- Epoch 581/700 -----------\n",
      " avg epoch loss: 0.10170948303677142 \n",
      "----------- Epoch 582/700 -----------\n",
      " avg epoch loss: 0.09052044919179753 \n",
      "----------- Epoch 583/700 -----------\n",
      " avg epoch loss: 0.09811693279631437 \n",
      "----------- Epoch 584/700 -----------\n",
      " avg epoch loss: 0.10225259580649436 \n",
      "----------- Epoch 585/700 -----------\n",
      " avg epoch loss: 0.09476815198292024 \n",
      "----------- Epoch 586/700 -----------\n",
      " avg epoch loss: 0.09143797494843602 \n",
      "----------- Epoch 587/700 -----------\n",
      " avg epoch loss: 0.096292551279068 \n",
      "----------- Epoch 588/700 -----------\n",
      " avg epoch loss: 0.09434645203175024 \n",
      "----------- Epoch 589/700 -----------\n",
      " avg epoch loss: 0.10791639882000163 \n",
      "----------- Epoch 590/700 -----------\n",
      " avg epoch loss: 0.09742200918123126 \n",
      "----------- Epoch 591/700 -----------\n",
      " avg epoch loss: 0.10077541756164282 \n",
      "----------- Epoch 592/700 -----------\n",
      " avg epoch loss: 0.09410974631365389 \n",
      "----------- Epoch 593/700 -----------\n",
      " avg epoch loss: 0.10252495744731277 \n",
      "----------- Epoch 594/700 -----------\n",
      " avg epoch loss: 0.09624991236813367 \n",
      "----------- Epoch 595/700 -----------\n",
      " avg epoch loss: 0.10102076128078624 \n",
      "----------- Epoch 596/700 -----------\n",
      " avg epoch loss: 0.10855388417001814 \n",
      "----------- Epoch 597/700 -----------\n",
      " avg epoch loss: 0.09954953672271222 \n",
      "----------- Epoch 598/700 -----------\n",
      " avg epoch loss: 0.09657270340994001 \n",
      "----------- Epoch 599/700 -----------\n",
      " avg epoch loss: 0.09442664761375635 \n",
      "----------- Epoch 600/700 -----------\n",
      " avg epoch loss: 0.10163164722314104 \n",
      "----------- Epoch 601/700 -----------\n",
      " avg epoch loss: 0.09645476938877255 \n",
      "----------- Epoch 602/700 -----------\n",
      " avg epoch loss: 0.09727275816071779 \n",
      "----------- Epoch 603/700 -----------\n",
      " avg epoch loss: 0.09839587416034191 \n",
      "----------- Epoch 604/700 -----------\n",
      " avg epoch loss: 0.09455738302320242 \n",
      "----------- Epoch 605/700 -----------\n",
      " avg epoch loss: 0.09832832522410899 \n",
      "----------- Epoch 606/700 -----------\n",
      " avg epoch loss: 0.08975120595563203 \n",
      "----------- Epoch 607/700 -----------\n",
      " avg epoch loss: 0.08900288446154446 \n",
      "----------- Epoch 608/700 -----------\n",
      " avg epoch loss: 0.10533963053254411 \n",
      "----------- Epoch 609/700 -----------\n",
      " avg epoch loss: 0.10444625693187118 \n",
      "----------- Epoch 610/700 -----------\n",
      " avg epoch loss: 0.09534069729363545 \n",
      "----------- Epoch 611/700 -----------\n",
      " avg epoch loss: 0.10645312145724893 \n",
      "----------- Epoch 612/700 -----------\n",
      " avg epoch loss: 0.08807158150663599 \n",
      "----------- Epoch 613/700 -----------\n",
      " avg epoch loss: 0.09227983921393752 \n",
      "----------- Epoch 614/700 -----------\n",
      " avg epoch loss: 0.09402392711956054 \n",
      "----------- Epoch 615/700 -----------\n",
      " avg epoch loss: 0.0990670888456516 \n",
      "----------- Epoch 616/700 -----------\n",
      " avg epoch loss: 0.09885162385832519 \n",
      "----------- Epoch 617/700 -----------\n",
      " avg epoch loss: 0.10247901843255386 \n",
      "----------- Epoch 618/700 -----------\n",
      " avg epoch loss: 0.09090443256869912 \n",
      "----------- Epoch 619/700 -----------\n",
      " avg epoch loss: 0.08768185255583376 \n",
      "----------- Epoch 620/700 -----------\n",
      " avg epoch loss: 0.09305566553247627 \n",
      "----------- Epoch 621/700 -----------\n",
      " avg epoch loss: 0.0905200023385696 \n",
      "----------- Epoch 622/700 -----------\n",
      " avg epoch loss: 0.09540080572338774 \n",
      "----------- Epoch 623/700 -----------\n",
      " avg epoch loss: 0.09493042870657518 \n",
      "----------- Epoch 624/700 -----------\n",
      " avg epoch loss: 0.10012647329457104 \n",
      "----------- Epoch 625/700 -----------\n",
      " avg epoch loss: 0.09288015283644199 \n",
      "----------- Epoch 626/700 -----------\n",
      " avg epoch loss: 0.09680110035371035 \n",
      "----------- Epoch 627/700 -----------\n",
      " avg epoch loss: 0.09973529216065072 \n",
      "----------- Epoch 628/700 -----------\n",
      " avg epoch loss: 0.09716990919294767 \n",
      "----------- Epoch 629/700 -----------\n",
      " avg epoch loss: 0.0944064305331558 \n",
      "----------- Epoch 630/700 -----------\n",
      " avg epoch loss: 0.09262763387756422 \n",
      "----------- Epoch 631/700 -----------\n",
      " avg epoch loss: 0.09358367036096751 \n",
      "----------- Epoch 632/700 -----------\n",
      " avg epoch loss: 0.09535220510978251 \n",
      "----------- Epoch 633/700 -----------\n",
      " avg epoch loss: 0.09252745711617172 \n",
      "----------- Epoch 634/700 -----------\n",
      " avg epoch loss: 0.10020913685113192 \n",
      "----------- Epoch 635/700 -----------\n",
      " avg epoch loss: 0.09716591524379328 \n",
      "----------- Epoch 636/700 -----------\n",
      " avg epoch loss: 0.10045273009873927 \n",
      "----------- Epoch 637/700 -----------\n",
      " avg epoch loss: 0.10084673038870096 \n",
      "----------- Epoch 638/700 -----------\n",
      " avg epoch loss: 0.09136462340457364 \n",
      "----------- Epoch 639/700 -----------\n",
      " avg epoch loss: 0.1012707170159556 \n",
      "----------- Epoch 640/700 -----------\n",
      " avg epoch loss: 0.09872946796147153 \n",
      "----------- Epoch 641/700 -----------\n",
      " avg epoch loss: 0.08844244086835533 \n",
      "----------- Epoch 642/700 -----------\n",
      " avg epoch loss: 0.10087057777587324 \n",
      "----------- Epoch 643/700 -----------\n",
      " avg epoch loss: 0.09745289295748807 \n",
      "----------- Epoch 644/700 -----------\n",
      " avg epoch loss: 0.09750772503390909 \n",
      "----------- Epoch 645/700 -----------\n",
      " avg epoch loss: 0.10276589165721088 \n",
      "----------- Epoch 646/700 -----------\n",
      " avg epoch loss: 0.09001193302497268 \n",
      "----------- Epoch 647/700 -----------\n",
      " avg epoch loss: 0.09819210575241595 \n",
      "----------- Epoch 648/700 -----------\n",
      " avg epoch loss: 0.0988612223090604 \n",
      "----------- Epoch 649/700 -----------\n",
      " avg epoch loss: 0.094651926365681 \n",
      "----------- Epoch 650/700 -----------\n",
      " avg epoch loss: 0.09712390464171768 \n",
      "----------- Epoch 651/700 -----------\n",
      " avg epoch loss: 0.09668285419000312 \n",
      "----------- Epoch 652/700 -----------\n",
      " avg epoch loss: 0.09708958003530278 \n",
      "----------- Epoch 653/700 -----------\n",
      " avg epoch loss: 0.09488043197523803 \n",
      "----------- Epoch 654/700 -----------\n",
      " avg epoch loss: 0.10130653877276927 \n",
      "----------- Epoch 655/700 -----------\n",
      " avg epoch loss: 0.09141888738982379 \n",
      "----------- Epoch 656/700 -----------\n",
      " avg epoch loss: 0.08881289846962317 \n",
      "----------- Epoch 657/700 -----------\n",
      " avg epoch loss: 0.0907884689355269 \n",
      "----------- Epoch 658/700 -----------\n",
      " avg epoch loss: 0.09596565202437342 \n",
      "----------- Epoch 659/700 -----------\n",
      " avg epoch loss: 0.0992815414769575 \n",
      "----------- Epoch 660/700 -----------\n",
      " avg epoch loss: 0.09412213279725984 \n",
      "----------- Epoch 661/700 -----------\n",
      " avg epoch loss: 0.09587741328403354 \n",
      "----------- Epoch 662/700 -----------\n",
      " avg epoch loss: 0.09371772210206837 \n",
      "----------- Epoch 663/700 -----------\n",
      " avg epoch loss: 0.09685892552882433 \n",
      "----------- Epoch 664/700 -----------\n",
      " avg epoch loss: 0.09712200371269136 \n",
      "----------- Epoch 665/700 -----------\n",
      " avg epoch loss: 0.09753801934048534 \n",
      "----------- Epoch 666/700 -----------\n",
      " avg epoch loss: 0.09797832191176713 \n",
      "----------- Epoch 667/700 -----------\n",
      " avg epoch loss: 0.09718621034547686 \n",
      "----------- Epoch 668/700 -----------\n",
      " avg epoch loss: 0.09349107670411468 \n",
      "----------- Epoch 669/700 -----------\n",
      " avg epoch loss: 0.09579679808160291 \n",
      "----------- Epoch 670/700 -----------\n",
      " avg epoch loss: 0.0962516865702346 \n",
      "----------- Epoch 671/700 -----------\n",
      " avg epoch loss: 0.1053469011913985 \n",
      "----------- Epoch 672/700 -----------\n",
      " avg epoch loss: 0.09936472374480218 \n",
      "----------- Epoch 673/700 -----------\n",
      " avg epoch loss: 0.09500862678140402 \n",
      "----------- Epoch 674/700 -----------\n",
      " avg epoch loss: 0.09154145157709717 \n",
      "----------- Epoch 675/700 -----------\n",
      " avg epoch loss: 0.09125615715887397 \n",
      "----------- Epoch 676/700 -----------\n",
      " avg epoch loss: 0.09315194622147828 \n",
      "----------- Epoch 677/700 -----------\n",
      " avg epoch loss: 0.09227184304455295 \n",
      "----------- Epoch 678/700 -----------\n",
      " avg epoch loss: 0.08674897599872201 \n",
      "----------- Epoch 679/700 -----------\n",
      " avg epoch loss: 0.09414983424730598 \n",
      "----------- Epoch 680/700 -----------\n",
      " avg epoch loss: 0.09898737867036835 \n",
      "----------- Epoch 681/700 -----------\n",
      " avg epoch loss: 0.09208092034654691 \n",
      "----------- Epoch 682/700 -----------\n",
      " avg epoch loss: 0.09185819264920428 \n",
      "----------- Epoch 683/700 -----------\n",
      " avg epoch loss: 0.09723506115749478 \n",
      "----------- Epoch 684/700 -----------\n",
      " avg epoch loss: 0.0952601703274995 \n",
      "----------- Epoch 685/700 -----------\n",
      " avg epoch loss: 0.09793671447318048 \n",
      "----------- Epoch 686/700 -----------\n",
      " avg epoch loss: 0.09332423347327858 \n",
      "----------- Epoch 687/700 -----------\n",
      " avg epoch loss: 0.09338739623501897 \n",
      "----------- Epoch 688/700 -----------\n",
      " avg epoch loss: 0.08763094852305948 \n",
      "----------- Epoch 689/700 -----------\n",
      " avg epoch loss: 0.09020701780961826 \n",
      "----------- Epoch 690/700 -----------\n",
      " avg epoch loss: 0.089749694484286 \n",
      "----------- Epoch 691/700 -----------\n",
      " avg epoch loss: 0.09770480671338737 \n",
      "----------- Epoch 692/700 -----------\n",
      " avg epoch loss: 0.09232489982899278 \n",
      "----------- Epoch 693/700 -----------\n",
      " avg epoch loss: 0.0893731803894043 \n",
      "----------- Epoch 694/700 -----------\n",
      " avg epoch loss: 0.09841191275045276 \n",
      "----------- Epoch 695/700 -----------\n",
      " avg epoch loss: 0.09675394067913294 \n",
      "----------- Epoch 696/700 -----------\n",
      " avg epoch loss: 0.10622850848315284 \n",
      "----------- Epoch 697/700 -----------\n",
      " avg epoch loss: 0.09835820508562028 \n",
      "----------- Epoch 698/700 -----------\n",
      " avg epoch loss: 0.09493303175922484 \n",
      "----------- Epoch 699/700 -----------\n",
      " avg epoch loss: 0.10203176769334824 \n",
      "----------- Epoch 700/700 -----------\n",
      " avg epoch loss: 0.09436004997696727 \n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "i = 1\n",
    "total_epoch_loss = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"----------- Epoch {epoch+1}/{epochs} -----------\")\n",
    "    epoch_loss = []\n",
    "\n",
    "    for batch_features, batch_labels in train_loader:\n",
    "\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(batch_features)\n",
    "\n",
    "      \n",
    "        \n",
    "\n",
    "        # loss calculation\n",
    "        loss = loss_function(outputs, batch_labels.squeeze())\n",
    "        total_epoch_loss.append(loss.item())\n",
    "\n",
    "        # backprop\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "    print(f\" avg epoch loss: {sum(epoch_loss)/len(epoch_loss)} \")\n",
    "\n",
    "        # update gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc9d282f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8110bc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_: tensor([20.2203, 14.5283, 10.5542,  9.5363,  9.6815, 19.0161, 17.2712, 21.9782,\n",
      "        14.6558, 24.3794,  6.3633, 13.1010, 19.1785, 31.8668,  6.4249, 23.0801,\n",
      "        22.3702, 10.0810, 29.5789, 21.4938, 24.9441,  5.7097, 11.0885, 23.6081,\n",
      "        11.7713, 30.3535, 27.1362, 11.5558, 21.3798, 15.7598,  8.2601, 22.8211,\n",
      "        24.1115, 14.2723,  4.6293,  9.6025,  5.5962,  7.4510, 17.6680, 11.4184,\n",
      "        12.6121, 26.0036,  9.6368, 17.2987, 24.6082,  8.3409, 14.0982,  9.6182,\n",
      "        12.6412,  7.2364, 22.3319,  6.1591,  8.4048, 22.2118,  3.3828, 13.1297,\n",
      "        24.8780,  5.1471, 12.2952,  6.2346, 15.6126, 15.3296, 18.7507, 13.4052]), len: 64\n",
      "Predicted: tensor([2, 8, 3, 3, 2, 9, 9, 7, 1, 7, 3, 6, 5, 9, 4, 5, 1, 8, 9, 7, 8, 4, 3, 1,\n",
      "        4, 9, 5, 6, 7, 8, 5, 5, 8, 4, 6, 8, 4, 6, 8, 6, 0, 1, 0, 2, 5, 2, 2, 4,\n",
      "        0, 4, 1, 0, 3, 1, 2, 2, 1, 0, 0, 6, 8, 2, 8, 4])\n",
      "Actual   : tensor([2, 8, 3, 3, 2, 9, 9, 7, 1, 7, 3, 6, 5, 9, 2, 5, 1, 8, 9, 7, 8, 3, 3, 1,\n",
      "        4, 9, 5, 6, 7, 8, 5, 5, 8, 4, 0, 8, 4, 2, 8, 6, 0, 1, 0, 2, 5, 6, 2, 2,\n",
      "        0, 4, 1, 6, 3, 1, 2, 2, 1, 0, 0, 2, 8, 2, 8, 4])\n",
      " total cnt : 63\n",
      "Accuracy of the model on the test set: 87.05%\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "model.eval()\n",
    "\n",
    "\n",
    "total = 0\n",
    "correct = 0 \n",
    "i = 1\n",
    "cnt =0\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_labels in test_loader:\n",
    "\n",
    "        cnt +=1\n",
    "            \n",
    "        test_outputs = model(batch_features)\n",
    "\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        \n",
    "        if i <2:\n",
    "            print(f\"_: {_}, len: {len(_)}\")\n",
    "            print(\"Predicted:\", predicted)\n",
    "            print(\"Actual   :\", batch_labels.squeeze())\n",
    "            i += 1\n",
    "        \n",
    "        total += batch_labels.shape[0]\n",
    "        correct += (predicted == batch_labels.squeeze()).sum().item()\n",
    "\n",
    "print(f\" total cnt : {cnt}\")\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy of the model on the test set: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c629b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of the model on the test set: 84.45%\n",
    "# Accuracy of the model on the test set: 84.70%\n",
    "# Accuracy of the model on the test set: 84.30%\n",
    "# Accuracy of the model on the test set: 84.90%\n",
    "# Accuracy of the model on the test set: 87.45%\n",
    "# Accuracy of the model on the test set: 85.92% - increasing the batch size to 100 \n",
    "# Accuracy of the model on the test set: 87.08% - batch size 100, learning rate 0.05, hidden layers: 128, 64, 16, 10,  dropout layers added\n",
    "# Accuracy of the model on the test set: 87.67% - batch size 64, learning rate 0.05, hidden layers: 128, 64, 16, 10,  dropout layers added, batch normalization layers added\n",
    "# Accuracy of the model on the test set: 86.30% - batch size 32, learning rate 0.05, hidden layers: 128, 64, 16, 10,  dropout layers added, batch normalization layers added\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2d0f5b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
