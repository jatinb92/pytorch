{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31239,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:41.460408Z","iopub.execute_input":"2026-01-09T09:10:41.461053Z","iopub.status.idle":"2026-01-09T09:10:41.744316Z","shell.execute_reply.started":"2026-01-09T09:10:41.461019Z","shell.execute_reply":"2026-01-09T09:10:41.743550Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/gutenberg-books-data/1661-0.txt\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#! pip install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:41.745748Z","iopub.execute_input":"2026-01-09T09:10:41.746193Z","iopub.status.idle":"2026-01-09T09:10:41.749515Z","shell.execute_reply.started":"2026-01-09T09:10:41.746168Z","shell.execute_reply":"2026-01-09T09:10:41.748660Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport numpy as np\n\nfrom collections import   Counter\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom nltk.tokenize import word_tokenize\nimport nltk\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:41.750689Z","iopub.execute_input":"2026-01-09T09:10:41.751018Z","iopub.status.idle":"2026-01-09T09:10:49.895091Z","shell.execute_reply.started":"2026-01-09T09:10:41.750967Z","shell.execute_reply":"2026-01-09T09:10:49.894356Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"path = '/kaggle/input/gutenberg-books-data/1661-0.txt'\ndocument = open(path).read()\nprint('corpus length:', len(document))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:49.896729Z","iopub.execute_input":"2026-01-09T09:10:49.897121Z","iopub.status.idle":"2026-01-09T09:10:49.923665Z","shell.execute_reply.started":"2026-01-09T09:10:49.897096Z","shell.execute_reply":"2026-01-09T09:10:49.923120Z"}},"outputs":[{"name":"stdout","text":"corpus length: 581888\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"document += \"\"\" You‚Äôve cleaned your data, engineered brilliant features, and trained a model. You‚Äôve validated and optimized it with an impressive F1-score. You are ready to push to production.\n\nThis isn‚Äôt about uploading a Jupyter notebook ‚Äî it‚Äôs about orchestrating a controlled, monitored transition that brings your model to millions of real users without breaking anything in the process.\n\nIn the world of Production-Grade Systems, ‚Äúdeployment‚Äù is not a binary switch. It is a spectrum of risk and reward. As a Senior Engineer, you must choose the strategy that best balances safety with speed.\n\nDeployment Strategies\n1. Shadow Deployment: Validate Without Impact\nThe Concept\nIn a shadow deployment (also known as ‚Äúdark launch‚Äù), production traffic is duplicated.\n\nThe current model responds to the user, while the new model processes the same data in the background without its output ever reaching the user.\n\nBoth models see the same traffic, but only the old model‚Äôs predictions are served to users.\n\nThe new model‚Äôs outputs are logged and compared offline.\n\nHow it works\nVersion 1 (current) ‚Üí receives traffic ‚Üí serves predictions to users ‚úÖ\n\nVersion 2 (new) ‚Üí receives same traffic ‚Üí logs predictions silently üìù\n\nCompare both offline: latency, output distributions, edge case behaviour etc.\n\nWhen to use it\nIn high-stakes domains like healthcare, finance, autonomous vehicles etc.\n\nWhen deploying a fairly new model architecture that you want to validate exhaustively.\n\nWhen you want zero real-world impact during validation\n\nPros\nIt has zero risk to users as the new version is solely deployed for observation and monitoring purpose.\n\nIt validates the model‚Äôs performance on real production traffic.\n\nYou can catch issues and edge cases before users experience them.\n\nCons\nIt doubles the computation cost as you will be running two models simultaneously.\n\nYou are not able to monitor real-world feedback on latency perceived by users.\n\nIt takes time to gather enough data for statistical confidence; hence you need to run both the versions (old + new) for long time.\n\n2. Canary Deployment: Gradual, Controlled Rollout\nThe Concept\nRoute a small percentage of traffic (e.g., 5%) to the new model.\n\nMonitor it closely and gradually increase traffic if metrics remain stable.\n\nIf metrics (latency, error rate, conversion) look good, you gradually increase the percentage (10% -> 50% -> 100%).\n\nRollout stages\nStage 1: v1 (95%) | v2 (5%) ‚Äî monitor v2 closely with fine-grained alerts\n\nStage 2: v1 (80%) | v2 (20%) ‚Äî still monitoring and comparing performance metrics\n\nStage 3: v1 (50%) | v2 (50%) ‚Äî evaluating overall impact\n\nStage 4: v1 (0%) | v2 (100%) ‚Äî full rollout completed\n\nWhen to use it\nIn high-traffic applications where you want gradual user exposure.\n\nIf Models are too complex and have a hard-to-predict behaviours in production.\n\nWhen catching issues early is critical but some exposure is acceptable\n\nPros\nMitigates risk by limiting initial exposure thereby reducing the blast radius if something goes wrong.\n\nIt provides real-time feedback on latency, load handling and cache behaviour.\n\nIt allows for fast rollback if issues emerge.\n\nIt enables you to detect long-tail issues (rare but impactful edge cases)\n\nCons\nSlower to reach full deployment\n\nRequires sophisticated traffic routing and monitoring\n\nStateful user experiences complicate rollout i.e. users seeing different models\n\n3. Blue-Green Deployment: Instant Switchover\nThe Concept\nBlue-Green deployment is a high-availability strategy designed to minimize downtime and reduce the risk of a release by maintaining two identical production environments.\n\nThe core of this strategy lies in the use of a Router or Load Balancer that can instantly redirect traffic between two environments, traditionally labelled ‚ÄúBlue‚Äù and ‚ÄúGreen‚Äù.\n\nRollout stages\nThe Blue Environment: This is the ‚ÄúCurrent‚Äù live environment. It is currently serving 100% of production traffic using the existing model version.\n\nThe Green Environment: This is the ‚ÄúNew‚Äù environment. It is an exact clone of the Blue environment‚Äôs infrastructure but hosts the new model version.\n\nWhile Green is ‚Äúidle‚Äù (not receiving public traffic), the engineering team performs final sanity checks and validation test in this live-cloned setting.\n\nThe Cutover: Once the Green environment is validated, the router switches all incoming traffic from Blue to Green instantly.\n\nWhen to use it\nUse it when the system requires near-zero downtime during updates.It is ideal for high-stakes environments where you need the ability to revert to a stable state immediately if the new model fails.This strategy should be used when the organization can afford to double its infrastructure costs.Pros Instantaneous Rollout: The transition happens at the routing layer, meaning users experience no downtime during the update.\n\nZero-Risk Rollback: If the new model (Green) begins to fail or produce unexpected results, you can switch the router back to Blue instantly, returning users to the stable, known state.\n\nIsolated Testing: Because Green is identical to production but isolated from users, you can catch infrastructure-level bugs (like memory leaks or dependency mismatches) before a single user is affected.\n\nCons\nResource Intensive: This is the most expensive rollout strategy because it requires doubling your infrastructure.\n\nYou must pay for the compute and memory of two full production clusters simultaneously.\n\nState Management: If your ML system relies on a database or a feature store that changes as users interact with the model, keeping the Blue and Green environments perfectly synchronized during the transition can be technically complex.\n\n4. A/B Testing Deployment: Causal Proof of Impact\nThe Concept\nThis strategy is used by routing traffic randomly to two (or more) model versions.\n\nYou can measure the impact on key business metrics ‚Äî not just technical metrics, but outcomes that matter: click-through rate, conversion rate, user engagement, revenue etc.\n\n\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:49.924572Z","iopub.execute_input":"2026-01-09T09:10:49.924764Z","iopub.status.idle":"2026-01-09T09:10:49.933053Z","shell.execute_reply.started":"2026-01-09T09:10:49.924745Z","shell.execute_reply":"2026-01-09T09:10:49.932362Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('punkt_tab')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:49.934055Z","iopub.execute_input":"2026-01-09T09:10:49.934336Z","iopub.status.idle":"2026-01-09T09:10:50.277788Z","shell.execute_reply.started":"2026-01-09T09:10:49.934308Z","shell.execute_reply":"2026-01-09T09:10:50.277123Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Extracting Tokens and Generating Data","metadata":{}},{"cell_type":"code","source":"# tokenise\n\ntokens = word_tokenize(document.lower())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:50.278656Z","iopub.execute_input":"2026-01-09T09:10:50.278880Z","iopub.status.idle":"2026-01-09T09:10:50.793173Z","shell.execute_reply.started":"2026-01-09T09:10:50.278860Z","shell.execute_reply":"2026-01-09T09:10:50.792286Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# build vocab\n\nvocab = {\"<UNK>\": 0}\n\nfor token in Counter(tokens).keys():\n    #print(token)\n    if token not in vocab:\n        vocab[token] = len(vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:50.794330Z","iopub.execute_input":"2026-01-09T09:10:50.794680Z","iopub.status.idle":"2026-01-09T09:10:50.814638Z","shell.execute_reply.started":"2026-01-09T09:10:50.794642Z","shell.execute_reply":"2026-01-09T09:10:50.814101Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# extracting sentence from data\n\ninput_sentences = document.split(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:50.815671Z","iopub.execute_input":"2026-01-09T09:10:50.815960Z","iopub.status.idle":"2026-01-09T09:10:50.832730Z","shell.execute_reply.started":"2026-01-09T09:10:50.815928Z","shell.execute_reply":"2026-01-09T09:10:50.832194Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def text_to_indices(sentence, vocab):\n    numerical_sentence = []\n\n    for word in sentence:\n        if word in vocab:\n            numerical_sentence.append(vocab[word])\n        else:\n            numerical_sentence.append(vocab['<UNK>'])\n\n    return numerical_sentence","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:50.834836Z","iopub.execute_input":"2026-01-09T09:10:50.835137Z","iopub.status.idle":"2026-01-09T09:10:50.848777Z","shell.execute_reply.started":"2026-01-09T09:10:50.835105Z","shell.execute_reply":"2026-01-09T09:10:50.848184Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# converting sentences to numerical tokens\ninput_numerical_sentences = []\nfor sentence in input_sentences:\n    if sentence:\n        input_numerical_sentences.append(text_to_indices(word_tokenize(sentence.lower()), vocab))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:50.849666Z","iopub.execute_input":"2026-01-09T09:10:50.849911Z","iopub.status.idle":"2026-01-09T09:10:51.573161Z","shell.execute_reply.started":"2026-01-09T09:10:50.849890Z","shell.execute_reply":"2026-01-09T09:10:51.572513Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# generating padded training seq\ntraining_seq = []\nmax_len = 0\npadded_training_seq = []\n\nfor sentence in input_numerical_sentences:\n\n    for i in range(1, len(sentence)):\n        seq = sentence[:i+1]\n        training_seq.append(sentence[:i+1])\n        if len(seq)>max_len:\n            max_len = len(seq)\n\n\n\nfor seq in training_seq:\n    seq = [0]*(max_len-len(seq)) + seq\n    #print(seq)\n    padded_training_seq.append(seq)\n\npadded_training_seq = torch.tensor(padded_training_seq, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:51.574015Z","iopub.execute_input":"2026-01-09T09:10:51.574252Z","iopub.status.idle":"2026-01-09T09:10:52.751615Z","shell.execute_reply.started":"2026-01-09T09:10:51.574231Z","shell.execute_reply":"2026-01-09T09:10:52.750998Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# generating X & y\nX = padded_training_seq[:, :-1]\ny = padded_training_seq[:, -1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:52.752490Z","iopub.execute_input":"2026-01-09T09:10:52.752710Z","iopub.status.idle":"2026-01-09T09:10:52.783692Z","shell.execute_reply.started":"2026-01-09T09:10:52.752689Z","shell.execute_reply":"2026-01-09T09:10:52.783046Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Generating Dataset & DataLoader","metadata":{}},{"cell_type":"code","source":"# generating custom dataset\n\nclass CustomDataset(Dataset):\n\n    def __init__(self, X, y):\n\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:52.784541Z","iopub.execute_input":"2026-01-09T09:10:52.784768Z","iopub.status.idle":"2026-01-09T09:10:52.799446Z","shell.execute_reply.started":"2026-01-09T09:10:52.784735Z","shell.execute_reply":"2026-01-09T09:10:52.798897Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"dataset = CustomDataset(X, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:52.800353Z","iopub.execute_input":"2026-01-09T09:10:52.800639Z","iopub.status.idle":"2026-01-09T09:10:52.816789Z","shell.execute_reply.started":"2026-01-09T09:10:52.800606Z","shell.execute_reply":"2026-01-09T09:10:52.816271Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"dataloader = DataLoader(dataset, batch_size=32, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:52.817704Z","iopub.execute_input":"2026-01-09T09:10:52.818015Z","iopub.status.idle":"2026-01-09T09:10:52.835473Z","shell.execute_reply.started":"2026-01-09T09:10:52.817962Z","shell.execute_reply":"2026-01-09T09:10:52.834895Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# for inp, out in dataloader:\n#     print(inp, out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:52.836256Z","iopub.execute_input":"2026-01-09T09:10:52.836569Z","iopub.status.idle":"2026-01-09T09:10:52.850235Z","shell.execute_reply.started":"2026-01-09T09:10:52.836523Z","shell.execute_reply":"2026-01-09T09:10:52.849522Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"class LSTMModel(nn.Module):\n\n    def __init__(self, vocab_size, embedding_dimension, no_of_neurons):\n\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dimension = embedding_dimension\n        self.embedding = nn.Embedding(vocab_size, embedding_dimension)\n        self.lstm = nn.LSTM(embedding_dimension, no_of_neurons, batch_first=True)\n        self.output = nn.Linear(no_of_neurons, vocab_size)\n\n    def forward(self, ques):\n        embedded_ques = self.embedding(ques)\n        intermeddiate_hidden_state, (final_hidden_state, final_cell_state) = self.lstm(embedded_ques)\n        output = self.output(final_hidden_state.squeeze(0))\n        return output\n        \n        \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:52.851097Z","iopub.execute_input":"2026-01-09T09:10:52.851681Z","iopub.status.idle":"2026-01-09T09:10:52.866729Z","shell.execute_reply.started":"2026-01-09T09:10:52.851656Z","shell.execute_reply":"2026-01-09T09:10:52.866051Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"embedding_dimension, no_of_neurons = 100, 150\nmodel = LSTMModel(len(vocab), embedding_dimension, no_of_neurons)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nprint(f\"device: {device}\")\n\nlearning_rate = 0.0001\nepochs = 50\n\nloss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:52.867687Z","iopub.execute_input":"2026-01-09T09:10:52.868139Z","iopub.status.idle":"2026-01-09T09:10:57.025546Z","shell.execute_reply.started":"2026-01-09T09:10:52.868116Z","shell.execute_reply":"2026-01-09T09:10:57.024900Z"}},"outputs":[{"name":"stdout","text":"device: cuda\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"\ntotal_loss = []\n\nfor epoch in range(epochs):\n    \n    for batch_x, batch_y in dataloader:\n        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n\n        optimizer.zero_grad()\n\n        y_pred = model(batch_x)\n        loss = loss_function(y_pred, batch_y)\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss.append(loss.item())\n\n    print(f\"for epoch: {epoch}/{epochs}, avg loss is {sum(total_loss)/len(total_loss)}\")\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:10:57.026484Z","iopub.execute_input":"2026-01-09T09:10:57.026953Z","iopub.status.idle":"2026-01-09T09:29:08.015092Z","shell.execute_reply.started":"2026-01-09T09:10:57.026924Z","shell.execute_reply":"2026-01-09T09:29:08.014269Z"}},"outputs":[{"name":"stdout","text":"for epoch: 0/50, avg loss is 6.192543764793778\nfor epoch: 1/50, avg loss is 5.907303421069575\nfor epoch: 2/50, avg loss is 5.732721591394201\nfor epoch: 3/50, avg loss is 5.601506417455019\nfor epoch: 4/50, avg loss is 5.494664381839984\nfor epoch: 5/50, avg loss is 5.403762524402655\nfor epoch: 6/50, avg loss is 5.324204175291626\nfor epoch: 7/50, avg loss is 5.253056709920511\nfor epoch: 8/50, avg loss is 5.188480683849271\nfor epoch: 9/50, avg loss is 5.129227634954578\nfor epoch: 10/50, avg loss is 5.074324729031412\nfor epoch: 11/50, avg loss is 5.023019718887938\nfor epoch: 12/50, avg loss is 4.974746580658129\nfor epoch: 13/50, avg loss is 4.929086029426224\nfor epoch: 14/50, avg loss is 4.885650362574226\nfor epoch: 15/50, avg loss is 4.844159717185516\nfor epoch: 16/50, avg loss is 4.80437989266748\nfor epoch: 17/50, avg loss is 4.766135772624976\nfor epoch: 18/50, avg loss is 4.7292360796396675\nfor epoch: 19/50, avg loss is 4.693589379321931\nfor epoch: 20/50, avg loss is 4.659053067149102\nfor epoch: 21/50, avg loss is 4.625540471743211\nfor epoch: 22/50, avg loss is 4.5929515664889555\nfor epoch: 23/50, avg loss is 4.5612124809934675\nfor epoch: 24/50, avg loss is 4.5302651999506285\nfor epoch: 25/50, avg loss is 4.500033710548257\nfor epoch: 26/50, avg loss is 4.470467016364453\nfor epoch: 27/50, avg loss is 4.441525315117306\nfor epoch: 28/50, avg loss is 4.413171141085357\nfor epoch: 29/50, avg loss is 4.385376228212576\nfor epoch: 30/50, avg loss is 4.358101388399363\nfor epoch: 31/50, avg loss is 4.331329602321565\nfor epoch: 32/50, avg loss is 4.305028117561813\nfor epoch: 33/50, avg loss is 4.279167337856474\nfor epoch: 34/50, avg loss is 4.25373807040422\nfor epoch: 35/50, avg loss is 4.228731328916494\nfor epoch: 36/50, avg loss is 4.204112208216845\nfor epoch: 37/50, avg loss is 4.17987452483048\nfor epoch: 38/50, avg loss is 4.1560078357974355\nfor epoch: 39/50, avg loss is 4.132491149930652\nfor epoch: 40/50, avg loss is 4.109314366841195\nfor epoch: 41/50, avg loss is 4.086470394628643\nfor epoch: 42/50, avg loss is 4.063940492159547\nfor epoch: 43/50, avg loss is 4.041718548709295\nfor epoch: 44/50, avg loss is 4.019799027972052\nfor epoch: 45/50, avg loss is 3.9981615943380544\nfor epoch: 46/50, avg loss is 3.976804116210189\nfor epoch: 47/50, avg loss is 3.9557212676037112\nfor epoch: 48/50, avg loss is 3.9349085827044785\nfor epoch: 49/50, avg loss is 3.9143449497229192\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"max_len","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:29:08.016255Z","iopub.execute_input":"2026-01-09T09:29:08.016605Z","iopub.status.idle":"2026-01-09T09:29:08.021285Z","shell.execute_reply.started":"2026-01-09T09:29:08.016580Z","shell.execute_reply":"2026-01-09T09:29:08.020556Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"66"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"def predict(model, vocab, query):\n\n    tokenised = word_tokenize(query.lower())\n    \n    numerical_ques = text_to_indices(tokenised, vocab)\n    \n    padded_query = torch.tensor([0]*(max_len - len(numerical_ques)) + numerical_ques, dtype=torch.long).unsqueeze(0).to(device)\n   \n    next_word = model(padded_query)\n\n    # print(next_word)\n    # print(next_word.shape)\n\n    value, index = torch.max(next_word, dim=1)\n    # print(value)\n    # print(index)\n\n    str_next_word = list(vocab.keys())[index]\n    # print(str_next_word)\n\n    \n    return query + \" \" + str_next_word\n\n\ndef next_n_words(n, model, vocab, query):\n    \n    for i in range(n):\n        query = predict(model, vocab, query)\n        print(query)\n\n    return query\n\n\nfinal_output = next_n_words(5, model, vocab, \"\"\"‚ÄúYou had my note?‚Äù he\"\"\")\n\nprint(f\"final_output: {final_output}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:33:03.065730Z","iopub.execute_input":"2026-01-09T09:33:03.066591Z","iopub.status.idle":"2026-01-09T09:33:03.082383Z","shell.execute_reply.started":"2026-01-09T09:33:03.066547Z","shell.execute_reply":"2026-01-09T09:33:03.081692Z"}},"outputs":[{"name":"stdout","text":"‚ÄúYou had my note?‚Äù he asked\n‚ÄúYou had my note?‚Äù he asked ,\n‚ÄúYou had my note?‚Äù he asked , ‚Äú\n‚ÄúYou had my note?‚Äù he asked , ‚Äú and\n‚ÄúYou had my note?‚Äù he asked , ‚Äú and i\nfinal_output: ‚ÄúYou had my note?‚Äù he asked , ‚Äú and i\n","output_type":"stream"}],"execution_count":30}]}